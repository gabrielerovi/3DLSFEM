\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{algpseudocode}
\usepackage{algorithm}

% ref packages
\usepackage{nameref}
% folowing  must be in this order
\usepackage{varioref}
\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{booktabs}
\usepackage {tikz}
\usetikzlibrary {positioning}
\definecolor {processblue}{cmyk}{0.96,0,0,0}



%SetFonts

%SetFonts
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\divr}{\operatorname{div}}
\newcommand{\tdiv}{\operatorname{div}}
\newcommand{\ess}{\operatorname{ess}}
\newcommand{\rotore}{\operatorname{rot}}
\newcommand{\curl}{\operatorname{\textbf{curl}}}
\newcommand{\bcurl}{\operatorname{\textbf{curl}}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\bA}{\textbf{A}}
\newcommand{\ba}{\textbf{a}}
\newcommand{\bb}{\textbf{b}}
\newcommand{\bB}{\textbf{B}}
\newcommand{\bc}{\textbf{c}}
\newcommand{\bC}{\textbf{C}}
\newcommand{\bd}{\textbf{d}}
\newcommand{\bD}{\textbf{D}}
\newcommand{\be}{\textbf{e}}
\newcommand{\bE}{\textbf{E}}
\newcommand{\bff}{\textbf{f}}
\newcommand{\bF}{\textbf{F}}
\newcommand{\bg}{\textbf{g}}
\newcommand{\bG}{\textbf{G}}
\newcommand{\bi}{\textbf{i}}
\newcommand{\bI}{\textbf{I}}
\newcommand{\bj}{\textbf{j}}
\newcommand{\bJ}{\textbf{J}}
\newcommand{\bh}{\textbf{h}}
\newcommand{\bH}{\textbf{H}}
\newcommand{\bk}{\textbf{k}}
\newcommand{\bK}{\textbf{K}}
\newcommand{\bl}{\textbf{l}}
\newcommand{\bL}{\textbf{L}}
\newcommand{\bm}{\textbf{m}}
\newcommand{\bM}{\textbf{M}}
\newcommand{\bn}{\textbf{n}}
\newcommand{\bN}{\textbf{N}}
\newcommand{\bp}{\textbf{p}}
\newcommand{\bP}{\textbf{P}}
\newcommand{\bo}{\textbf{o}}
\newcommand{\bq}{\textbf{q}}
\newcommand{\bQ}{\textbf{Q}}
\newcommand{\br}{\textbf{r}}
\newcommand{\bR}{\textbf{R}}
\newcommand{\bs}{\textbf{s}}
\newcommand{\bS}{\textbf{S}}
\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\bt}{\textbf{t}}
\newcommand{\bv}{\textbf{v}}
\newcommand{\bV}{\textbf{V}}
\newcommand{\bw}{\textbf{w}}
\newcommand{\bW}{\textbf{W}}
\newcommand{\bu}{\textbf{u}}
\newcommand{\bU}{\textbf{U}}
\newcommand{\by}{\textbf{y}}
\newcommand{\bY}{\textbf{Y}}
\newcommand{\bbx}{\textbf{x}}
\newcommand{\bX}{\textbf{X}}
\newcommand{\beps}{\boldsymbol{\varepsilon}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\aaa}{\`a}
\newcommand{\eee}{\`e}
\newcommand{\iii}{\`i}
\newcommand{\ooo}{\`o}
\newcommand{\uuu}{\`u}
\newcommand{\aaaa}{\'a}
\newcommand{\eeee}{\'e}
\newcommand{\iiii}{\'i}
\newcommand{\oooo}{\'o}
\newcommand{\uuuu}{\'u}
\newcommand{\AAA}{\`A}
\newcommand{\EEE}{\`E}
\newcommand{\III}{\`I}
\newcommand{\OOO}{\`O}
\newcommand{\UUU}{\`U}
\newcommand{\AAAA}{\'A}
\newcommand{\EEEE}{\'E}
\newcommand{\IIII}{\'I}
\newcommand{\OOOO}{\'O}
\newcommand{\UUUU}{\'U}
\newcommand{\ND}{\mathcal{ND}}
\newcommand{\RT}{\mathcal{RT}}
%% argmin argmax
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


%%% theorems
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\renewcommand\qedsymbol{$\blacksquare$}

%%% norm and abs
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\abs[1]{\left\vert#1\right\vert}
\title{Signorini problem: adaptive least/squares }
\author{Gabriele Rovi}
%\date{}							% Activate to display a given date or no date


\begin{document}
\maketitle 
\section{Theorem (Badea, 2013)} 
\begin{enumerate}[label=\Roman*.]
\item $V$ reflexive Banach space, $K \subset V$ non empty closed convex set. Let $F: V\to \mathbb{R}$ be a Gateaux differentiable functional which is:
\begin{itemize}
\item coercive on K: $ F(v) \to \infty$, for $\|v\| \to \infty$
\item $\alpha \| v-u \|^2 \leq \langle F'(v)-F'(u), v-u\rangle \leq \beta \|v -u \|^2 $
\end{itemize}
\item For a given $w \in K$, define $K_j$ for $j=J,...,1$:
\begin{itemize}
\item For $j=J$, $0 \in K_J$, $K_J \subset \{v_J \in V_J: w +v_J \in K \}$
\item For $J-1\geq j \geq 1$, $0 \in K_j$, $K_j \subset \{v_j \in V_j : w_{j+1}+v+j \in K_{j+1}\}$, so that $K_j \subset K_J$
\end{itemize}
\item 
\begin{itemize}
\item $\exists$ $0<\beta_{jk}\leq 1$, $\beta_{jk}=\beta_{kj}$, $j,k=J,...,1$: 
\begin{align*}
\langle F'(v+v_{ij})-F'(v),v_{kl}\rangle \leq \beta \beta_{jk} \|v_{ij} \|  \|v_{kl}\|
\end{align*}
$\forall v \in V$, $V_{ji}\in V_{ji}$, $v_{kl} \in V_{kl}$, $i=1,...,I_j$, $l=1,...,I_k$
\item $\exists \: C_1$:
\begin{align*}
\norm{\sum_{j=1}^J \sum_{i=1}^{I_j} w_{ji}} \leq C_1 \left( \sum_{j=1}^J \sum_{i=1}^{I_j}\| w_{ji}\|^2 \right)^{1/2}\\
\end{align*}
\end{itemize}
\item $\exists$ $C_2>0$ such that $\forall w \in K$, $w_{ji} \in V_{ji}$, $w_{j1}+...+w_{ji} \in K_j$, for $j=J,...,1$, $i=1,...,I_j$, and $u \in K$, there exists $u_{ji} \in V_{ji}$, $j=J,...,1$, $i=1,...,I_j$ which satisfy:
\begin{align*}
u_{j1} \in K_j \qquad w_{j1}+...+w_{ji-1}+&u_{ji}  \in K_j \quad i=2,...,I_j, \: j=J,...,1\\
u-w&=\sum_{j=1}^J \sum_{i=1}^{I_j} u_{ji}
\end{align*}
and
\begin{align*}
\sum_{j=1}^J \sum_{i=1}^{I_j} \| u_{ji} \|^2\leq C_2^2 \left( \|u-w\|^2+\sum_{j=1}^J \sum_{i=1}^{I_j}\| w_{ji}\|^2 \right)^{1/2}\\
\end{align*}
\end{enumerate}
\subsection{Assumption I)}
\subsubsection{Coercivity of the LS functional for homogeneous BC}
$U = \left[ H^1(\Omega) \right]^2 \times \left[H_{\text{div}}(\Omega)\right]^2$ is an Hilbert space, so it is also Banach and reflexive (\textbf{right}?). Furthermore the functional is Gateaux-differentiable, coercive and satisfies the other properties.\\\\
The linear elastic LS functional is quadratic and convex. Then exists a unique minimizer of the problem. Ellipticity and continuity have been studied in CS04 for the homogeneous case.
\begin{align}
\mathcal{F}(\bu,\bsigma,\bff)=||\text{div} \bsigma+\bff||^2+||\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu)||^2
\end{align}
For its generalization to contact, the complementarity has to be considered. Since both stresses and displacements are unknowns of the problem, such a condition is non linear and adding it to the functional is an easier way to deal with it.
\begin{align}
\mathcal{J}(\bu,\bsigma,\bff,g)=||\text{div} \bsigma+\bff||^2+||\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu)||^2+\langle \bu \cdot \bn - g, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C}
\end{align}
The complementarity term is not convex, so it is not obvious if the overall functional is convex:
\begin{align}
&\langle \bn^T (t\bsigma +(1-t)\btau) \bn, (t \bu +(1-t)\bv))\cdot \bn - (gt +(1-t)g)\rangle =\\
&  t  \langle \bn^T \bsigma  \bn, \bu \cdot \bn - g \rangle + (1-t)\langle \bn^T \btau  \bn, \bv \cdot \bn - g \rangle +t (t-1) \langle \bn^T (\bsigma  -\btau) \bn, (\bu-\bv) \cdot \bn   \rangle\\
\end{align}
Here the computations:
\begin{align*}
&\langle \bn^T (t\bsigma +(1-t)\btau) \bn, (t \bu +(1-t)\bv))\cdot \bn - (gt +(1-t)g)\rangle =\\
& \pm t  \langle \bn^T \bsigma  \bn, \bu \cdot \bn - g \rangle \pm (1-t) \langle \bn^T \btau  \bn, \bv \cdot \bn - g \rangle +\\
&t^2 \langle \bn^T \bsigma  \bn, \bu \cdot \bn - g \rangle +(1-t)^2 \langle \bn^T \btau  \bn, \bv \cdot \bn - g \rangle+\\
&t(1-t)  \langle \bn^T \bsigma  \bn, \bv \cdot \bn - g \rangle+t(1-t)  \langle \bn^T \btau  \bn, \bu \cdot \bn - g \rangle=\\
&  t  \langle \bn^T \bsigma  \bn, \bu \cdot \bn - g \rangle + (1-t)\langle \bn^T \btau  \bn, \bv \cdot \bn - g \rangle +\\
&t (t-1) \langle \bn^T \bsigma  \bn, \bu \cdot \bn - g \rangle -t(1-t) \langle \bn^T \btau  \bn, \bv \cdot \bn - g \rangle+\\
&t(1-t)  \langle \bn^T \bsigma  \bn, \bv \cdot \bn - g \rangle+t(1-t)  \langle \bn^T \btau  \bn, \bu \cdot \bn - g \rangle=\\
&  t  \langle \bn^T \bsigma  \bn, \bu \cdot \bn - g \rangle + (1-t)\langle \bn^T \btau  \bn, \bv \cdot \bn - g \rangle +\\
&t (t-1) \langle \bn^T \bsigma  \bn, (\bu-\bv) \cdot \bn   \rangle+t(1-t) \langle \bn^T \btau  \bn,(\bu- \bv) \cdot \bn \rangle=\\
&  t  \langle \bn^T \bsigma  \bn, \bu \cdot \bn - g \rangle + (1-t)\langle \bn^T \btau  \bn, \bv \cdot \bn - g \rangle +t (t-1) \langle \bn^T (\bsigma  -\btau) \bn, (\bu-\bv) \cdot \bn   \rangle\\
\end{align*}
The last term $ \langle \bn^T (\bsigma  -\btau) \bn, (\bu-\bv) \cdot \bn   \rangle$ is not necessarily bigger than zero. Indeed the $(\bsigma-\btau, \bu-\bv)$ does not belong to the convex set anymore and it can be whatever. Therefore the complementarity term is, in general, not convex. \\\\
In the following, it is shown the coercivity of the contact LS-functional for functions with homogeneous boundary conditions, not necessarily belonging to the convex set. Then this result is used to prove the strong convexity, and consequently the coercivity, of the functional.
\subsubsection{Coercivity}
We want to show that exists $C_*$ such that:
\begin{align}
M(\bs,\bw) \leq C_* \mathcal{J}(\bw,\bs,0,0) \qquad \forall \:[\bs,\bu] \in \boldsymbol{\Sigma}_0 \times \bU_0
\end{align}
where:
\begin{align}
&\boldsymbol{\Sigma}_0 =\{\bs \in H_{\text{div}}(\Omega), \bs \bn|_{\Gamma_N}=0 \}\\
&\bU_0 =\{\bu \in H^1(\Omega), \bu |_{\Gamma_D}=0 \}\\
&M(\bs,\bw)= \|  \boldsymbol{\varepsilon}(\bw) \|_{L^2} +  \| \btau \|_{H_{div}}^2 =  \|  \boldsymbol{\varepsilon}(\bw) \|_{L^2}^2 +  \| \bs \|_{L^2}^2 +  \| \text{div} \bs \|_{L^2}^2 
\end{align}
It is sufficient to bound all the terms in $M$ with the functional. This has already been studied in the linear elastic case in CS04. Here we will consider also the contact boundary and give exactly a value for the constant $C_*$. \\
Knowing that $\|\mathcal{A}\btau\| \leq \dfrac{1}{2 \mu} \| \btau \|$:
\begin{align}
 \| \text{div}\bs\|_{L^2}^2  + \| \boldsymbol{\varepsilon}(\bw) \|^2 \leq  \| \text{div}\bs\|_{L^2}^2 +2 \| \boldsymbol{\varepsilon}(\bw) - \mathcal{A}\bs\|^2 + 2 \| \mathcal{A}\bs\|^2  \leq 2 \mathcal{F}(\bs,\bw,0) +  \dfrac{1}{2 \mu }\| \bs\|^2
\end{align} 
Therefore it is now sufficient to bound $\dfrac{2 \mu +1}{2 \mu} \| \bs \|_{L^2} $. We are not going to use the relation   (3.13) in CS04 because I THINK it only holds for homogeneous bc and empty $\Gamma_C$:
\begin{align}
\| \text{div} \bs \|_{-1}= \sup_{\|\bv \|_{H^1}=1} \langle   \text{div} \bs , \bv \rangle = \sup_{\|\bv \|_{H^1}=1}  ( \bs, \nabla \bv ) + \int_{\partial \Omega} \bs \bn \cdot \bv \leq \| \bs \|_{H_{div}}  \quad \iff \quad \text{homogeneous bc} + \Gamma_C=\emptyset
\end{align} 
Therefore we just exploit:
\begin{align}
(\mathcal{A}\bs,\bs )=\dfrac{1}{2 \mu} \left( \| \bs \|^2 -\dfrac{ \lambda }{d \lambda + 2 \mu} \| \text{tr} \bs \|^2 \right) \geq \dfrac{1}{2 \mu}  \| \bs \|^2
\end{align}
Now we just have to show that:
$(2 \mu +1) (\mathcal{A}\bs,\bs ) \leq  C\mathcal{F}(\bw,\bs,0) $:
\begin{align}
(\mathcal{A}\bs,\bs ) &= (\mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw),\bs ) + (\boldsymbol{\varepsilon}(\bw),\bs)=  (\mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw),\bs ) + (\bs - \frac{\bs -\bs^T}{2},\nabla \bw)\\
&=(\mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw),\bs )-(\text{div} \bs,\bw) +\int_{\partial \Omega} \bs \bn \cdot \bw - \left(\bs-\bs^T, \nabla \bw \right)\\ 
& \leq \|  \mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw) \| \| \bs\| +\|\text{div} \bs\| \| \bw \| +\|\frac{\bs-\bs^T}{2}\| \|\nabla \bw \|+\int_{\partial \Omega} \bs \bn \cdot \bw \\
& \leq \mu \|  \mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw) \|^2+    \dfrac{1}{4 \mu} \| \bs\|^2
+  \|\boldsymbol{\varepsilon}(\bw) \| \left( \|\text{div} \bs\| + \|\frac{\bs-\bs^T}{2}\|\right)+\int_{\partial \Omega} \bs \bn \cdot \bw 
\end{align}
And by using $\| \bs -\bs^T\| \leq 4 \mu \|\mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw) \|$, $\| \bs \|^2 \leq 2 \mu (\mathcal{A} \bs, \bs)$, 
$\| \mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw)\| \leq  \mathcal{F}(\bs,\bw,0)^{1/2}$, $\| \bs \| \leq  \mathcal{F}(\bs,\bw,0)^{1/2}$, $\| \bs -\bs^T\| \leq 4 \mu \|\mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw) \|$:
\begin{align}
(\mathcal{A}\bs,\bs ) & \leq 2 \mu \|  \mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw) \|^2+ 2 \left( 1+ 2 \mu \right) \mathcal{F}(\bs,\bw,0)^{1/2}  \|\boldsymbol{\varepsilon}(\bw) \| 
+\int_{\partial \Omega} \bs \bn \cdot \bw 
\end{align}
Now it is exploited again $ \|  \mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw) \|^2\leq \mathcal{F}(\bs,\bw,0) $ and $ \| \boldsymbol{\varepsilon}(\bw) \|  \leq  \mathcal{F}(\bs,\bw,0)^{1/2} + \| \mathcal{A} \bs\| \leq \mathcal{F}(\bs,\bw,0)^{1/2} +  \dfrac{1}{2 \mu }\| \bs\|$, so that:
\begin{align}
(\mathcal{A}\bs,\bs ) & \leq (2 +6 \mu)\mathcal{F}(\bs,\bw,0) 
+\dfrac{\left( 1+ 2 \mu \right) }{\mu}\mathcal{F}(\bs,\bw,0)^{1/2}  \| \bs \| 
+2 \int_{\partial \Omega} \bs \bn \cdot \bw 
\end{align}
By Young's inequality:
\begin{align}
(\mathcal{A}\bs,\bs ) & \leq 2 (2 +6 \mu)\mathcal{F}(\bs,\bw,0) 
+2 \dfrac{ \left( 1+ 2 \mu \right)^2 }{2 \mu^2}\mathcal{F}(\bs,\bw,0)
+4 \int_{\partial \Omega} \bs \bn \cdot \bw 
\end{align}
In the end:
\begin{align}
(\mathcal{A}\bs,\bs ) & \leq 2 (2 +6 \mu)\mathcal{F}(\bs,\bw,0) 
+2 \dfrac{ \left( 1+ 2 \mu \right)^2 }{2 \mu^2}\mathcal{F}(\bs,\bw,0)
+4 \int_{\partial \Omega} \bs \bn \cdot \bw \\
& \leq \dfrac{4 \mu^2 + 12 \mu^3 + 1 + 4 \mu^2 + 4 \mu}{\mu^2} \mathcal{F}(\bs,\bw,0) +4 \int_{\partial \Omega} \bs \bn \cdot \bw 
\end{align}
By rearranging the terms:
\begin{align}
M(\bs,\bw)&=  \|  \boldsymbol{\varepsilon}(\bw) \|_{L^2}^2 +  \| \bs \|_{L^2}^2 +  \| \text{div} \bs \|_{L^2}^2\\
& \leq 2 \mathcal{F}(\bs,\bw,0) +  \dfrac{4 \mu^2 + 12 \mu^3 + 1 + 4 \mu^2 + 4 \mu}{(1+2 \mu) \mu^2}  \mathcal{F}(\bs,\bw,0)  +4 \int_{\partial \Omega} \bs \bn \cdot \bw \\
&\leq C_* \left( \mathcal{F}(\bs,\bw,0) + \int_{\partial \Omega} \bs \bn \cdot \bw\right) 
\end{align} 
The presence of the boundary integral is, in principle, not trivial. Indeed this term could be bounded thanks to Cauchy-Schwarz and trace inequalities. Nevertheless in this way both the trace constant for $H_{-1/2}$ and $H_{1/2}$ would appear, together with Korn's constant:
\begin{align}
\int_{\partial \Omega} \bs \bn \cdot \bw \leq C_S C_W  \| \bs \|_{H_{div}} \|(\bw) \|_{H^1} \leq C_s C_w   \left(  \dfrac{\varepsilon}{2} \|\bs\|_{H_{div}}^2 +  \dfrac{C_K^2}{2 \varepsilon} \|\boldsymbol{\varepsilon}(\bw) \|^2
\right)
\end{align}
By putting this last contribution to the left handside:
\begin{align}
 \left(1 - \dfrac{C_* C_S C_W C_K^2 \varepsilon}{2} \right) \|  \boldsymbol{\varepsilon}(\bw) \|^2 +\left(1 - \dfrac{C_* C_S C_W }{2  \varepsilon} \right)   \| \bs \|_{ H_{div}}^2  \leq C_* \mathcal{F}(\bs,\bw,0)
\end{align}
the above relation is satisfied for:
\begin{align}
\varepsilon \in (\dfrac{C_S C_W C_*}{2}, \dfrac{2}{C_S C_W C_* C_K^2})
\quad \text{with} \quad 
C_K< \left( \dfrac{2}{C_S C_W C_*} \right)
\end{align}
Unfortunately $C_S$, $C_W$ and $C_K$ are not known, so it is not possible to infere that this condition is fulfilled. \\
Therefore, in addition to Neumann and Dirichlet boundary conditions on $\Gamma_{N}$ and $\Gamma_{D}$, it is also useful to enforce the frictionless condition on $\Gamma_C$, i.e. $\bsigma_t|_{\Gamma_C}=0$. This means that, on the contact boundary: $\bsigma \bn = \sigma_n \bn_{obst}$, while $\bu  = u_n \bn_{obst} + u_t \bt_{obst} $, where $\bn_{obst}$ and $\bt_{obst}$ represent the normal and tangent vectors of the obstacle.  By taking advantage of orthogonality between normal and tangent components:
 \begin{align}
\langle\bs \bn, \bw\rangle_{\Gamma_C} =
\langle \left( s_n \bn_{obst} + \bt_{obst} s_t \right), \left( w_n \bn_{obst} + \bt_{obst} w_t \right)\rangle_{\Gamma_C} =
\langle s_n  ,w_n  \rangle_{\Gamma_C} 
  \end{align} 
Finally:
\begin{align}
\begin{cases}
M(\bs,\bw)  \leq C_* \mathcal{J}(\bs,\bw,0,0)\\
C_* =\text{max} \left(4,   \dfrac{  1 + 4 \mu + 10 \mu^2 + 16 \mu^3} {(1+2 \mu) \mu^2} \right) 
\end{cases}
\end{align}

\subsubsection{Convexity of the LS contact functional}
However the overall functional could still be convex. Indeed it is strongly convex, i.e:
\begin{align}
\mathcal{J}( t \bu + (1-t) \bv, t \bsigma + (1-t) \btau, \bff,g) \leq t \mathcal{J}( \bu ,\bsigma , \bff) + (1-t)\mathcal{J}(  \bv,  \btau, \bff,g)- \dfrac{1}{C_*} \:t (t-1) M(\bsigma-\btau,\bu-\bv)
\end{align}
Indeed, by properly rearraging the terms:
\small
\begin{align}
\mathcal{J}( t \bu + (1-t) \bv, t \bsigma + (1-t) \btau, \bff,g)&= t \mathcal{J}( \bu ,\bsigma , \bff,g) + (1-t)\mathcal{J}(  \bv,  \btau, \bff,g)\\
& +t (t-1) \left[ \| \text{div} (\bsigma - \btau )\|^2 + \|\mathcal{A}(\bsigma-\btau)-\boldsymbol{\varepsilon}(\bu-\bv)\|^2 +  \langle \bn^t (\bsigma -\btau) \bn, (\bu-\bv)\cdot \bn\rangle \right]\\
&= t \mathcal{J}( \bu ,\bsigma , \bff,g) + (1-t)\mathcal{J}(  \bv,  \btau, \bff,g)+ t(t-1) \mathcal{J}(\bu-\bv,\bsigma-\btau,0,0)\\
& \leq t \mathcal{J}( \bu ,\bsigma , \bff) + (1-t)\mathcal{J}(  \bv,  \btau, \bff)+ t(t-1)\dfrac{1}{C_*} M(\bu-\bv,\bsigma-\btau)
\end{align}
\normalsize
where the last inequality holds because $\bw=\bu-\bv$ and $\bs=\bsigma-\btau$ satisfy both homogeneous boundary conditions. It is also known the $\mathcal{J}(   t \bu + (1-t) \bv, t \bsigma + (1-t) \btau, \bff,g)\geq0$ $\forall \bsigma, \btau, \bu, \bv$ in the adimissible set. Then, by  fixing $t \in (0,1)$ and chossing $\bv$, $\btau$ as the minimizer of the problem, so that $\mathcal{J}(  \bv,  \btau, \bff,g)=0$:
\begin{align*}
\dfrac{C_*}{1-t}  \mathcal{J}( \bu ,\bsigma , \bff)& \geq  M(\bu-\bv,\bsigma-\btau) =\\
&( \|  \boldsymbol{\varepsilon}(\bu ) \|_{L^2}^2  +  \|  \boldsymbol{\varepsilon}(\bv ) \|_{L^2}^2 -2 \| \boldsymbol{\varepsilon}(\bu )\|_{L^2} \| \boldsymbol{\varepsilon}(\bv )\|_{L^2} +
  \| \btau \|_{H_{div}}^2+ \| \bsigma \|_{H_{div}}^2 - 2  \| \btau \|_{H_{div}}  \| \bsigma \|_{H_{div}}) \to \infty
\end{align*}
The right handside is a quadratic expression in $ \| \bsigma \|_{H_{div}}$ and $\|  \boldsymbol{\varepsilon}(\bu ) \|_{L^2}$. Note that $\bu$ and $\bsigma$ satisfy in general non-homogeneous boundary conditions. Since the functional $ \mathcal{J}( \bu ,\bsigma , \bff)$ is strongly convex and coercive, exists a unique minimizer.
\subsubsection{Other property}
Since $F'$ is linear, not only in the increment, but also in the point in which it is evaluated:
 $\alpha \| v-u \|^2 \leq \langle F'(v-u), v-u\rangle \leq \beta \|v -u \|^2 $:
 \begin{align*}
  \langle \mathcal{J}'(\bu,\bsigma,\bff,g)-\mathcal{J}'(\bv,\btau,\bff,g)&, [\bsigma-\btau,\bu-\bv]\rangle =\\
 & =\norm{\mathcal{A}(\bsigma-\btau)-\boldsymbol{\varepsilon}(\bu-\bv)}^2 +\norm{\text{div}(\bsigma-\btau)}^2 +2 \langle \bn^T (\bsigma-\btau)\bn, (\bu-\bv)\cdot \bn \rangle_{\Gamma_C}\\
 & \approx \mathcal{J}(\bu-\bv,\bsigma-\btau,0,0)
 \end{align*}
The approx symbol is due to the presence of the 2. But I have to check. Anyhow it is not important.
Therefore recovering the proof for the coercivity, we also can show that $\alpha \| v-u \|^2 \leq \langle F'(v-u), v-u\rangle $. For the other inequality, we just use  the continuity:
\begin{align*}
&\norm{\mathcal{A}(\bsigma-\btau)-\boldsymbol{\varepsilon}(\bu-\bv)}^2  \leq 2 \left( \norm{\mathcal{A}(\bsigma-\btau)}^2+\norm{\boldsymbol{\varepsilon}(\bu-\bv)}^2\right)\leq \dfrac{1}{2 \mu^2}\norm{\bsigma-\btau}^2+ 2 \norm{\boldsymbol{\varepsilon}(\bu-\bv)}^2\\
&\norm{\mathcal{A}(\bsigma-\btau)-\boldsymbol{\varepsilon}(\bu-\bv)}^2+\norm{\text{div}(\bsigma-\btau)}^2 \leq 2 \max \left(\dfrac{1}{2 \mu^2},1 \right) \left(\norm{\bsigma -\btau}_{H_{\text{div}}}^2 +  \norm{\boldsymbol{\varepsilon}(\bu-\bv)}^2\right)
\end{align*}
Furthermore:
\begin{align*}
2 \langle \bn^T (\bsigma-\btau)\bn, (\bu-\bv)\cdot \bn \rangle_{\Gamma_C} &\leq2 \norm{(\bsigma-\btau)\bn}_{H^{-1/2}(\Gamma_C)} \norm{\bu-\bv}_{H^{1/2}(\Gamma_C}\\
& \leq 2C_{S}C_W \norm{\bsigma-\btau}_{H_{\text{div}}(\Omega)}\norm{\bu-\bv}_{H^{1}(\Omega)}\\
&\leq C_{S}C_W \left(\norm{\bsigma-\btau}_{H_{\text{div}}(\Omega)}^2+\norm{\bu-\bv}_{H^{1}(\Omega)}^2 \right)\\
&\leq C_{S}C_W \max\left(C_K,1 \right)\left(1+C_P \right)\left(\norm{\bsigma-\btau}_{H_{\text{div}}(\Omega)}^2+\norm{ \boldsymbol{\varepsilon}(\bu-\bv)}_{L^2(\Omega)}^2 \right)
\end{align*}
where $C_S$ and $C_W$ are the trace constants, $C_K$ is the Korn's constant and $C_P$ is the Poincare's constant (since $bu-\bw|_{\Gamma_D}=0$ we can apply the theorem).

\subsection{Assumption II)}
In order to properly describe contact conditions, it is wise to locally change the coordinate system of the contact boundary $\Gamma_C$. In this way, the scalar constraints have to be checked directly on the normal components and not on some linear combinations of the unknown.\\ In the following the contact normal is assumed to be known in all vertices and all midpoints of the faces belonging to $\Gamma_C$.
Let $\bp$ be a vertex of the tasselation $\emph{T}_h$ that also belongs to $\Gamma_C$ and $\bn$ the obstacle normal in $\bp$. Then consider the vector $ \bu_{\bp} \in \mathbb{R}^d$ that contains the degrees of freedom of the displacement in $\bp$. Define the Householder transformation relative to the ouward normal $\bn_{\bp} $ as:
\begin{align}
\bH_{\bp}= \bI- 2\: \bn_{\bp} ^T \bn_{\bp} 
\end{align}
and the local displacement in the normal-tangent coordinate system (the first coordinate is the normale one):
\begin{align*}
\bu_{\bp,nt}= \bH_{\bp} \bu_{\bp}
\end{align*}
A similar argument has to be applied to the stress components. For each face $F_i \in \Gamma_C$, we can express each vector unknown $\bC_i$ in terms of the normal and tangent forces. To this aim, we define the external force $\bsigma \bn_{ext} $ that is then rewritten in terms of the normal and tangent component $\bS_i$ thanks to the Householder transformation:
\begin{align*}
\bsigma \bn_{ext} = \left( \bphi_i \cdot \bn_{i,ext} \right) \bC_i = \bH_i \bS_i \quad \iff \quad \bC_i = \dfrac{1}{ \left( \bphi_i \cdot \bn_{i,ext} \right)}\bH_i \bS_i =\pm \bH_i \bS_i = \bQ_i \bS_i
\end{align*}
It is not convenient to use direclty the HouseHolder transformation, because we have no control on the sign of $\left( \bphi_{\bp}  \cdot \bn_{\bp}  \right)$. In its place, it is preferrable the transformation $\bQ$.\\
In this way the first component of $\bS_{\bp,nt}$ is actually positive in the direction of the normal $\bn_{\bp}$ and, as $ \left( \bphi_{\bp}  \cdot \bn_{\bp}  \right) =\pm 1$, the transformation $\bQ$ is still orthogonal. \\
From now on all the degrees of freedom on the contact boundary will be treated as normal or tangent. In this way, we can define $\bPhi_i$, which is the tensor whose rows are the $\RT_0$ shape functions on the face i, and the corresponding $\bPsi_i$:
\begin{align*}
\bPhi_i =
\begin{bmatrix}
\bphi_i\\
\bphi_i\\
\bphi_i\\
\end{bmatrix}
\quad
\bPsi_i =
\begin{bmatrix}
\bpsi_i\\
\bpsi_i\\
\bpsi_i\\
\end{bmatrix}
\quad
\bPsi_i=\begin{cases}
\bPhi_i \bQ_i & i \in \Gamma_C \\
\bPhi_i   & \text{otherwise}
\end{cases}
\end{align*}
Since $\dfrac{\partial \phi_v}{\partial w}=0$ for $w,v=x,y,z$, $w \neq v$, we also notice that:
\begin{align*}
&[\bPsi]_{r,s}
= \sum_{w=x,y,z} \phi_w \: q_{w, s}
\quad & r,s=x,y,z
\\
&\text{div} \left(\bPsi
 \right)_r
 =\sum_{s=x,y,z}\sum_{w=x,y,z} \dfrac{\partial \phi_w}{\partial w} \: q_{w, s}=\sum_{w=x,y,z} \dfrac{\partial \phi_w}{\partial w} \: q_{w, w} & r=x,y,z
\end{align*}
and consequently:
\begin{align*}
|\text{div} \bpsi |=\abs{\sum_{w=x,y,z} \dfrac{\partial \phi_w}{\partial w} \: q_{w, s}}=
 \abs{\dfrac{\alpha}{H}  \sum_{w=x,y,z}q_{w, w}} \leq 
 \abs{\dfrac{d}{H}} =\abs{\text{div} \bphi }
\end{align*}
Now we can redefine the functions in a way such that on $\Gamma_C$ the unknown vector describes directly the normal and the tangent components:
\begin{align*}
\bsigma=\sum_{F_i} \bPhi_i \bC_i = \sum_{F_i} \bPsi_i \bS_i 
\end{align*} 
Then the check on the normal component is easy because it is just a scalar equation:
\begin{align*}
&S_i^1 \leq \phi_i \quad \text{on}\: \Gamma_C
\end{align*}
We know define the convex sets $K_j$ recurisvely.
For level $J$:
\begin{align}
&\phi_J=\phi-w\\
&K_J=[-\infty, \phi_J]=\{ v\in V_{h_j}:  v \leq \phi_J\}\\
&\text{and consider}\: w_J \in K_J
\label{convexsetdefinitionJ}
\end{align}
At level $j=J-1,...,1$:
\begin{align}
&\phi_j=I_{h_j} (\phi_{j+1}-w_{j+1})\\
&K_j=[-\infty, \phi_j]=\{ v\in V_{h_j}:  v \leq \phi_j\}\\
&\text{and consider}\: w_j \in K_j
\label{convexsetdefinitionj}
\end{align}
\textbf{Proposition}\\
The assumption on the convex sets holds for $j=1,...,J$.\\
Evidently $ 0\in K_j$, $\forall j=1,...,J$. Then $w+v_J\in K_J$ by definition. For the lower levels, by definition the first inequality:
\begin{align*}
S^1 \leq \phi_j = I_{h_j} (\phi_{j+1}-w_{j+1})\leq \phi_{j+1}-w_{j+1}
\end{align*}
So $K_j \subset K_{j+1}$.

\subsection{Assumption III)}
\begin{enumerate}
\item With $\beta_{jk}=1$:
\begin{align*}
\langle F'(v+v_{ij})-F'(v),v_{kl}\rangle =\langle F'(v_{ij}),v_{kl}\rangle \leq
\beta  \|v_{ij} \|  \|v_{kl}\|
\end{align*}
\item Consider a color $m$ and all $i \in m$. Then it holds:
\begin{align*}
\norm{ \sum_{ i \in m} w_{ji}}^2 = \sum_{i\in m}  \|w_{ji}\|^2 
\end{align*}
Then $C_1=\sqrt{m_j}$:
\begin{align*}
\norm{\sum_{i=1}^{I_j} w_{ji}}^2 =\norm{\sum_{m=1}^{m_j}  \sum_{i\in m}  w_{ji}}^2 \leq m_j \sum_{m=1}^{m_j} \norm{  \sum_{i\in m}  w_{ji}}^2 =   m_j \sum_{m=1}^{m_j} \sum_{i\in m}  \|w_{ji}\|^2 =\sum_{i=1}^{I_j} \norm{w_{ji}}^2 =
\end{align*}
And therefore:
\begin{align*}
\norm{\sum_{j=1}^J \sum_{i=1}^{I_j} w_{ji}}^2\leq
J \max_{j=1,...,J} m_j \sum_{j=1}^J 
 \sum_{i=1}^{I_j}
  \norm{w_{ji}}^2 
\end{align*}
\end{enumerate}

\subsection{Assumption IV)}
\subsubsection{Non-Linear Interpolation Operator $\RT_0$}
In the following, a non-linear interpolation for $\RT_0$ is introduced for the contact problem. In particular two kinds of degrees of freedom have to be distinguished: the ones relative to the normal component on $\Gamma_C$ and the others.\\
For the first ones, more conditions are required regarding the constraints. Anyhow also on the second ones are required some conditions. Indeed we have to fulfill the stability condition that involve the $H_{div}$ norm.\\
Since on $\Gamma_C$ the constraints regard only the normal direction, it is easier to locally change the coordinate system, from cartesian to the normal-tangents one. In this way, the first component, i.e. the normal one, will take into consideration the normal constraint, while the others will not care.\\
For each face $F_i \in \Gamma_C$, we can express each vector unknown $\bC_i$ in terms of the normal and tangent forces. To this aim, we define the external force $\bsigma \bn_{ext} $ that is then rewritten in terms of the normal and tangent component $\bS_i$ thanks to the Householder transformation:
\begin{align*}
\bsigma \bn_{ext} = \left( \bphi_i \cdot \bn_{i,ext} \right) \bC_i = \bH_i \bS_i \quad \iff \quad \bC_i = \dfrac{1}{ \left( \bphi_i \cdot \bn_{i,ext} \right)}\bH_i \bS_i =\pm \bH_i \bS_i = \bQ_i \bS_i
\end{align*}
In this way, we can define $\bPhi_i$, which is the tensor whose rows are the $\RT_0$ shape functions on the face i, and the corresponding $\bPsi_i$:
\begin{align*}
\bPhi_i =
\begin{bmatrix}
\bphi_i\\
\bphi_i\\
\bphi_i\\
\end{bmatrix}
\quad
\bPsi_i =
\begin{bmatrix}
\bpsi_i\\
\bpsi_i\\
\bpsi_i\\
\end{bmatrix}
\quad
\bPsi_i=\begin{cases}
\bPhi_i \bQ_i & i \in \Gamma_C \\
\bPhi_i   & \text{otherwise}
\end{cases}
\end{align*}
Since $\dfrac{\partial \phi_v}{\partial w}=0$ for $w,v=x,y,z$, $w \neq v$, we also notice that:
\begin{align*}
&[\bPsi]_{r,s}
= \sum_{w=x,y,z} \phi_w \: q_{w, s}
\quad & r,s=x,y,z
\\
&\text{div} \left(\bPsi
 \right)_r
 =\sum_{s=x,y,z}\sum_{w=x,y,z} \dfrac{\partial \phi_w}{\partial w} \: q_{w, s}=\sum_{w=x,y,z} \dfrac{\partial \phi_w}{\partial w} \: q_{w, w} & r=x,y,z
\end{align*}
and consequently:
\begin{align*}
|\text{div} \bpsi |=\abs{\sum_{w=x,y,z} \dfrac{\partial \phi_w}{\partial w} \: q_{w, s}}=
 \abs{\dfrac{\alpha}{H}  \sum_{w=x,y,z}q_{w, w}} \leq 
 \abs{\dfrac{d}{H}} =\abs{\text{div} \bphi }
\end{align*}
Let us now consider a coarse face $F_i^H$ and all the fine faces $F_i^h \subset F_i^H$. Then we define $\sigma|_{F_i}^{+}$/$\sigma|_{F_i}^{-}$ as the set of all non negative/positive degrees of freedom belonging to that face: 
\begin{align*}
\sigma|_{F_i^H}^{+}=\{ \sigma|_{F_i^h}:  \sigma|_{F_i^h} \bpsi_i \cdot \bn_i^H\geq 0 \} \quad
\sigma|_{F_i^H}^{-}=\{ \sigma|_{F_i^h}:  \sigma|_{F_i^h} \bpsi_i \cdot \bn_i^H\leq 0 \}
\end{align*}
We want to define a non-linear operator $\bI_{h_j}: V_{j+1} \to V_j$ such that:
\begin{align}
 &0\leq  \left( \bI_{h_j}  \bsigma \right)|_{F_i^H} \leq \bsigma|_{F_i^h} \quad 0\leq  \bsigma|_{F_i^h}, \: F_i^h\in F_i^H \cap \Gamma_C\\
& 0\geq \left( \bI_{h_j}  \bsigma \right)|_{F_i^H} \geq \bsigma|_{F_i^h} \quad 0\geq  \bsigma|_{F_i^h}, \: F_i^h\in F_i^H \cap \Gamma_C\\
&\forall  \bsigma,\: \btau \: \in K_{j+1},\quad 
\bsigma-\bI_{h_j}(\bsigma-\btau) \in K_{j+1} 
\label{convex_combination}\\
&\| \bI_{h_j}\bsigma  \|_{H_{div}} \leq C \| \bsigma \|_{H_{div}} 
\end{align}
Let us write:
\begin{align*}
\displaystyle
&\forall F_i^H \in \Gamma_C: \quad
\begin{cases}
\left(\bI_{h_j} \bsigma \right)   |_{F_i^H} =\left( \bI_{h_j}^+ \bsigma \right)|_{F_i^H}-\left( \bI_{h_j}^- \bsigma \right)|_{F_i^H} \\
\left( \bI_{h_j}^+ \bsigma \right)|_{F_i^H} =\min\left( \sigma|_{F_i^H}^{+}, \:C
\max \limits_{K_i^h \subset K_i^H}{ \dfrac{|\sum_{F_r^h K_i^h} \gamma_r^h S_r^h|}{|F_i^H|} } \right) \\
\left( \bI_{h_j}^- \bsigma \right)|_{F_i^H} =\min\left( -\sigma|_{F_i^H}^{-}, \: C
\max \limits_{K_i^h \subset K_i^H}{ \dfrac{|\sum_{F_r^h K_i^h} \gamma_r^h S_r^h|}{|F_i^H|} } \right) \\
\end{cases}
\\
& 
\forall F_i^H \notin \Gamma_C:\quad
\begin{cases}
\left(\bI_{h_j} \bsigma \right)   |_{F_i^H} =\left( \bI_{h_j}^+ \bsigma \right)|_{F_i^H}-\left( \bI_{h_j}^- \bsigma \right)|_{F_i^H} \\
\left( \bI_{h_j}^+ \bsigma \right)|_{F_i^H} =\min\left(\max \sigma|_{F_i^H}^{+},  \:C
\max \limits_{K_i^h \subset K_i^H}{ \dfrac{|\sum_{F_r^h K_i^h} \gamma_r^h S_r^h|}{|F_i^H|} } \right) \\
\left( \bI_{h_j}^- \bsigma \right)|_{F_i^H} =\min\left( -\max \sigma|_{F_i^H}^{-},  C
\max \limits_{K_i^h \subset K_i^H}{ \dfrac{|\sum_{F_r^h K_i^h} \gamma_r^h S_r^h|}{|F_i^H|} } \right) \\
\end{cases}
\\
&\bI_{h_j} \bsigma =\sum_{F_i^H} \bpsi_i  \: \left(\bI_{h_j} \bsigma \right) |_{F_i^H} 
\end{align*}
\textbf{FIRST TWO CONDITIONS:\\
}Let us verify the first two conditions. By definition, if at least one $\sigma|_{F_i^h}$, for $F_i^h \in F_i^H$, attains the zero value or there are two $\sigma|_{F_i^h}$ with opposite signs, then $\bI_{h_j} \bsigma |_{F_i}=0$. This case is trivial, so let us consider the two,
Assuming $\sigma|_{F_i^h}=\sigma|_{F_i^h}^+\geq 0$, by definition $\bI_{h_j} \bsigma |_{F_i}=\bI_{h_j}^+ \bsigma |_{F_i}\leq \sigma|_{F_i^h}^+$; while if  $\sigma|_{F_i^h}=\sigma|_{F_i^h}^-\leq 0$, $\bI_{h_j} \bsigma |_{F_i}=-\bI_{h_j}^- \bsigma |_{F_i}\leq \sigma|_{F_i^h}^-$.\\
Due to the two above properties, it is possible to define $\forall \: F_i^H$:
\begin{align*}
\theta\left( \bsigma \right)|_{F_i^h}=
\begin{cases}\dfrac{\left(\bI_{h_j} \bsigma \right)   |_{F_i^H} }{ \sigma |_{F_i^h} } & \sigma |_{F_i^h} \neq 0
\\
0 & \sigma |_{F_i^h}=0
\end{cases}
 \qquad \forall F_i^h \subset F_i^H
\end{align*}
which satisfies $ 0 \leq \theta\left( \bsigma \right)|_{F_i^h} \leq 1$. \\
\textbf{CONVEX COMBINATION CONDITION}\\
\begin{align*}
&\forall  \bsigma,\: \btau \: \in K_{j+1},\quad 
\bsigma-\bI_{h_j}(\bsigma-\btau) \in K_{j+1} 
\end{align*}
This is equivalent to:
\begin{align*}
\bsigma|_{F_i^h}-\bI_{h_j}(\bsigma-\btau)|_{F_i^h}
\end{align*}
Then:
\begin{align*}
&\bsigma|_{F_i^h}-\theta(\bsigma-\btau)|_{F_i^h}\left(\bsigma-\btau)\right)=(1-\theta(\bsigma-\btau)|_{F_i^h})\bsigma|_{F_i^h}+ \theta(\bsigma-\btau)|_{F_i^h}\btau
\end{align*}
that is a convex combinatation of $\bsigma \in K_{j+1}$ and $\btau \in K_{j+1}$. Since this argument can be applied for each fine face which belong to $\Gamma_C$ and since there are no constraints for the interior or the Neumann boundary, then $\bsigma-\bI_{h_j}(\bsigma-\btau) \in K_{j+1} $.\\
\textbf{Bound of the $H_{div}$ norm}\\
Exploiting the fact that $\RT_0$ is a FE space and that the degrees of freedom of the coarse projection are bounded by the original fine function:
\begin{align*}
\norm{\bI_{h_j}  \bsigma}_{L^2(K_i^H)}^2 \leq C h^{-d} \abs{\left(\bI_{h_j} \bsigma\right)|_{K_i^H}}^2 \leq C h^{-d} \abs{\bsigma|_{K_i^H}}^2 \leq C\norm{ \bsigma}_{L^2(K_i^H)}^2
\end{align*}
In addition:
\begin{align*}
\norm{\text{div}\left(\bI_{h_j}  \bsigma\right)}_{L^2(K_i^H)}^2 &=\int_{K_i^H}\left( \sum_{F_i^H \in K_i^H}  \left( \bI_{h_j}  \bsigma \right)|_{F_i^H}  \text{div}\bpsi_i \right)^2\\
&=
\int_{K_i^H}\left(\sum_{F_i^H \in K_i^H} \left( \bI_{h_j}  \bsigma \right)|_{F_i^H}\dfrac{ \alpha_i }{H_i}\sum_{w}q_{w,w} \right)^2\\
&=
\int_{K_i^H}   \left(\sum_{w}q_{w,w}\right)^2(d+1)  \sum_{k=1}^{d+1} \left(\dfrac{   \left( \bI_{h_j}  \bsigma \right)|_{F_i^H} }{H_i} \right)^2\\
&\leq
\dfrac{(d+1)}{|K_i^H|}  \sum_{k=1}^{d+1}  \left( \left( \bI_{h_j}  \bsigma \right)|_{F_i^H}    \abs{F_i^H} \right)^2\\
&\leq
\dfrac{(d+1)}{|K_i^H|}  \sum_{k=1}^{d+1}  \left( \left( \bI_{h_j}  \bsigma \right)|_{F_i^H}    \abs{F_i^H} \right)^2\\
\end{align*}
By definition, if $\forall {F_i^h} \subset F_i^H$ the divergence of $ \bsigma|_{F_i^h}$ is zero, then $\bI_{h_j} \bsigma |_{F_i^H}=0$, $\forall {F_i}^H \in K_i^H$. The bound is then obvious. Similarly
if at least one $\sigma|_{F_i^h}$, for $F_i^h \in F_i^H$, attains the zero value or there are two $\sigma|_{F_i^h}$ with opposite signs, then $\bI_{h_j} \bsigma |_{F_i}=0$.This case is trivial as well. \\
So let us consider the one for which $\bI_{h_j} \bsigma |_{F_i}=\bI_{h_j}^+ \bsigma |_{F_i}$ or $\bI_{h_j} \bsigma |_{F_i}=\bI_{h_j}^- \bsigma |_{F_i}$. Then:
\begin{align*}
C 
 \max_{K_i^h \in K_i^H }  \dfrac{\abs{\text{div} \bsigma|_{K_i^h}}}{\abs{F_i^H}}  \geq \abs{ \left(\bI_{h_j}^+ \bsigma \right)|_{F_i^H} }=
\begin{cases}
\abs{\min \sigma|_{F_i^H}^{+} } \\
C \max_{K_i^h \in K_i^H }  \dfrac{\abs{\text{div} \bsigma|_{K_i^h}}}{\abs{F_i^H}} 
 \end{cases}
\end{align*}
We now use:
\begin{align*}
\norm{\bsigma }_{L^2(K_i^H)}^2=\sum_{K_i^h \subset K_i^H} \int_{K_i^h} \left( \text{div} \bsigma \right)^2
&=
\sum_{K_i^h \subset K_i^H} \int_{K_i^h} \left( \sum_{F_r^h K_i^h}  \text{div}\bpsi_r S_r^h \right)^2\\
&=
\sum_{K_i^h \subset K_i^H} \int_{K_i^h} \left( \sum_{F_r^h K_i^h}  \dfrac{\alpha_r S_r^h}{H_r}  \sum_{w=x,y,z}q_{w, w}
 \right)^2
 \\=&
 \sum_{K_i^h \subset K_i^H} \int_{K_i^h} \dfrac{1}{(K_i^h)^2}
 \left( \sum_{F_r^h K_i^h}
\dfrac{\alpha_r L_r S_r^h}{d}  \sum_{w=x,y,z}q_{w, w}
 \right)^2
  \\=&
 \sum_{K_i^h \subset K_i^H} \int_{K_i^h} \dfrac{1}{(K_i^h)^2}
 \left( 
 \sum_{F_r^h K_i^h} \gamma_r^h S_r^h 
 \right)^2
   \\=&
 \sum_{K_i^h \subset K_i^H}  \dfrac{C(d)}{\abs{K_i^H}}
 \left( \text{div} \bsigma|_{K_i^h}
 \right)^2
\end{align*}
where $C(d)$ is a funtiond depending on the dimension of the problem:
\begin{align*}
C(d)=
\begin{cases}
4 &d=2\\
8 &d=3
\end{cases}
\end{align*}
Going back to the previous calculations:
\begin{align*}
\norm{\text{div}\left(\bI_{h_j}  \bsigma\right)}_{L^2(K_i^H)}^2 & \leq
\dfrac{(d+1)}{|K_i^H|}  \sum_{k=1}^{d+1}  \left( \left( \bI_{h_j}  \bsigma \right)|_{F_i^H}    \abs{F_i^H} \right)^2\\
& \leq
\dfrac{(d+1)}{|K_i^H|}  \sum_{k=1}^{d+1}  \left(C 
 \max_{K_i^h \in K_i^H }  \text{div}  \bsigma|_{K_i^h}   \right)^2\\
 & \leq
\dfrac{(d+1)^2 C^2 }{|K_i^H|} 
 \sum_{K_i^h \in K_i^H }   \left( \text{div}  \bsigma|_{K_i^h}   \right)^2
 \\
  & =
\bar{C}^2
 \sum_{K_i^h \in K_i^H }   \left( \text{div}  \bsigma|_{K_i^h}   \right)^2
 \\
  &=
\bar{C}^2
\norm{\text{div}  \bsigma }_{L^2(K_i^H)}^2 
 \\
\end{align*}
where $\bar{C}^2=\dfrac{(d+1)^2 C^2 }{C(d)} $. Here the constant $C$ is arbitrary, but fixed. \\
Therefore we proved that:
\begin{align*}
\norm{\bI_{h_j}  \bsigma}_{H_{div}(K_i^H)}^2 \leq\norm{\bsigma }_{H_{div}(K_i^H)}^2 \: \forall K_i^H \in T_H \quad \to \quad 
\norm{\bI_{h_j}  \bsigma}_{H_{div}(\Omega^H)}^2 \leq\norm{\bsigma }_{H_{div}(\Omega^H)}^2 
\end{align*}
We want to stress out the fact that, for having a bound of the type:
\begin{align*}
\norm{\text{div}\left(\bI_{h_j}  \bsigma\right)}_{L^2(K_i^H)}^2 \leq C \norm{\text{div}  \bsigma }_{L^2(K_i^H)}^2 =0
\end{align*}
it is sufficient $\text{div}\left(\bI_{h_j}  \bsigma\right)=0$.  So it is not necessary to enforce to zero all the degrees of freedom of $\left(\bI_{h_j}  \bsigma\right)$ on $K_i^H$. Nevertheless in order to apply $\text{div}\left(\bI_{h_j}  \bsigma\right)=0$, it is required to already know the values of all the degrees of freedoms of the given element $K_i^H$. Since each face communicates with other elements, it would be necessary to solve a minimization problem globally and not locally. However it is preferrable to maintain the locality of the interpolation operator.
\subsubsection{Non-Linear interpolation Operator for $P^1$ }
As for the $\RT_0$, the assumptions of the theorem have to be valid also for $P^1$ functions. In this case, the kind of norm we are interested in is the following:
\begin{align*}
\norm{\boldsymbol{\varepsilon}\left(\bI_{h_j}(\bu_{ji})\right)} \leq C
\norm{\boldsymbol{\varepsilon}(\bu_{ji})}
\end{align*}
Ok qui non sono sicuro, ma almeno per i patch, ho bc omogenee di dirichlet  e vale la disuguaglianza di Korn. Poi qui riciclo l'operatore di Badea perche' senno' muoio. Allora vale la disuguaglianza sulla seminorma H1:
\begin{align*}
\norm{\boldsymbol{\varepsilon}\left(\bI_{h_j}(\bu_{ji})\right)} \leq C \norm{\nabla\left(\bI_{h_j}(\bu_{ji})\right)}
\leq C \norm{\nabla \bu_{ji}} \leq C
\norm{\boldsymbol{\varepsilon}(\bu_{ji})}
\end{align*}
Actually I use this inequality for global function, i.e. the $u_j=v_j - v_{j-1}=v_j - I_{h_j}(v_j - w_j)$. So we must enforce proper homogeneous dirichlet boundary conditions on the lower levels. We know that $v_j$ and $w_j$ belong to $K_j$, so $v_j-w_j$ is zero on $\Gamma_D$. 
\subsection{Computations}
Given $u,v \in K_J$ and $w_j \in K_j$, ow we define:
\begin{align*}
&v_J=u-w\\
&v_j=I_{h_j}(v_{j+1}-w_{j+1}) \quad j=J-1,...,1\\
&u_j=v_j-v_{j-1}=v_j-I_{h_{j-1}}(v_j-w_j) \quad j=J,...,2\\
&u_1=v_1=I_{h_1}(v_2-w_2)
\end{align*}
\textbf{Lemma}\\
If $K_j$ are defined as in and also $v_j$ and $u_j$ are defined as in (\ref{convexsetdefinitionJ}) and (\ref{convexsetdefinitionj}), then:
\begin{align*}
&u_j, v_j \in K_j \\
&u-w=\sum_{j=1}^J u_j
\end{align*}
\textbf{Proof}\\
\begin{align*}
v_J=u-w\leq \phi-w=\phi_J \quad \to \quad v_J \in K_J
\end{align*}
Assuming $v_{j+1} \in K_{j+1}$ and knowing that $z \geq y$ implies $I_{h_j}(z)\geq I_{h_j}(y)$:
\begin{align*}
v_j=I_{h_j}(v_{j+1}-w_{j+1})\leq I_{h_j}(\phi_{j+1}-w_{j+1})=\phi_j \quad \to \quad v_j \in K_j
\end{align*}
Then for $j=J,...,2$:
\begin{align*}
u_j=v_j-v_{j-1}=v_j-I_{h_j-1}(v_j-w_j)=
v_j-\dfrac{I_{h_j-1}((v_j-w_j))}{(v_j-w_j)}(v_j-w_j)=
(1-\theta_{v_j-w_j})v_j +\theta_{v_j-w_j} w_j 
\end{align*}
so $u_j \in K_j$.\\
Now we show how we can use these definitions also for the last inequality. Notice that in the following we use the property of the operators considered, i.e $\norm{\bI_j(\bsigma)}_{H_{\text{div}}} \leq \norm{\bsigma}_{H_{\text{div}}}$ and $\norm{\boldsymbol{\varepsilon}\left(\bI_j(\bu)\right)}_{L^2} \leq \norm{\boldsymbol{\varepsilon}(\bu)}_{L^2}$:
\begin{align*}
j<J:\quad 
\norm{\bv_j}_U&=\norm{\bI_j(\bv_{j+1}-\bw_{j+1})}_U \\
&\leq \norm{\bv_{j+1}-\bw_{j+1}}_U  \\
&\leq \norm{\bv_{j+1}}_U + \norm{\bw_{j+1}}_U  \\
&\leq \norm{\bv_{J}}_U + \sum_{k=j+1}^J\norm{\bw_{k}}_U  \\
\norm{\bv_j}_U^2 &\leq (J-j) \left(\norm{\bv_{J}}_U^2 + \sum_{k=j+1}^J\norm{\bw_{k}}_U^2 \right)  \\
\sum_{j=1}^J \norm{\bv_j}_U^2 &\leq J(J-1) \left(\norm{\bv_{J}}_U^2 + \sum_{k=2}^J\norm{\bw_{k}}_U^2 \right) 
\end{align*}
We now use the definition $\bw_j=\sum_{i=1}^{I_j} \bw_{ji}$ and substitute it:
\begin{align*}
\sum_{j=1}^J \norm{\bv_j}_U^2 &\leq J(J-1) \left(\norm{\bv_{J}}_U^2 + \sum_{k=2}^J\norm{\sum_{i=1}^{I_k} \bw_{ki}}_U^2 \right) 
\end{align*}
We take advantage of the definition of color:
\begin{align*}
\norm{\sum_{i=1}^{I_k} \bw_{ki}}_U^2=
\norm{\sum_{m=1}^{m_k} \bw_{k,m}}_U^2
\leq 
m_k
\sum_{m=1}^{m_k}
\norm{ \bw_{m,k}}_U^2
\leq 
m_k
\sum_{m=1}^{m_k} \sum_{i \in m}
\norm{ \bw_{k,i}}_U^2
\leq m_k
\sum_{i=1}^{I_k}
\norm{ \bw_{ki}}_U^2
\end{align*}
Finally:
\begin{align*}
\sum_{j=1}^J \norm{\bv_j}_U^2 &\leq J(J-1)\max_j m_j \left(\norm{\bu-\bw}_U^2 + \sum_{j=1}^J \sum_{i=1}^{I_j}  \norm{\bw_{ji}}_U^2 \right) 
\end{align*}
So the constant $C_2=J(J-1) \max_j m_j $.\\\\\\
Now, for a given level j, consider patches $\Omega_i$. Let $k$ be a face dof of the mesh. Then:
 \begin{align*}
  \displaystyle
 \theta_i(\sum_{k=1}^{I_j} \phi_k u_k)=
 \sum_{k \in P_i} \phi_k \dfrac{u_k}{d}
 \end{align*}
 Also:
  \begin{align*}
  \displaystyle
(1-\theta_i)(v)+\theta_i(u)&=(1-\theta_i)(\sum_{k=1}^{I_j} \phi_k v_k)+ \theta_i(\sum_{k=1}^{I_j} \phi_k u_k)\\
&= \sum_{k \notin P_i} \phi_k  v_k+
\sum_{k \in P_i} \phi_k \dfrac{d-1}{d} v_k+\sum_{k \in P_i} \phi_k \dfrac{u_k}{d}\\
&= \sum_{k \notin P_i} \phi_k  v_k+\sum_{k \in P_i} \phi_k \left(\dfrac{d-1}{d} v_k + \dfrac{u_k}{d}\right)
 \end{align*}
 By inspection of each component:
 \begin{align*}
 \begin{cases}
 v_k\leq \phi_k & k \notin P_i\\
 \dfrac{d-1}{d} v_k + \dfrac{u_k}{d}\leq \dfrac{d-1}{d} \phi_k + \dfrac{\phi_k}{d}=\phi_k & k \in P_i
 \end{cases}
 \end{align*}
 In this way, since there are only $d=2,3$ points, and so patches, for each edge/face:
 \begin{align*}
 \displaystyle
 \sum_{i=1}^{I_j} \theta_i \left(\sum_{k=1}^{I_j} \phi_k u_k\right)= \sum_{k=1}^{I_j} \phi_k u_k
 \end{align*}
 Therefore if $v$, $u$ $\in K_j$, then $(1-\theta_i)(v)+\theta_i(u) \in K_j$.\\ 
 In this presentation the function $\theta_i$ is the same as the $L_h(\theta_i)$ in Badea's paper. Indeed, I THINK, there the multiplication of $\theta_i$ and a $P^1$ function is intrinsecally quadratic. Then the application of $L_h$ makes the product linear again. So, $L_h(\theta(\cdot))$ is used only, I GUESS, for turning on and off the degrees of freedom.\\\\
\textbf{ PAY ATTENTION: IN THE FOLLOWING} $v_i=u_{j,i}$, $v=u_j$, $w=0$:!!!!
\\\\
We then define:
\begin{align*}
v_1 &=\theta_1(v-w) +(1-\theta_1) w_1\\
v_i & =\theta_i(v-w-\sum_{j=1}^{i-1}v_j) +(1-\theta_i) w_i\\
\end{align*}
We see that:
\begin{align*}
v_1 +w&=\theta_1(v) +(1-\theta_1)w +(1-\theta_1) w_1\\
&=\theta_1(v) +(1-\theta_1)(w +w_1)
\end{align*}
But $w+w_1$, $v$ belong to $K_j$ by assumption. Also:
\begin{align*}
v-v_1 +w_1&=v+w1-\theta_1(v-w)-(1-\theta_1)w1=\\
             &=(1-\theta_1)v+ w1 + \theta_1 w-(1-\theta_1)w1=\\
             &=(1-\theta_1)v+ \theta_1(w+w1) 
\end{align*}
 And also in this case, $w-v_1+w_1 \in K_j$. Then we can generalize for $w+\sum_{j=1}^{i-1} w_j +v_i$:
 \begin{align*}
 w+\sum_{j=1}^{i-1}w_j + v_i&=\theta_i(v-w - \sum_{j=1}^{i-1} v_j)+(1-\theta_i)w_i+ w+\sum_{j=1}^{i-1}w_j=\\
&=\theta_i(v) + \theta_i( -w - \sum_{j=1}^{i-1} v_j)+(1-\theta_i)w_i + w+\sum_{j=1}^{i-1}w_j\\
&=\theta_i(v) +(1-\theta_i) w - \theta_i  \sum_{j=1}^{i-1} v_j +(1-\theta_i)w_i +\sum_{j=1}^{i-1}w_j \\
&=\theta_i(v) +(1-\theta_i) (w+w_i)  +\sum_{j=1}^{i-1} (w_j-\theta_i v_j) \\
&=\theta_i(v) +(1-\theta_i) (w+\sum_{j=1}^{i-1}w_j+w_i)  + \theta_i \sum_{j=1}^{i-1} (w_j- v_j)\\
&=\theta_i(v + \sum_{j=1}^{i-1} (w_j- v_j) ) +(1-\theta_i) (w+\sum_{j=1}^{i-1}w_j+w_i)  
 \end{align*}
 where by assumption on the previous computation $v + \sum_{j=1}^{i-1} (w_j- v_j)  \in K$ and by hypothesis $w+\sum_{j=1}^{i-1}w_j+w_i \in K$. Now we have to show that $v + \sum_{j=1}^{i} (w_j- v_j)  \in K$, so that the above recursive assumption is true:
 \begin{align*}
  v+\sum_{j=1}^{i} w_j - \sum_{j=1}^{i} v_j  &=
 v+\sum_{j=1}^{i-1} w_j - \sum_{j=1}^{i-1} v_j + w_i - v_i &\\
&=  v+\sum_{j=1}^{i-1} w_j - \sum_{j=1}^{i-1} v_j + w_i -\theta_i(v-w - \sum_{j=1}^{i-1} v_j)-(1-\theta_i)w_i\\
&=  (1-\theta_i) v -(1-\theta_i)\sum_{j=1}^{i-1} v_j + \sum_{j=1}^{i-1} w_j  + w_i +\theta_i w -(1-\theta_i)w_i\\
&=  (1-\theta_i) (v - \sum_{j=1}^{i-1} v_j -w_i)+  \sum_{j=1}^{i-1} w_j  + w_i +\theta_i w \\
&=  (1-\theta_i) (v - \sum_{j=1}^{i-1} v_j -w_i + \sum_{j=1}^{i-1} w_j)+  w_i +\theta_i (w+\sum_{j=1}^{i-1} w_j )\\
&=  (1-\theta_i) (v + \sum_{j=1}^{i-1} w_j- \sum_{j=1}^{i-1} v_j )+\theta_i (w+\sum_{j=1}^{i-1} w_j 
+w_i )
 \end{align*}
 From the previous iterate, $v + \sum_{j=1}^{i-1} w_j- \sum_{j=1}^{i-1} v_j  \in K$ and $w+\sum_{j=1}^{i} w_j \in K$ by hypothesis.\\
 Finally we define:
 \begin{align*}
 v_m=v-w -\sum_{j=1}^{m-1} v_j
 \end{align*}
 and we show that $v_m +w + \sum_{j=1}^{m-1} \in K_j$:
 \begin{align*}
 v_m +w + \sum_{j=1}^{m-1}w_j &= v-w -\sum_{j=1}^{m-1} v_j+w + \sum_{j=1}^{m-1}w_j= v + \sum_{j=1}^{m-1}w_j-\sum_{j=1}^{m-1} v_j  \\
 \end{align*}
 which belongs to $K_j$ due to the previous iterate. In this way we also prooved that:
 \begin{align*}
 v-w=\sum_{j=1}^m v_j
 \end{align*}
 So we have built $u_{ji}$ such that the first two conditions of the assumption IV) is satisfied. Now we have to show that also the inequality holds.\\
One can also see that:
 \begin{align*}
 u_1&=\theta_1 u + (1-\theta_1)w_1\\
  u_2&=\theta_2 (u-u_1) + (1-\theta_2)w_2\\
  &=\theta_2 (u-\theta_1 u - (1-\theta_1)w_1) + (1-\theta_2)w_2\\
    &=\theta_2 (1-\theta_1)u-\theta_2 (1-\theta_1)w_1+ (1-\theta_2)w_2\\
   u_3&=\theta_3 (u-u_1-u_2) + (1-\theta_3)w_3\\   
 &=\theta_3 (u-\theta_1 u - (1-\theta_1)w_1 -
 \theta_2 (1-\theta_1)u+\theta_2 (1-\theta_1)w_1- (1-\theta_2)w_2
 ) + (1-\theta_3)w_3\\
  &=\theta_3 (1-\theta_1)(1-\theta_2)u+w_1 [-\theta_3 (1-\theta_2)(1-\theta_1) ] +w_2 [-\theta_3 (1-\theta_2)]+ (1-\theta_3)w_3\\
  u_i&=u \: \theta_i \prod_{k=1}^{i-1}(1-\theta_k) +\sum_{k=1}^{i-1}w_k\prod_{k=1}^{i-1}   (-\theta_i) (1-\theta_k) + (1-\theta_i) w_i
 \end{align*} 
 ${}$
 \\\\\\\\\\\\\\\\\\\\\\\\\\\\
 What have we done so for is true for a given level $j$. Therefore:
 \begin{align*}
 \sum_{j=1}^J \sum_{i=1}^{I_j}\| u_{j,i} \|_U^2 \leq C (\| \sum_{j=1}^J \sum_{i=1}^{I_j}  u_{j,i} \|_U^2 + \sum_{j=1}^J \sum_{i=1}^{I_j}\|w_{j,i} \|_U^2 )
 \end{align*}
 Due to the subdivision in colors, it is possible to write:
  \begin{align*}
 \sum_{j=1}^J \sum_{m=1}^{m_j}\| u_{j,m} \|_U^2 \leq C (\| \sum_{j=1}^J \sum_{i=1}^{I_j}  u_{j,i} \|_U^2 + \sum_{j=1}^J \sum_{i=1}^{I_j}\|w_{j,i} \|_U^2 )
 \end{align*}
 Then it is sufficient to show:
 \begin{align*}
 \| u_{j,m} \|_U^2 \leq C(\| \sum_{j=1}^J \sum_{i=1}^{I_j}  u_{j,i} \|_U^2 + \sum_{j=1}^J \sum_{i=1}^{I_j}\|w_{j,i} \|_U^2 )
 \end{align*}
 to obtain $C=J \max_j m_j$. We observe that:
 \begin{align*}
  \| u_{j,m} \|_U^2 = \sum_{i \in m}   \| u_{j,i} \|_{U(P_i) }^2
 \end{align*}
 Since for a given color, all the patches $P_i \in m$ do not overlap, if we show:
 \begin{align*}
 \| u_{j,i} \|_{U(P_i)}^2 \leq C  \|  \sum_{j=1}^J \sum_{i=1}^{I_j} u_{j,i} \|_{U(P_i)}^2
 \end{align*}
 
 
\begin{align*}
\|v_1 \|_{U(P_i)}^2&=\| \theta_1(v-w) +(1-\theta_1) w_1 \|_{U(P_i)}^2\\
&\leq \| \theta_1(v-w) \|_{U(P_i)}^2 +\|(1-\theta_1) w_1 \|_{U(P_i)}^2\\
&=
\dfrac{1}{d}\| (v-w)\|_{U(P_i)}^2 +\dfrac{d-1}{d} \|w_1 \|_{U(P_i)}^2\\
\|v_i \|_{U(P_i)}^2& =\| \theta_i(v-w-\sum_{j=1}^{i-1}v_j) +(1-\theta_i) w_i \|_{U(P_i)}^2 \\
&\leq\dfrac{1}{d} \| v-w\|_{U(P_i)}^2
+\dfrac{1}{d}  \sum_{j<i, j \in P_i}\|v_j\|_{U(P_i)}^2 +\dfrac{d-1}{d} \|w_i \|_{U(P_i)}^2 \\
\end{align*}
 The norm of a $v_i$ is bounded also by all the $v_j$ with $j<i$ and $j \in P_i$. For a given patch $P_i$, if $T$ is the maximum number of nodes on the border of that patch, then a $v_j$ will be considered only $T$
 \begin{align*}
   \sum_{j<i, j \in P_i}\|v_j\|_{U(P_i)}^2 \leq    \sum_{j<i, j \in P_i}\|v_j\|_{U(P_j)}^2 
 \end{align*}
 Since there are T surrounding nodes, there are also T contribution of the type $v_i$. 
 \begin{align*}
 \sum_{i=1}^{I_j} \|v_i \|_{U(P_i)}^2 \leq  \sum_{i=1}^{I_j} \dfrac{1}{d} \| v-w\|_{U(P_i)}^2
+\dfrac{1}{d}  \sum_{i=1}^{I_j}  \sum_{j<i, j \in P_i}\|v_j\|_{U(P_i)}^2 +\dfrac{d-1}{d}  \sum_{i=1}^{I_j} \|w_i \|_{U(P_i)}^2
 \end{align*}
 
 One can also see that:
 \begin{align*}
 u_1&=\theta_1 u + (1-\theta_1)w_1\\
  u_2&=\theta_2 (u-u_1) + (1-\theta_2)w_2\\
  &=\theta_2 (u-\theta_1 u - (1-\theta_1)w_1) + (1-\theta_2)w_2\\
    &=\theta_2 (1-\theta_1)u-\theta_2 (1-\theta_1)w_1+ (1-\theta_2)w_2\\
   u_3&=\theta_3 (u-u_1-u_2) + (1-\theta_3)w_3\\   
 &=\theta_3 (u-\theta_1 u - (1-\theta_1)w_1 -
 \theta_2 (1-\theta_1)u+\theta_2 (1-\theta_1)w_1- (1-\theta_2)w_2
 ) + (1-\theta_3)w_3\\
  &=\theta_3 (1-\theta_1)(1-\theta_2)u+w_1 [-\theta_3 (1-\theta_2)(1-\theta_1) ] +w_2 [-\theta_3 (1-\theta_2)]+ (1-\theta_3)w_3\\
  u_i&=u \: \theta_i \prod_{k=1}^{i-1}(1-\theta_k) +\sum_{k=1}^{i-1}w_k\prod_{k=1}^{i-1}   (-\theta_i) (1-\theta_k) + (1-\theta_i) w_i
 \end{align*}
 In this case only the $w_k$ such that $k \in P_i$ will be different from zero, due to $\theta_i$. Therefore, if $T$ is the maximum number of nodes of a patch:
\begin{align*}
\|  u_i\|_U^2&= \|u_i \|_{U(P_i)}^2 \leq \dfrac{T}{d^2}\|u\|_{U(P_i)}^2+ \sum_{k \in P_i, k \neq i} T  \dfrac{\left(d-1 \right)^2}{d^4} \| w_k \|_{U(P_i)}^2+T \left( \dfrac{d-1}{d} \right)^2 \| w_i \|_{U(P_i)}^2\\
\sum_{i=1}^{I_j }\|  u_i\|_U^2&\leq \sum_{i=1}^{I_j }\dfrac{T}{d^2}\|u\|_{U(P_i)}^2+  \sum_{i=1}^{I_j } T^2 \left( \dfrac{d-1}{d} \right)^2\| w_k \|_{U(P_i)}^2\\
&=\sum_{m=1}^{m_j} \dfrac{T}{d^2}\sum_{i \in m} \|u\|_{U(P_i)}^2+  \sum_{i=1}^{I_j } T^2 \left( \dfrac{d-1}{d} \right)^2 \| w_i\|_{U(P_i)}^2
\end{align*}
On a given color, each dof is counted only once. \textbf{PLEASE IN THE FOLLOWING NOTE THIS}.  If the patch is defined such that it contains ALSO the boundary faces, then it is \textbf{OBVIOUS} that:
\begin{align*}
\|\bu_{j,m}\|_{U(P_{j,m})}^2 \leq \|\bu_{j}\|_{U(P_{j})}^2
\end{align*}
because $\bu_{j,m}$ is defined on not-intersecting patches and on each element is identical to $\bu_{j}$. So they are basically identical, except that on some elements $\bu_{j,m}$ is zero. Therefore we can bound its $L^2$ norm in this way. \textbf{THIS IS TRUE FOR HDIV BUT ALSO FOR H1}. So theoretically I would prefer for this kind of patch. Practically maybe it is not necessary. I am N\textbf{OT SO SURE HOW THE DEFINITION OF COLOR WOULD CHANGE IN THIS WAY, ANYHOW. STILL, I WANT TO MAKE THIS ASSUMPTION BECAUSE OTHERWISE I DO NOT KNOW HOW TO OBTAIN A SIMILAR INEQUALITY.}. 
\begin{align*}
\sum_{j=1}^J
\sum_{i=1}^{I_j }\|  \bu_{ji}\|_U^2&\leq \sum_{j=1}^J \sum_{i=1}^{I_j }\dfrac{T}{d^2}\|\bu_j\|_{U(P_{ji})}^2+ \sum_{j=1}^J \sum_{i=1}^{I_j } T^2 \left( \dfrac{d-1}{d} \right)^2\| \bw_{ji} \|_{U(P_{ji})}^2\\
&\leq \sum_{j=1}^J \sum_{m=1}^{m_j }\dfrac{T}{d^2}\|\bu_{j,m}\|_{U(P_{j,m})}^2+ \sum_{j=1}^J \sum_{i=1}^{I_j } T^2 \left( \dfrac{d-1}{d} \right)^2\| \bw_{ji} \|_{U(P_{ji})}^2\\
&\leq \dfrac{T m_J}{d^2}\sum_{j=1}^J\|\bu_{j}\|_{U}^2+ \sum_{j=1}^J \sum_{i=1}^{I_j } T^2 \left( \dfrac{d-1}{d} \right)^2\| \bw_{ji} \|_{U(P_{ji})}^2\\
\end{align*}
\begin{align*}
u_J&=v_J-v_{J-1}=u-w-I_{h_{J-1}}(v_J-w_J)=
u-w-I_{h_{J-1}}(u-w-w_J)\\
\norm{u_J} &\leq \norm{u-w} +\norm{I_{h_{J-1}}(u-w-w_J)}\\
u_{J-1}&=v_{J-1}-v_{J-2}=I_{h_{J-1}}(v_J-w_J)-I_{h_{J-1}}(v_{J-1}-w_{J-1})=
u-w-I_{h_{J-1}}(u-w-w_J)\\
\norm{u_{j}} &\leq \norm{v_j} +\norm{v_{j-1}}\\
u_j&=v_j-v_{j-1}=I_{h_j}(v_{j+1}-w_{j+1})-I_{h_j-1}(v_j-w_j)\\
u_1&=v_1=I_{h_1}(v_{2}-w_{2})
\end{align*}
\begin{align*}
\sum_{j=1}^J \norm{u_j}^2 \leq 2\sum_{j=2}^J \left( \norm{v_j}^2+ \norm{v_{j-1}}^2 \right) + \norm{v_1}^2=2\norm{v_J}^2+
\sum_{j=2}^{J-1}4   \norm{v_j}^2+3 \norm{v_1}^2 \leq 4 \sum_{j=1}^J \norm{v_j}^2
\end{align*}
So
\begin{align*}
\sum_{j=1}^J
\sum_{i=1}^{I_j }\|  \bu_{ji}\|_U^2&\leq  \dfrac{4 T m_J}{d^2} \sum_{j=1}^J \norm{\bv_j}^2
+ \sum_{j=1}^J \sum_{i=1}^{I_j } T^2 \left( \dfrac{d-1}{d} \right)^2\| \bw_{ji} \|_{U(P_{ji})}^2
\end{align*}
and:
\begin{align*}
j<J:\quad 
\norm{\bv_j}_U^2&=\norm{\bI_j(\bv_{j+1}-\bw_{j+1})}_U^2 \\
&\leq \norm{\bv_{j+1}-\bw_{j+1}}_U^2  \leq 2 \norm{\bv_{j+1}}_U^2+2\norm{\bw_{j+1}}_U^2 \\
&\leq 2^{J-j} \|\bu -\bw \|_U^2 +\sum_{k=j}^J 2^{k-j+1} \norm{\bw_k}_U^2
\end{align*}
So:
\begin{align*}
\sum_{j=1}^J \norm{\bv_j}_U^2
&\leq \sum_{j=1}^J 2^{J-j} \|\bu -\bw \|_U^2 +\sum_{j=1}^J \sum_{k=j}^J 2^{k-j+1} \norm{\bw_k}_U^2\\
& \leq J 2^J \left(\| \bu -\bw \|_U^2+\sum_{j=1}^J \sum_{k=j}^J\norm{\bw_k}_U^2\right)
\end{align*}
SE INVECE CONSIDERO:

\section{LS contact formulation}
The solution of a standard primal formulation problem subject to linear inequality constraints has to fulfill the KKT conditions. The lagrange multipliers represent the dual variables necessary to not violate the constraints. \\
In the context of a LS formulation, primal and dual variable are both unknowns of the problem. This means that the lagrange multipliers for the primal/dual unknown is a quantity belonging to the dual/primal space, but it is not the dual/primal unknown.\\
In the case of LS contact, the constraints are expressed in the following way:
\begin{align*}
\bu \cdot \bn_{\text{obs}} -g \leq 0\\
\bn_{\text{obs}}^T \bsigma \bn \leq 0\\
\left(\bu \cdot \bn_{\text{obs}} -g \right) \left( \bn_{\text{obs}}^T \bsigma \bn \right)  =0
\end{align*}
These conditions have to be valid for all $\bbx \in \Gamma_C$. By discretizing the problem, also the above expression have to be discretized.  Let us assume that $g \in P^1$ is linearly interpolated. Then for the lowest order approximation, $P^1$ for the displacement and $\RT_0$ for the stresses,  the impenetrability and the negative pressure conditions can be respectively checked just on the vertices and the face-midpoints of the elements of $\Gamma_C$. \\\\
The lagrange multipliers represent the dual variables necessary to not violate the constraints. By satisfying the KKT  for the primal/dual formulation, the complementarity condition is then automatically fulfilled. 
\begin{align*}
\begin{cases}
A u + B^T \lambda =f\\
B u =g\\
u_i \geq h_i\\
\lambda_i \geq 0\\
\lambda_i u_i =0
\end{cases}
\end{align*}

\begin{align*}
\begin{cases}
\begin{bmatrix}
A_{\sigma,\sigma} &A_{\sigma,u}\\
A_{\sigma,u}^T &A_{u,u}
\end{bmatrix}
\begin{bmatrix}
\sigma\\
u
\end{bmatrix}
+
\begin{bmatrix}
B_{\sigma,\sigma}^T &0\\
0 &B_{u,u}^T
\end{bmatrix}
\begin{bmatrix}
\lambda_{\sigma}\\
u_{u}
\end{bmatrix}
=
\begin{bmatrix}
f_{\sigma}\\
f_u
\end{bmatrix}\\\\
\begin{bmatrix}
B_{\sigma,\sigma} &0\\
0 &B_{u,u}
\end{bmatrix}
\begin{bmatrix}
\sigma\\
u
\end{bmatrix}=
\begin{bmatrix}
g_{\sigma}\\
g_{u}
\end{bmatrix}\\\\
\end{cases}
\end{align*}


But, when the lagrange multiplier cannot be interpreted as the corresponding dual/primal variable, then this is not true anymore. Indeed in the following relations:
\begin{align*}
\begin{cases}
\bu \cdot \bn_{\text{obs}} -g \leq 0 \\
\left(\bu \cdot \bn_{\text{obs}} -g \right) \lambda_{\bu} = 0 \\
\end{cases}
\qquad
\begin{cases}
\bn_{\text{obs}}^T \bsigma \bn \leq 0\\
\left(\bn_{\text{obs}}^T \bsigma \bn  \right) \lambda_{\bsigma} = 0 \\
\end{cases}
\end{align*}
there is not coupling between $\lambda_{\bu}$/$\lambda_{\bsigma}$ and $\bu$/$\bsigma$.\\
Since in a mixed formulation the complementarity condition couples the primal and the dual variables that are unknowns of the problem, such a condition has to be enforced additionally and cannot be recovered by KKT.\\
Unfortunately complementarity is a non-linear term, so it is difficult to deal with it. For this reason, it is preferrable to add this term to the original functional. In this way, it will be exactly satisfied only in the continuum, but 
 should be check on both, the vertices and the midpoints. This means that is not sufficient to only check:
\begin{align*}
\end{align*}

\section{Normal-tangent Coordinate system}
In order to properly describe contact conditions, it is wise to locally change the coordinate system of the contact boundary $\Gamma_C$. In this way, the scalar constraints have to be checked directly on the normal components and not on some linear combinations of the unknown.\\ In the following the contact normal is assumed to be known in all vertices and all midpoints of the faces belonging to $\Gamma_C$.
Let $\bp$ be a vertex of the tasselation $\emph{T}_h$ that also belongs to $\Gamma_C$ and $\bn$ the obstacle normal in $\bp$. Then consider the vector $ \bu_{\bp} \in \mathbb{R}^d$ that contains the degrees of freedom of the displacement in $\bp$. Define the Householder transformation relative to the ouward normal $\bn_{\bp} $ as:
\begin{align}
\bH_{\bp}= \bI- 2\: \bn_{\bp} ^T \bn_{\bp} 
\end{align}
and the local displacement in the normal-tangent coordinate system (the first coordinate is the normale one):
\begin{align*}
\bu_{\bp,nt}= \bH_{\bp} \bu_{\bp}
\end{align*}
A similar argument has to be applied to the stress components. Let $\bp$ be a face-midpoint of the tasselation $\emph{T}_h$ that also belongs to $\Gamma_C$ and $\bn$ the obstacle normal in $\bp$. Then consider the vector $ \bS_{\bp} \in \mathbb{R}^d$ that contains the degrees of freedom of the stress in $\bp$.
In particular, since the stress components are not of a lagrangian type:
\begin{align*}
\bsigma \bn|_{\bp}  =\left( \bphi_{\bp}  \cdot \bn_{\bp}  \right) \bS_{\bp} =
\text{sgn}\left( \bphi_{\bp}  \cdot \bn_{\bp}  \right) \tilde{\bS}_{\bp} 
\end{align*}
it is not convenient to use direclty the HouseHolder transformation, because we have no control on the sign of $\left( \bphi_{\bp}  \cdot \bn_{\bp}  \right)$. In its place, it is preferrable the transformation $\bQ$:
\begin{align*}
\bS_{\bp,nt}= \bQ_{\bp}  \tilde{\bS}_{\bp} \qquad \bQ_{\bp} = \left( \bphi_{\bp}  \cdot \bn_{\bp}  \right) \bH_{\bp}
\end{align*}
In this way the first component of $\bS_{\bp,nt}$ is actually positive in the direction of the normal $\bn_{\bp}$ and, as $\tilde{\bS}_{\bp} $ has been considered instead of $\bS_{\bp} $, the transformation $\bQ$ is still orthogonal. From now on all the degrees of freedom on the contact boundary will be treated as normal or tangent. 
\section{Gap function on coarser levels}
Given the current iterate $(\bsigma_n, \bu_n)$, a necessary and sufficient condition that a correction $(\bc_{\bsigma}, \bc_{\bu})$ has to satisfy to make the new iterate $(\bsigma_n+\bc_{\bsigma}, \bu_n+\bc_{\bu})$ admissible is :
\begin{align*}
\begin{cases}
\bn^T \left(\bsigma_n+\bc_{\bsigma}\right) \bn \leq 0  &\Gamma_C\\
 \left(\bu_n+\bc_{\bu} \right) \cdot \bn \leq g &\Gamma_C
 \end{cases}
\end{align*}
In a multigrid setting, these conditions have to be fulfilled on the fine mesh, but most of the corrections are computed on the coarser ones. A check relative to a fine mesh would imply a suboptimal complexity. To recover an optimal complexity, instead of the previous requirement, only a sufficient condition can be taken into consideration for coarser levels. This goal can be achievied by using proper interpolation operators for the gap function required on each level.\\
First of all, there are two gap functions to be analyzed, one belongs to the space $H^{1/2}$, the other one to $H^{-1/2}$. Their discrete representation is also different. On $\Gamma_C$, $H^{1/2}$ functions are piecewise linear on each element, while $H^{-1/2}$ functions are piecewise constant. Therefore also two different interpolation operators have to be defined.\\
Let $p$ be an element $K$, a node $n$, an edge $e$, or a face $f$ of the mesh $T_h$. Define the element/node/edge/face patch of p as:
\begin{align*}
P_{p, \diamond,h} =\{ \diamond \in T_h: \exists K_h \in T_h \: \text{such that} \: p,  \diamond \in K_h \} , \qquad \diamond = K, n,e,f
\end{align*}
\subsection{Interpolation for $P_1$ on $\Gamma_C$}
Given a mesh $T_H$ and its uniform refinement $T_h$, consider a coarse edge $e_H \in T_H \cap \Gamma_C$, which contains two coarse nodes $p_{H,1}$, $p_{H,2}$, on its ends, and a fine midpoint $p_h$. Let $g_h$ and $g_H$ be the gap function on level h and H. Since $g_h$ is linear, a sufficient condition so that $g_H \leq g_h$ is the following:
\begin{align}
\label{coarsegapfunctionconditionP1}
\begin{cases}
& g_H(p_{H,1}) \leq g_h(p_{H,1})\\
&  g_H(p_{H,2}) \leq g_h(p_{H,2})\\
&  \dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2})) \leq g_h(p_{h})
\end{cases}
\end{align}
It is easy to see that, on $e_H$, the two following values satisfy the three conditions above. We call this kind of i\textbf{nterpolation Linear Monotone Interpolation (just to let you know that I used it for the square case)}.
\begin{align}
\label{coarsegapfunctionconditionP1LinearInterpolation}
\begin{cases}
g_H({p_{H,1}})= \min( g_h(p_{H,1}),\max( g_h(p_{h}), 2 g_h(p_{h}) - g_h(p_{H,2})) )\\
g_H({p_{H,2}})=\min( g_h(p_{H,2}),\max( g_h(p_{h}), 2 g_h(p_{h})- g_h(p_{H,1}) ) )
\end{cases}
\end{align}
There are two other ways of defining the gap function and are based on the minimum value on the support of the shape function. Furthermore it is easy to check that both definitions satisfy (\ref{coarsegapfunctionconditionP1}).
 Since at the moment we are referring to only one edge, this means searching for the minimum of $g_h$, on $e_H$ intersected with the support of the shape function. If the shape function is the coarse one, then $g_H(q)=\min(g_h(p_{H,1}),g_h(p_h),g_h(p_{H,2}))$, with $q=p_{H,1},p_{H,2}$. But this approach is very roguh (I GUESS IT IS THE ONE IN BADEA? IT DEPENDS ON THE DEFINITION OF SUPPORT FOR HIM. IF IT IS THE CLOSURE OR NOT). On the other hand, if the shape function of $p_{H,i}$ is the fine one,  $g_H(p_{H,i})=\min(g_h(p_{H,i}),g_h(p_h))$, for $i=1,2$. This last approach does not differ so much from the first one given, altough in general is still rougher.\\
Now we want to show that (\ref{coarsegapfunctionconditionP1}) is actually true for (\ref{coarsegapfunctionconditionP1LinearInterpolation}). Let $a=g_h(p_{H,1})$, $b=g_h(p_h)$, $c=g_h(p_{H,2})$. Then:
\begin{enumerate}
\item 
\begin{enumerate}
\item $ a \geq c \geq b$, $a\geq 2b -c$ ($b \geq 2 b -c$, $ b\geq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,2b -c)+\min(c,b))=\dfrac{1}{2}(2b-c+c) = b
\end{align*}
\item  $ a \geq c \geq b$, $a \leq 2 b - c$  ($b \geq 2 b -c$, $ b\geq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,2b -c)+\min(c,b))=\dfrac{1}{2}(a+c) \leq b
\end{align*}
\end{enumerate}
\item $ a \geq c \geq b$ ($b \geq 2 b -c$, $ b\geq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,b)+\min(c,b))=\dfrac{1}{2}(b+b) = b
\end{align*}
\item 
\begin{enumerate}
\item $ c \geq b \geq a$, $2 b - a \geq c$  ($b \geq 2 b -c$, $ b\leq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,b)+\min(c,2b-a))=\dfrac{1}{2}(a+c) \leq b
\end{align*}
\item $ c \geq b \geq a$, $2 b - a \leq c$ ($b \geq 2 b -c$, $ b\leq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,b)+\min(c,2b-a))=\dfrac{1}{2}(a+2b-a) = b
\end{align*}
\end{enumerate}
\item  $ c \geq a \geq b$  ($b \geq 2 b -c$, $ b\geq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,b)+\min(c,b))=\dfrac{1}{2}(b+b) = b
\end{align*}
\item $ b \geq a \geq c$ or $b \geq c \geq a$ ($b \leq 2 b -c$, $ b\leq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,b)+\min(c,b))=\dfrac{1}{2}(a+c) \leq b
\end{align*}
\end{enumerate}
This requirement has to be true for all edges $e_H$ belonging to its patch $E_H=\{e_H \in P_{p_H, e,H} \}$.
It is possible to define a projection operator such that these conditions are fulfilled. Let $p_{H,i}$ be a node of the coarse mesh $T_H$ and $\phi_{H,i}$ the respective coarse basis function. Then:
\begin{align*}
\displaystyle
&g_H(p_{H,1})= \min_{e_H \in E_H} \left[ \min( g_h(p_{H,1}),\max( g_h(p_{h}), 2 g_h(p_{h}) - g_h(p_{H,2})) )  \right]\\
&g_H=R_{P^{1,H}}^{P^{1,h}} g_h= \sum_{p_{H,i} \in T_H} \phi_{H,i
} \:g_H(p_{H,i})
\end{align*}
Since $g_H$ is a piecewise linear function such that (\ref{coarsegapfunctionconditionP1}) is always fulfilled, it is evident that:
\begin{align*}
0 \leq R_{P^{1,H}}^{P^{1,h}} g_h(x)=g_H(x) \leq g_h(x) \quad \text{if} \quad g_h\geq 0
\end{align*}
Consequently the function:
\begin{align*}
\theta_g(x)= \begin{cases}
\dfrac{R_{P^{1,H}}^{P^{1,h}} g_h(x)}{g_h(x)} & g_h \neq 0 \\
0 & g_h =0
\end{cases}
\end{align*}
satisfies $0 \leq \theta_g \leq 1 $.
\subsection{Interpolation for $\RT_0$ on $\Gamma_C$}
Given a mesh $T_H$ and its uniform refinement $T_h$, consider a face $f_H \in T_H \cap \Gamma_C$ and all the fine faces belonging to it, i.e. patch $F_{h,H}=\{f_h \in f_H \}$. Then, since a discrete $H^{-1/2}$ function is constant on each face of $\Gamma_C$, it is sufficient to define:
\begin{align*}
\displaystyle
R_{\RT_{0,h}}^{\RT_{0,H}} (v)|_{f_H}= \min_{F_{h,H}} v
\end{align*} 
 Let $f_{H,i}$/$f_{h,j}$ be the midpoint of the face $i$/$j$ of the coarse/fine mesh $T_H$/$T_h$. Then let $\phi_{H,i}$ be the respective coarse basis function:
\begin{align*}
\displaystyle
&g_H(f_{H,i})= \min_{f_{h} \in f_{H,i}} g_h(f_h)\\
&g_H=R_{RT_0^{1,H}}^{RT_0^{1,h}} g_h= \sum_{f_{H,i} \in T_H} \phi_{H,i
} \:g_H(f_{H,i})
\end{align*}
Also in this case we can prove:
\begin{align*}
\theta_g(x)= \begin{cases}
\dfrac{R_{RT_0^{1,H}}^{RT_0^{1,h}} g_h(x)}{g_h(x)} & g_h \neq 0 \\
0 & g_h =0
\end{cases}
\end{align*}
satisfies $0 \leq \theta_g \leq 1 $ on $\Gamma_C$. 
\\
\textbf{PLEASE NOTE THAT THETA IS STILL A $\RT_0$ FUNCTION. ESSENTIALLY IT IS DEFINED WITH THE AVERAGE VALUE ON THE FACE WHERE I DO NOT HAVE THE CONSTRAINTS AND WITH THE MINIMUM ON $\Gamma_C$. } Therefore later when I need to use $0 \leq \theta_g \leq 1 $, I use this ONLY on $\Gamma_C$. But I need it to show that $u_j$ is in $K_j$.
%For both interpolations, $g_h$ on the fine level is defined as:
%\begin{align*}
%\begin{cases}
%\tilde{g}_h := g_h - c_{j,k,\bp} & k=\bu,\bsigma, \bp \in T_{j+1}\\
%\end{cases}
%\end{align*}


\section{Truncated Basis}
Whenever a degree of freedom $p_J$ comes into contact, then the corrections should not apply to it. This is guaranteed by the fact that, also on coarser levels, the constraints are always fulfilled. 
Now, considering the $P^1$ case, let $q=p_{J-1,1}, p_{J-1,2}$ be the coarse points belonging to the same coarse edge $e_{J-1}$, whose midpoint is $p_J$.  Then $g_J(p_J)=0$ implies that all $g_{J-1}(q)=0$. Of course, going deeper and deeper, the number of points in which the coarse gap function is zero increases and the corrections will be smaller and smaller and less effective. For this reason, to accelerate convergence, it is better to remove degrees of freedom which are known to be in contact. Once these are removed, they will not affect the coarse gap function. Practically, this is equivalent to remove the rows of the projection $P_{j-1}^j$ relative to the working set degrees of freedom and to put $g_j(q)=\infty$ in all points $q$ which are in contact, before computing $g_{j-1}$.
\begin{algorithm}
\caption{\textbf{[x,WS]=V-CYCLE(A,b,x,g,P,j,J,k)}}
\begin{algorithmic}
\State{$\bbx$=solution, $\textbf{WS}$=working set, $\bA$=system matrix, $\bb$=right hand side, $\bg=$ gap}  
\State{$\bP$=projections, $j$=level, $J$=maximum level, $k$=number of smoothing-steps }  
\State{TruncatedBasis: remove degrees of freedom belonging to the WorkingSet WS}
\State{CoarseConstraint: define $\bg_{j-1}$ such that $\bbx_{j}+\bP_{tr}\bbx_{j-1}\leq \bg_j $}
\State{}
     \If{  ($j=1$) }\\
\textbf{[x,WS]=ActiveSet(A,b,x,g)}
   \Else
\begin{enumerate}
\item Do $k$ non-linear smoothing-steps: compute $\bbx$
\item Compute $ \bP_{tr} =\text{TruncatedBasis}(\bP,j,\textbf{WS}) $
\item Compute $\br_j=\bb-\bA \bbx$
\item Compute $\br_{j-1}=\bP_{tr}^T \br_j$
\item Compute $\textbf{G}_{j}=\bg_j-\bbx$
\item Compute $\bg_{j-1}=\text{CoarseConstraint}(\textbf{G}_{j})$
\item Compute $\bA_{j-1}=\bP_{tr}^T \bA_j \bP_{tr}^T$
\item \textbf{[x,WS]=V-CYCLE($\bA_{j-1},\br_{j-1}, \textbf{0} ,\bg_{j-1},\bP,j-1,J,k$)}
\item Do $k$ non-linear smoothing-steps
\end{enumerate}   
    \EndIf 
  \end{algorithmic}
\end{algorithm}

\section{Numerical Experiments}
The refinement of a mesh that approximates a circle should better approximate the circle. Nevertheless in these experiments, given a coarse mesh, we have a standard uniform refinement. \\
The results for the Signorini problem are meaningful for 2 levels (we give pictures only of this case in the plot \ref{ContactPressureDisplacement}), but for 3 levels, the fact that the refinement is not a better circle, makes the finest level far from being a circle. Therefore the physical results are not very good. Anyhow, we studied the convergence behaviour for the 3 level case, as well. But physical results refer only to the 2 level case.
\begin{figure}[htbp!]
	\includegraphics[width=0.6\textwidth]{img/ContactPressure.eps}
\quad
	\includegraphics[width=0.6\textwidth]{img/ContactDisplacement.eps}
			\label{ContactPressureDisplacement}
\end{figure}


\begin{figure}[htbp!]
	\includegraphics[width=0.6\textwidth]{img/Square2D2LComparison.eps}
\:
	\includegraphics[width=0.6\textwidth]{img/Square2D3LComparison.eps}
		\caption[Square2D3LComparison]{Square mesh. There are three fixed fine meshes, $F=5,4,3$. Then we have 2 or 3 levels of coarsening ($C=4,3,2$ and $C=3,2,1$). For all cases, the linear interpolation has been used for the gap function.\\
The rate of convergence is basically linear, but the slop depends on the number of levels.}
		\label{Square2D3LComparison}
\end{figure}


\begin{figure}[htbp!]
\centering
		\includegraphics[width=0.45\textwidth]{img/Signorini2D2LComparison.eps}
		\quad 
				\includegraphics[width=0.45\textwidth]{img/Signorini2D3LComparison.eps}
		\caption[Signorini2D2LComparison]{Signorini problem. \\
		\textbf{Left}\\
		For two fixed coarse mesh, we consider one level of refinement: I) and II) represent these two cases. In II) the number of dofs is higher.\\
For both cases, we consider 3 kinds of interpolation for the gap function from fine to coarse. In blue: on a coarse patch, we take the minimum on the whole coarse patch. In black: we take the minimum on the fine patch.
In red: we consider the linear monotone interpolation (\ref{coarsegapfunctionconditionP1LinearInterpolation}).
The green lines represent the residuals for both cases computed by using a direct solver.\\
It is evident that there is basically no difference between the red and the black ones. Nevertheless the red one requires more topologic informations. Given a coarse node $p_{H,1}$, the corresponding edge $e_H$ that ends in $p_{H,2}$ , it is also necessary to know which is the fine midpoint $p_{h}$. Knowing the fine and the coarse patches, it is possible to find $p_h$, but is more work that, practically, it is not determinant. It suffices to consider the fine patch and take the minimum.\\
As we can see there are two slops, relative to the case in which we search for contact dofs and the case in which contact dofs are already known. \\
\textbf{Right}\\
We see the level dependence of the algorithm. In diamond-shape the 3 level method, while the 2L is characterized by a continuous line. In red: the linear case. In blue: on a coarse patch, we take the minimum on the whole coarse patch.
}
		\label{Square2D3LComparison}
\end{figure}

\section{Idea: Monotone Parallel Gauss-Seidel}
Let us consider a minimization of a discrete functional written as:
\begin{align*}
\displaystyle
\mathcal{J}_h(\bbx)=\dfrac{1}{2}\bbx^T \bA \bbx - \bb^T \bbx=\dfrac{1}{2}\sum_i \sum_j a_{ij} x_i x_j -\sum_i b_i x_i
\end{align*}
The minimization problem is: find $\bbx \in K_h$ such that $\mathcal{J}_h(\bbx) \leq \mathcal{J}_h(\by)$ $\forall \by \in K_h$.\\
Now let us assume that the matrix $\bA$ is sparse, due to the local support of the basis functions. Therefore let us suppose that $x_m$ and $x_n$ are such that $a_{mn}=a_{nm}=0$. Given an approximation $\by \in K_h$ of $\bbx$, we define:
\begin{align*}
\bc_i^{n}=\begin{cases}
x_n & i=n\\
0    & i \neq n
\end{cases}
\quad
\bc_i^{m}=\begin{cases}
x_m & i=m\\
0    & i \neq m
\end{cases}
\quad
\bc=\bc^{n}+\bc^m
\end{align*}
We want to find $\bc$ such that $  \mathcal{J}_h(\by+\bc)\leq   \mathcal{J}_h(\by+\bw)$ $\forall \bw$ defined in the same way as $\bc$.
\begin{align*}
\mathcal{J}_h(\by+\bc)=&  \left[ \dfrac{1}{2}\sum_{i\neq m,n} \sum_{j \neq m,n} a_{ij} x_i x_j -\sum_i b_i x_i \right]
+
\dfrac{1}{2} \sum_{i= m,n} \sum_{j= m,n} a_{ij} x_i x_j -\sum_{i=m,n} b_i x_i
 \\
=&\text{const}+
\dfrac{1}{2} \left[ a_{mm} x_m^2 + a_{nn} x_n^2 + 2 a_{mn} x_n x_m \right] -[  b_m x_m + b_n x_b]\\=&
\text{const}+
 \left[\dfrac{1}{2} a_{mm} x_m^2 -  b_m x_m \right] +  \left[\dfrac{1}{2} a_{nn} x_n^2 - b_n x_b\right]
\end{align*}
Then $\min_{\bc \in K_h} \mathcal{J}_h(\by+\bc)= \min_{\bc^mm \in K_h} \mathcal{J}_h(\by+\bc^m) +  \min_{\bc^n \in K_h} \mathcal{J}_h(\by+\bc^n)$ 
This kind of argument is based on the fact that the two subspaces $m$ and $n$ are independent. Also, it is not necessary to solve the local problems $\min_{\bc^mm \in K_h} \mathcal{J}_h(\by+\bc^m) $ and $ \min_{\bc^n \in K_h} \mathcal{J}_h(\by+\bc^n)$. It is sufficient to find a correction that minimizes the functional with respect to $\by$, i.e:
\begin{align*}
\mathcal{J}_h(\by+\bc^m) \leq \mathcal{J}_h(\by)  \quad \mathcal{J}_h(\by+\bc^n) \leq \mathcal{J}_h(\by) 
\end{align*}
This means that on the subspace $m$ and $n$ instead of using an active set strategy for the exact solution, it is possible to use a non-linear Gauss-Seidel for an approximation of $\bc^n$ and $\bc^m$. \\
In the figure \ref{meshcoloring}, it is shown how to perform a parallal non-linear Gauss-Seidel that also minimizes energy, under the condition that on each subspace, the energy is minimized. The given mesh and the corresponding subspaces need at least 4 colors. Instead of solving exactly the smallest subspaces, we solve approximately larger subspaces. Also these ones can be colored. Then a Gauss-Seidel step loops on all the colors (this is sequential) and then, for a given color, it solves approximately and in parallel all the subdomains relative to that color. For example, we loop on c=purple, yellow, black, brown. Then for the given color, we find the subspaces $i_c$ which are independent and on each $i_c$ we use a non-linear Gauss-Seidel. We update the solution and move to the next color. And so on.
\begin{figure}[htbp!]
		\caption{Left: coloring of the the smallest subspaces; use active set on each of these for the exact correction. Right: coloring of larger subspaces; use non-linear Gauss-Seidel on each of these for an approximate correction.}
		\label{meshcoloring}
		\includegraphics[width=0.8\textwidth]{img/meshcoloring.pdf}
		\end{figure}
		
\section{Monotone Multigrid}
Monotone Multigrid is a multigrid used for solving efficiently obstacle problems. Let $V$ be an Hilbert space, $F$ be a convex and differentiable functional and $K$ a closed convex subset of $V$. Then we search for $u \in K $ such that:
\begin{align*}
\langle F'(u),v-u \rangle \geq 0 \quad \forall \: v \in K
\end{align*} 
Due to both properties of F, one can also show that this problem is equivalent to the following problem. Find $u \in K$ such that:
\begin{align*}
F(u) \leq F(v) \quad \forall v \in V
\end{align*}
The way this problem is stated suggests to build a sequence $u_k$ such that $F(u_{k+1} ) \leq F(u_k)$ $\forall k \in \mathbb{N}$.  This condition is for sure not sufficient for convergence, since it is always possible to choose $u_0 \in K$ and then $u_{k+1}=u_k$. Given a correction $c_k$ so that $u_{k+1}=u_{k}+c_k$, to ensure convergence, it is also necessary that this correction is not too small. Projected Gauss-Seidel is actually a global convergent algorithm. Unfortunately its speed of convergence depends on the inverse of the meshwidth parameter and its complexiti increases with the number of degrees of freedom.
 \\
The idea of monotone multigrid is to consider a global convergent algorithm, which could be very slow, and to accellerate its convergence by corrections that only decrease the energy. Then it is possible to prove that the combination of these two ingredients gives rise to a sequence that is still convergent, but it is faster.\\\\

\subsection{Discrete approximation property for $\RT_0$}
In general, approximation property of the $\RT$ $L^2$ projections are relative to $H^k$ norms. Therefore something like:
\begin{align*}
\| u - \Pi u\| \leq h^{k+1} |u|_{H^{k+1}}
\end{align*}
Therefore here $k \geq0 $ and we need to use at least the $H^1$ norm. Anyhow, I GUESS that in the discrete setting, if $u \in \RT_0(T_j)$ and $\Pi: \RT_0(T_j) \to \RT_0(T_{j-1})$, then we can have on the right hand side also the $H_{div}$ norm. We use the $L^2$ projection property, for which
\begin{align*}
(u - \Pi u, v)_{L^2}=0 \quad \forall v \in \RT_0(T_{j-1})
\end{align*}
And see that:
\begin{align*}
\| u - \Pi u\|_{L^2}^2 = (u - \Pi u, u ) -  (u - \Pi u,\Pi u) = (u - \Pi u, u ) =(u,u)-(u,\Pi u)=(u,u)-(\Pi u, \Pi u)=\|u \|^2-\|\Pi u \|^2
\end{align*}
where we have substituted in the definition of $L^2$ projection $v=\Pi u$ and see that $(u,\Pi u)=(\Pi u, \Pi u)$.
\begin{align*}
\| u - \Pi u\|_{L^2}^2 = \|u \|^2-\|\Pi u\|^2
\end{align*}
In order to show, if it possible, that :
\begin{align*}
\| u - \Pi u\|\leq C h \left( \|\text{div}u\|_{L^2}\right) 
\end{align*}
a way is:
\begin{align*}
\|u \|^2\leq \|\Pi u\|^2 + C h^2 \|\text{div}u\|_{L^2}^2
\end{align*}
To do so, we write:
\begin{align*}
&\|u\|_{L^2}=\sum_{K \in T_j} \int_{K_j} \left(  \sum_{i \in K} \bphi_i u_i \right) \cdot \left(  \sum_{j \in K} \bphi_j u_j \right)  = \sum_{K_ j \in T_j} \sum_{i \in K} \sum_{j \in K}  u_i u_j  \int_{K_j} \bphi_i \cdot \bphi_j\\
&\|\text{div}u\|_{L^2}=\sum_{K_ j \in T_j} \int_{K} \left[ \text{div}\left(  \sum_{i \in K} \bphi_i u_i \right) \right]^2  =\sum_{K \in T_j}  \sum_{i \in K}  \sum_{j \in K} u_i u_j  \int_{K}  \text{div} \bphi_i \text{div} \bphi_j 
\end{align*}
Here we  ASSUME THAT on $K$ $\text{div}\left(  \sum_{i \in K} \bphi_i u_i \right)  \neq 0$. Otherwise we cannot bound the first term, with the second. But if this term is different from zero, then this is possible. Then we redefine $\tilde{u}_i=u_i \alpha_i $ where:
\begin{align*}
\begin{cases}
\int_{F_i}  \bphi_i \cdot \bn_i = |F_i| \\
\bphi_i=\alpha_i \dfrac{|F_i|}{d |K|} \left( \bbx - \bp_i \right)=
\dfrac{\alpha_i}{h_i} \left( \bbx - \bp_i \right)
\end{cases}
\end{align*}
so that $\text{div} \bphi_i u_i =\dfrac{d}{h_i} \tilde{u}_i$. Therefore:
\begin{align*}
&\|u\|_{L^2}^2= \sum_{K \in T_j} \sum_{i \in K} \sum_{j \in K} \tilde{u}_i \tilde{u}_j  \int_{K}\dfrac{1}{h_i h_j }\left( \bbx - \bp_i \right) \cdot \left( \bbx - \bp_j \right)\\
&\|\text{div}u\|_{L^2}^2 =\sum_{K \in T_j}  \sum_{i \in K}  \sum_{j \in K} \tilde{u}_i \tilde{u}_j  \int_{K} \dfrac{d^2}{h_i h_j} 
\end{align*}
Therefore it is sufficient to show that:
\begin{align*}
 \int_{K}\left( \bbx - \bp_i \right) \cdot \left( \bbx - \bp_j \right) \leq C  h^2 |K| d^2
\end{align*}
We use the fact that $w-p_w \leq h$ with $x,y,z$:
\begin{align*}
 \int_{K}\left( \bbx - \bp_i \right) \cdot \left( \bbx - \bp_j \right) \leq \|\bbx - \bp_i  \| \|\bbx - \bp_j \| \leq 
   h^2 |K| \leq d^2 h^2 |K|
\end{align*}
Therefore:
\begin{align*}
\text{div} \bu \neq 0 \quad \to \quad \|\bu \|_{L^2(\Omega)} \leq h \|\text{div} \bu \|_{L^2(\Omega)} 
\end{align*}
\textbf{NO E' FALSO PERCHE' NEI DOPPI PRODOTTI POSSO AVERE SEGNI OPPOSTI.}
\section{Proofs}
As in TX01, pag 117, we decompose $v=\sum v_j$, with $v_j =(Q_j-Q_{j-1})v$ and $ v_j =\sum_{k=1}^{m_{c_j}} v_j^k$. Then we want to show:
\begin{align*}
\displaystyle \sum_{j=1}^J \sum_{i=1}^{m_{c_j}} \left( \| \boldsymbol{\varepsilon}(\bu_i)\|^2  +  \|\bsigma_i \|^2  +  \|\text{div}\bsigma_i \| \right) \leq C \left(
\| \sum_{j=1}^J \sum_{i=1}^{m_{c_j}}  \boldsymbol{\varepsilon}(\bu_i)  \|^2
+\|\sum_{j=1}^J \sum_{i=1}^{m_{c_j}} \bsigma_i  \|^2
+\|\sum_{j=1}^J \sum_{i=1}^{m_{c_j}} \text{div}\bsigma_i  \|^2
 \right )
\end{align*}
Then we prove that, since the :
\begin{align*}
\sum_{i=1}^{m_{c_j}}  \|\text{div}\bsigma_i \|^2&=
 \sum_{i=1}^{m_{c_j}} \sum_{s_i} \int_{\omega_{s_i}} |\text{div}\bsigma_i |^2= \sum_{i=1}^{m_{c_j}} \sum_{s_i} \sum_{K \in {\omega_{s_i}}} \int_K |\text{div}\bsigma_i |^2\\
 &= \sum_{i=1}^{m_{c_j}} \sum_{s_i} \sum_{K \in {\omega_{s_i}}} \int_K \left[\text{div}\left(\sum_{t \in K} \bphi_t \dfrac{  \bsigma_{t,i} }{d} \right)\right]^2\\
 &= \sum_{i=1}^{m_{c_j}} \sum_{s_i} \sum_{K \in {\omega_{s_i}}} \dfrac{1}{|K| }  \left(\sum_{t \in K} \alpha_t \dfrac{  \bsigma_{t,i} }{d}\right)^2\\
\end{align*}
For the uniformity of the mesh exists $\gamma$ such that for each inner diameter $\rho_k$: $\rho_k \geq \dfrac{h}{\gamma}$. Then $|K| \geq \rho_k^d C(d) \geq h^d$, with $C=2 \pi$ for $d=2$ and $C=3/4\pi$ for $d=3$. This means that:
\begin{align*}
\sum_{i=1}^{m_{c_j}}  \|\text{div}\bsigma_i \|^2&\leq C h^{-d}
 \sum_{i=1}^{m_{c_j}} \sum_{s_i} \sum_{K \in {\omega_{s_i}}}  \left(\sum_{t \in K} \alpha_t \dfrac{  \bsigma_{t,i} }{d} \right)^2\\
 &\leq C h^{-d}
 \sum_{i=1}^{m_{c_j}} \sum_{s_i} \sum_{K \in {\omega_{s_i}}} \left(\sum_{t \in K} \alpha_{t}^2 \right)  \left(\sum_{t \in K}\left(\dfrac{  \bsigma_{t,i} }{d}\right)^2 \right)\\
 &\leq C h^{-d}
 \sum_{i=1}^{m_{c_j}}  \left(\sum_{s_i} \sum_{K \in {\omega_{s_i}}}  \sum_{t \in K} \left(\dfrac{  \bsigma_{t,i} }{d}\right)^2 \right)\\
  &\leq C h^{-d}
 \sum_{f}  \bsigma_{t,i}^2 \\
   &\leq C h^{-d} h^d
\|\bsigma \|_0^2=C\|\bsigma \|_0^2=
\end{align*}
since $\left(\sum_{t \in K} \alpha_{t}^2 \right) \leq d+1$ for $\RT_0$.\\\\
I would say that, since to an edge/face belong 2/3 nodes, there are only d patches that will take into account this edge/face. Also these patches would belong to different colors. For this reason, given a function v in $\RT_0$ of a certain level, we can decompose it:
\begin{align*}
v=\sum_{i=1}^{m_c} v_i=\sum_{f} \phi_{f}^{RT_0} v_f\\
\sum_{i=1}^{m_c}v_i= \sum_{i=1}^{m_c}\sum_{\omega_{s_i}} \sum_{f \in \omega_{s_i}} \dfrac{1}{d}  \phi_{f}^{RT_0}  v_f 
\end{align*}
To ensure the equality, we need to know that for a given edge/face there are 2/3 patches that will take it into account. So, since we sum on different colors, on an edge $f$ we consider   $ v_f/d$. Then when we sum upnthe different colors, we recover the right value (I GUESS). This is true for edges/faces...For the displacement field, which is lagrangian and with one dof for patch, we do not have to do anything.






\section{RERELOAD AGAIN}
\begin{align*}
\displaystyle  \sum_{j=1}^J \| u_{j}\|_U^2 =  \sum_{j=1}^J \sum_{i=1}^{n_j} \| u_{ji}\|_U^2  \leq C \| u \|_U^2
\end{align*}
Then on a level j, we can also write:
\begin{align*}
 \sum_{i_j=1}^{n_j} \| u_{ji}\|_U^2 = \sum_{m=1}^{m_j} \|u_m\|_U^2
\end{align*}
For every $u \in K$, with $j>1$:
\begin{align*}
u&= \sum_{i=1}^{n_j} \phi_{j,i} u_{j,i}  - \sum_{i=1}^{n_{j-1}} \phi_{j-1,i} u_{j-1,i }+\sum_{i=1}^{n_{j-1}} \phi_{j-1,i} u_{j-1,i }\\
&= \sum_{i=1}^{n_j} \phi_{j,i} u_{j,i}  - \sum_{i=1}^{n_{j-1}} \left(\sum_{s_i=1}^{(d+1)^2} \alpha_{j-1,s_i} \phi_{j,i} \right)u_{j-1,i }+\sum_{i=1}^{n_{j-1}} \phi_{j-1,i} u_{j-1,i }\\
&= \sum_{i=1}^{n_j} \phi_{j,i} \left(u_{j,i} -\sum_{b_i=1}^d \alpha_{j-1,b_i}  u_{j-1,i } \right)+\sum_{i=1}^{n_{j-1}} \phi_{j-1,i} u_{j-1,i }\\
&= \sum_{i=1}^{n_j} \phi_{j,i}  \bar{u}_{j,i}+\sum_{i=1}^{n_{j-1}} \phi_{j-1,i} u_{j-1,i }\\
&= u_j+u_{j-1}\
\end{align*}
We can assume that the boundary normals on $\Gamma_C$ equal the $\RT_0$ normals. Also note that only the shape function relative to a coarse face is a combination of the fine shape function of the same face. Due do dof definition, the others cannot have these shape functions. Therefore on each of the fine faces belonging to a coarse face, $u_{j,i}-\sum_{b_i=1}^d \alpha_{j-1,b_i}  u_{j-1,i } = u_{j,i}-1/2  u_{j-1,i } $, where $ u_{j-1,i } =\min$ of all the values of the same coarse face.
Then we want $u_j$ and $u_{j-1}$ to belong to $K$. \\
Given $u_j =  \sum_{i=1}^{n_j} \phi_{j,i}  \bar{u}_{j,i}$, then it is necessary to decompose it into the sums of other functions belonging to each patch of the level j. In order to do so, it is sufficient to notice that each face belongs precisely to $d$ nodes, where $d$ is the dimension. Therefore there are only $d$ patches to which the given degrees of freedom relative ti that face belong. This means that each $\phi_{j,i}\bar{u}_{j,i}$ can be viewed as the $d-times$ sum of $\phi_{j,i}\dfrac{\bar{u}_{j,i}}{d}$. Therefore each patch is described by $u_j$, restricted to the patch and divided by $d$. We cal such functions $u_{j,i}$ and we can write:
\begin{align*}
\displaystyle
u_j=\sum_{i=1}^{n_j} u_{j,i}
\end{align*}
Then we can collect into a given colour $m$ all the $u_{j,i}$ such that, given $\forall r, s \in m$ and $r \neq s$, $\text{supp}(u_{j,s}) \cap \text{supp}(u_{j,r}) =\emptyset$. We define:
\begin{align*}
\begin{cases}
u_{j,m}= \sum_{i \in m} u_{j,i} \\
   \|u_{j,m}\|^2=\sum_{i \in m} \|u_{j,i} \|^2
   \end{cases}
\end{align*}
Therefore:
\begin{align*}
\displaystyle  \sum_{j=1}^J \| u_{j}\|_U^2 =  \sum_{j=1}^J \sum_{i=1}^{n_j} \| u_{ji}\|_U^2  =   \sum_{j=1}^J \sum_{i=1}^{m_j} \| u_{j,m}\|_U^2\leq C \| u \|_U^2
\end{align*}
But we know that $m_j  $ is less than the maximum number of nodes of a patch of that level (\textbf{I GUESS}). Then we just have to show that:
\begin{align*}
\displaystyle
\|u_{j,m}\|^2 \leq C \| u \|^2 \forall j, m=1,...,m_j \quad \to \quad C^2= J \max_j m_j=J m_c
\end{align*}
Let us consider an element $K$. Then consider $v=\sum_{i=1}^d \phi_i v_i$. 
\begin{itemize}
\item $\text{div}v=0$.\\
Then:
\begin{align*}
\displaystyle
0 = \text{div} v=\sum_{i=1}^d \text{div}(  \phi_i) v_i = \dfrac{1}{K} \sum_{i=1}^d \alpha_i L_i v_i= \dfrac{1}{K} \sum_{i=1}^d \bar{v}_i \quad \to \quad v_1=-(\sum_{i=2}^d  \bar{v}_i )
\end{align*}
Then:
\begin{align*}
\sum_{i=1}^d   \phi_i v_i= \sum_{i=1}^d   \left( \bbx - \ba_i \right) \dfrac{1}{K} \alpha_i L_i v_i= \dfrac{1}{K} \sum_{i=1}^d   \left( \bbx - \ba_i \right) \bar{v}_i= 
\dfrac{1}{K} \sum_{i=2}^d   \left(  \ba_1 - \ba_i \right) \bar{v}_i =\text{const}
\end{align*}
In this case, since $\| \text{div} v \|=0$, and the $\RT_0$ is a finite element space:
\begin{align*}
|\bar{v}|^2 h^d C \leq \|v\|^2 =\|v\|^2 +\|\text{div} v \|^2 \leq |\bar{v}|^2 h^d C 
\end{align*}
But also $|\bar{v}|^2 \min_i (L_i) \leq |v|^2 \leq \max_i (L_i) |\bar{v}|^2$. So:
\begin{align*}
|v|^2 h^d C \leq \|v\|^2 =\|v\|^2 +\|\text{div} v \|^2 \leq |v|^2 h^d C 
\end{align*}
\item $\text{div}v \neq0$.
Then:
\begin{align*}
\displaystyle
0 \neq \text{div} v=\sum_{i=1}^d \text{div}(  \phi_i) v_i = \dfrac{1}{K} \sum_{i=1}^d \alpha_i L_i v_i= \dfrac{1}{K} \sum_{i=1}^d \bar{v}_i 
\end{align*}
Then this always holds:
\begin{align*}
|v|^2 h^d C \leq \|v\|^2  \leq |v|^2 h^d C 
\end{align*}
Furthermore:
\begin{align*}
\int_K (\text{div} v)^2 =\dfrac{1}{K}\left(\sum_{i=1}^d \bar{v}_i \right)^2 \leq C \dfrac{1}{K} \left(\sum_{i=1}^d \bar{v}_i^2 \right)
\end{align*}
Then:
\begin{align*}
 \int_K (\text{div} v)^2 =\dfrac{1}{K}\left(\sum_{i=1}^d \bar{v}_i \right)^2=\dfrac{1}{K} \left(\sum_{i=1}^d  \bar{v}_i^2+\sum_{i\neq j}^d 2 \bar{v}_i  \bar{v}_j \right) \geq \dfrac{1}{C} \dfrac{1}{K} \left(\sum_{i=1}^d \bar{v}_i^2 \right)
\end{align*}
So the equality holds for $\dfrac{C-1}{C}  \geq  \gamma$ and $C-1 \geq C \gamma$, with $\gamma>1$, so $C(1-\gamma) \geq 1$.
\begin{align*}
\sum_{i=1}^d (C-1) \bar{v}_i^2 \geq -C\sum_{i=1}^d \sum_{j\neq i}^d  \bar{v}_i  \bar{v}_j \\
-\sum_{i=1}^d \sum_{j\neq i}^d  \bar{v}_i  \bar{v}_j \leq \sum_{i=1}^d  \bar{v}_i^2 
\end{align*}

\end{itemize}




\section{RELOAD EVERYTHING}
We now redefine the shape functions. In this way, the divergence of them is proportional to $1/h$, as the gradient of P1 functions:
\begin{align*}
\begin{cases}
\int_{F_i}  \bphi_i \cdot \bn_i = |F_i| \\
\bphi_i=\alpha_i \dfrac{|F_i|}{d |K|} \left( \bbx - \bp_i \right)=
\dfrac{\alpha_i}{h_i} \left( \bbx - \bp_i \right)
\end{cases}
\end{align*}
Given a $RT_0$ function on the level $j$ $v$, by definition:
\begin{align*}
v=\sum_{f} \phi_{f}^{RT_0} v_f=\sum_{i=1}^{m_c} v_i
\end{align*}
On the left hand side, each face is counted only one time, while on the right hand side each face appears $d$ times, where $d$ is the number of nodes, and so patches, which it belongs. These patches belong also to different colors and therefore:
\begin{align*}
v=\sum_{m=1}^{m_c}v_m= \sum_{m=1}^{m_c} \sum_{i \in m}  \sum_{f \in \omega_{s_i}} \phi_{f}^{RT_0}   \dfrac{v_f }{d} =\sum_{f} \phi_{f}^{RT_0} v_f
\end{align*}
Then we prove that, since the :
\begin{align*}
\sum_{m=1}^{m_{c}}  \|\text{div}\bsigma_m \|_{L^2(\Omega_m)}^2&=
 \sum_{m=1}^{m_{c}} \sum_{i \in m} \int_{\omega_i} |\text{div}\bsigma_m |^2= \sum_{i=1}^{m_{c}} \sum_{i \in m} \sum_{K \in {\omega_{i}}} \int_K |\text{div}\bsigma_m |^2\\
 &=  \sum_{i=1}^{m_{c}} \sum_{i \in m} \sum_{K \in {\omega_{i}}}\int_K \left[\text{div}\left(\sum_{f \in K} \bphi_f \dfrac{  \bsigma_{f} }{d} \right)\right]^2\\
 &=\sum_{i=1}^{m_{c}} \sum_{i \in m} \sum_{K \in {\omega_{i}}} |K| \left(\sum_{f \in K} \dfrac{ \alpha_f d}{h_f}\dfrac{  \bsigma_{f} }{d}\right)^2\\
  &=\sum_{i=1}^{m_{c}} \sum_{i \in m} \sum_{K \in {\omega_{i}}} |K| \left(\sum_{f \in K} \dfrac{ \alpha_f }{h_f} \bsigma_{f} \right)^2\\
\end{align*}
For the uniformity of the mesh exists $\gamma$ such that for each inner diameter $\rho_k$: $\rho_k \geq \dfrac{h}{\gamma}$. Also $h_i \geq \rho_k$ (AM I SO SURE?). 
Furthermore $|K| \leq  C(d) h^d$, with $C(d)=2 \pi$ for $d=2$ and $C(d)=3/4\pi$ for $d=3$ and using Caucy-Schwarz and $\left(\sum_{f \in K} \alpha_{f}^2 \right) \leq d+1$ for $\RT_0$.
\begin{align*}
\sum_{m=1}^{m_{c}}  \|\text{div}\bsigma_m \|_{L^2(\Omega_m)}^2
  &\leq \dfrac{C(d) \left( d+1 \right)}{\gamma^2} h^{d-2}\sum_{i=1}^{m_{c}} \sum_{i \in m} \sum_{K \in {\omega_{i}}} \sum_{f \in K}\bsigma_{f}^2\\
\end{align*}
Then we consider that on a patch, a face appears 2 times. Also on all colors a face appear $d$ times. Therefore:
\begin{align*}
\sum_{m=1}^{m_{c}}  \|\text{div}\bsigma_m \|_{L^2(\Omega_m)}^2
  &\leq 2\dfrac{C(d) d \left( d+1 \right)}{\gamma^2} h^{d-2} \sum_{f \in K}\bsigma_{f}^2\\
\end{align*}
By using the fact that a finite element space is equivalent to the euclidean norm through:
\begin{align*}
\|v\|_0^2 \cong h^d \sum_{f} v_f^2
\end{align*}
\begin{align*}
\sum_{m=1}^{m_{c}}  \|\text{div}\bsigma_m \|_{L^2(\Omega_m)}^2
  &\leq C h^{-2} \| \bsigma\|_{L^2(\Omega)}^2\\
\end{align*}
Since:
\begin{align*}
\displaystyle
  \| \bsigma_m \|_{L^2(\Omega_m)}^2  \cong h^d \sum_{f} \bsigma_{m,f}^2 \leq h^d \sum_{f} \bsigma_{f}^2 \cong   \| \bsigma_j \|_{L^2(\Omega)}^2 \\
\sum_{j=1}^J\sum_{m=1}^{m_{c}}  \| \bsigma_m \|_{L^2(\Omega_m)}^2  \leq m_c  \sum_{j=1}^J\| \bsigma_j \|_{L^2(\Omega)}^2 \leq 
\sum_{j=1}^J \| \bsigma_j \|_{L^2(\Omega)}^2
\end{align*}
THEN IF WE CAN PROVE THAT: $ \| \bsigma_j \|_{L^2(\Omega)} \leq  \| \bsigma\|_{L^2(\Omega)}$, we are fine.
we can say that:
\begin{align*}
\displaystyle
\sum_{j=1}^J \sum_{m=1}^{m_{c}} \left(\|\text{div}\bsigma_m \|_{L^2(\Omega_m)}^2+   \|\bsigma_m \|_{L^2(\Omega_m)}^2 \right) \leq \left(C h^{-2} + m_c \right)\sum_{j=1}^J\|\bsigma_j\|_{L^2(\Omega)}^2
\end{align*}







\section{Decomposition??????????}
We consider $J$ tassellation, built starting from a coarse mesh $T_1$ in the followinf way: given a mesh $T_{j-1}$, build the mesh $T_j$ by refine each element $K$, triangle or tetrahedron, by 4 triangles or 8 tetrahedra. To do so, define the midpoint on each edge and consequently create the elements.\\
We want to write, for a given element $K$:
\begin{align*}
\displaystyle
\sum_{i=1}^{(d+1)^2} \bphi_i u_i  &= \sum_{i=1}^{(d+1)^2} \bphi_i v_i +\sum_{k=1}^{d+1} \boldsymbol{\Phi}_i U_k \\
&= \sum_{i=1}^{(d+1)^2} \bphi_i v_i +\sum_{k=1}^{d+1}   \sum_{i=1}^{(d+1)^2}\alpha_{k j} \bphi_i U_k \\
&= \sum_{i=1}^{(d+1)^2} \bphi_i \left( v_i +\sum_{k=1}^{d+1} \alpha_{k j}  U_k  \right)
\end{align*} 
Then recursively this definition $\sum_{i=1}^{(d+1)^2} \bphi_i u_i  $ can be applied for $\sum_{k=1}^{d+1} \boldsymbol{\Phi}_i U_k $.\\
It is important to notice that the fine shape functions corresponding to a coarse edge/face will be part only fo the corresponding coarse shape functions, not of the others. Since contact constraints regard only the boundary, this means that only boundary dofs will influence the boundary constraints. So we can have control on this. Everywherelese, it does not really matter how we define $U_k$. Therefore we focus on the boundary. Once $U_k$ are known, the definitions of $v_i$ direclty follow. \\
Then we can make a transformation for each shape functions belonging to $\Gamma_C$. Let $Q$ be the transformation such that if $C_F$ is the dofs vector of the face $F$ 
\begin{align*}
\bsigma \bn = (\bphi \cdot \bn) \bC = \bH \bS \quad \to \quad \bC= \bQ \bS
\end{align*}
So on each contact edge/face (2D or 3D):
\begin{align*}
\begin{bmatrix}
\bphi_i\\
\bphi_i
\end{bmatrix}
\bc_i
=
\begin{bmatrix}
\bphi_i\\
\bphi_i
\end{bmatrix}
\bQ \bs_i
=
\begin{bmatrix}
\bpsi_i\\
\bpsi_i
\end{bmatrix}
\bs_i
\end{align*}
And we see that nothing changes for the definition of the coarse shape functions: \textbf{NOT SO SURE ABOUT THIS. INDEED I HAVE INTERNAL DOFS WHICH ARE MULTIPLIED BY Q.}
\begin{align*}
\begin{bmatrix}
\bPhi_k\\
\bPhi_k
\end{bmatrix}
\bC_k
=
\sum_{i=1}^{(d+1)^2}
\alpha_{jk}
\begin{bmatrix}
\bphi_i\\
\bphi_i
\end{bmatrix}
\bQ \bS_k
=
\sum_{i=1}^{(d+1)^2}
\alpha_{jk}
\begin{bmatrix}
\bpsi_i\\
\bpsi_i
\end{bmatrix}
\bS_k
\end{align*}
If the dof $i$ does not belong to $\Gamma_C$, then $\bphi_i=\bpsi_i$.\\
Now we know that the first component of $\bS$, denoted by $S_k$, is the one in which we are interested and we know it is positive. We define:
BUT WE CAN DO THE SAME FOR ALL THE OTHERS COMPONENTS
\begin{align*}
\displaystyle
S_k=\min_{i \in K} u_i, \quad u_i\leq \xi_i
\end{align*}
\textbf{ASSUMING external ALL NORMALS ON $\Gamma_C$ for all levels.
}In 2D for $i \in \Gamma_C$, only the relative dof survives (say $U_k=S_k>0$ but thos is true also for other components):
\begin{align*}
&u_i=v_i +\dfrac{1}{2} U_k \leq \xi_i\\
& v_i= u_i -\dfrac{1}{2}U_k\leq \xi_i- \leq \xi_i 
\end{align*}




























\section{Bounds for Operators}
Given $I_{h_j}$, it is always possible to bound:
\begin{align*}
\|I_{h_j}v \|_{L^2(K)} \leq \|v\|_{L^2(K)}
\end{align*}

This is due to the fact that, since $h/H_i\leq C$ and $|K| \leq C h^d$:
\begin{align*}
\|I_{h_j}v \|_{L^2(K)} \leq C h^d \|v\|_{L^{\infty}(K)} \leq C h^d (d+1)^2 |v|^2 \leq 
 C h^d  (d+1)^2  h^{-d} \|v\|_{L^2(K)}= C  \|v\|_{L^2(K)}
\end{align*}
Then we want to bound also the divergence. We observe that we can split $v=v_{ext}+v_{int}$, where the first/second one is $v$ whose dofs which are/are not in the interior of K are zero. Then:
\begin{align*}
I_{h_j}(v)=I_{h_j}(v_{ext}+v_{int})=I_{h_j}(v_{ext})
\end{align*}
Indeed if $g$ is the number of subdivision of each face ($g=2,4$ for $d=2,3$):
\begin{align*}
\beta_j L_j V_j=\int_{\partial K_j} I_{h_j}(v) \cdot \bn_j=\int_{\partial K_j} \sum_{i=1}^g  \alpha_i \bphi_i v_i \cdot \bn_j= \sum_{i=1}^{g_j} \dfrac{L_i}{g} \alpha_i  v_i 
\end{align*}
Then ( OK do this for 2D for now):
\begin{align*}
\text{div}(I_{h_j}(v))= \dfrac{1}{|K|}(\sum_{j=1}^{d+1} \beta_j L_j V_j)=\dfrac{1}{|K|}(\sum_{j=1}^{d+1}
\sum_{i=1}^{g_j} \dfrac{L_i}{g} \alpha_i  v_i )
\end{align*}
( OK do this for 2D for now):
\begin{align*}
|K|\text{div}(I_{h_j}(v))=&
\dfrac{L_{12}}{2} \alpha_1  v_1+ \dfrac{L_{12}}{2} \alpha_2  v_2 +\dfrac{L_{23}}{2} \alpha_3 v_3+ \dfrac{L_{23}}{2}  \alpha_4  v_4 +  
\dfrac{L_{13}}{2} \alpha_5 v_5+\dfrac{L_{13}}{2}   \alpha_6  v_6 \\
=&
\dfrac{L_{12}}{2} \alpha_1  v_1+ \dfrac{L_{12}}{2} \alpha_2  v_2 +L_7 \alpha_7  v_7+\\
&
\dfrac{L_{23}}{2} \alpha_3 v_3+ \dfrac{L_{23}}{2}  \alpha_4  v_4 +  
L_8 \alpha_8  v_9+\\
&
\dfrac{L_{13}}{2} \alpha_5 v_5+\dfrac{L_{13}}{2}   \alpha_6  v_6+
L_9 \alpha_9  v_9+ \\
&
- L_7 \alpha_7  v_7- L_8 \alpha_8  v_8-L_9 \alpha_9  v_9  \\
\end{align*}
\begin{align*}
|K|^2 \text{div}(I_{h_j}(v))^2\leq &
4 \left[\dfrac{L_{12}}{2} \alpha_1  v_1+ \dfrac{L_{12}}{2} \alpha_2  v_2 +L_7 \alpha_7  v_7\right]^2+\\
&4\left[\dfrac{L_{23}}{2} \alpha_3 v_3+ \dfrac{L_{23}}{2}  \alpha_4  v_4 +  
L_8 \alpha_8  v_9\right]^2+\\
&4\left[\dfrac{L_{13}}{2} \alpha_5 v_5+\dfrac{L_{13}}{2}   \alpha_6  v_6+
L_9 \alpha_9  v_9\right]^2 +\\
&4\left[L_7 \alpha_7  v_7+ L_8 \alpha_8  v_8 + L_9 \alpha_9  v_9\right]^2\\
\end{align*}
Therefore:
\begin{align*}
\int_K \text{div}(I_{h_j}(v))^2 \leq \sum_{i=1}^4 \dfrac{4}{|K|}\sum_{K_s}\int_{K_s} (\text{div}(v))^2 = \int_K (\text{div}(v))^2
\end{align*}
So that:
\begin{align*}
\| I_{h_j}v\|_{H_{div}}\leq  \|v\|_{H_{div}}
\end{align*}
\textbf{But HERE we have used the mean value, not the minimum!!!!}

\section{Bound L2 norm with divergence FOR SINGLE QUANTITY}
We consider a patch $P_i$ and all the faces $k \in P_i$ on its border. 
\begin{align*}
\norm{\theta_i u}_U^2=\norm{\theta_i u}_{L^2}^2+\norm{\text{div} \theta_i u}_{L^2}^2 + \norm{\epsilon (\theta_i u)}_{L^2}^2 
\end{align*}
We consider now just the $H_{\text{div}}$ part.
\begin{align*}
\norm{\sum_{j \in P_i} \phi_i u_i}_{H_{\text{div}}(P_i)} &=
\norm{u -\sum_{k} \phi_k u_k }_{H_{\text{div}}(P_i)} \\
&\leq \norm{u}_{H_{\text{div}}(P_i)} + \norm{\sum_{k} \phi_k u_k }_{H_{\text{div}}(P_i)}\\
& \leq  \norm{u}_{H_{\text{div}}(P_i)}+ T \sum_{k}  \norm{\phi_k u_k }_{H_{\text{div}}(P_i)}
\end{align*}
Then we use the fact the $\RT_0$ space is a FE space for bounding the $L^2$ part:
\begin{align*}
 \norm{\phi_k u_k }_{L^2(P_i)}^2 \leq C h^d |u_k|^2 \leq  C h^d \left( \sum_{j \in P_i} u_i^2+\sum_{k} u_k^2 \right)  \leq  C   \norm{u }_{L^2(P_i)}^2
\end{align*}
For the divergnece part,\textbf{ WE LOOSE. AIM ZORRY.}
 we want to bound:
\begin{align*}
 \norm{\text{div}\phi_k u_k }_{L^2(P_i)}=\int_K \left( \dfrac{d}{H_k} \right)^2  u_k^2 \leq C \int_K \dfrac{(\bbx-\bp_k)^2}{H_k^2}  u_k^2 =C\norm{\phi_k u_k }_{L^2(P_i)} 
\end{align*}
We want to bound:
\begin{align*}
d^2 |K| \leq C \int_K (\bbx-\bp_k)^2 \leq C L^2 |K| \quad \to \quad C= \dfrac{d^2}{L^2}
\end{align*}
\begin{align*}
 \int_K \dfrac{(\bbx-\bp_k)^2}{H_k^2}  u_k^2 \geq \int_K \dfrac{(\bbx-\bp_k)^2}{L^2 H_k^2}  u_k^2
\end{align*}
\textbf{NO FALSO, VALE INVECE:}
\begin{align*}
 \int_K \dfrac{(\bbx-\bp_k)^2}{H_k^2}  u_k^2 \leq \int_K \dfrac{L^2d^2 }{d^2 H_k^2}  u_k^2 =\left(\dfrac{L}{d}\right)^2\int_K \dfrac{d^2}{H_k^2} u_k^2
\end{align*}
\section{Bound L2 norm with divergence}
Assume that $\text{div}v \neq 0$:
\begin{align*}
\text{div}v =\sum_{i=1}^{d+1} d \dfrac{\alpha_i}{H_i} v_i=\sum_{i=1}^{d+1}  \bar{v}_i d \neq 0
\end{align*}
Then we compute:
\begin{align*}
\|v \|_{L^2(K)}^2=\int_K \left(\sum_{i=1}^{d+1} (\bbx-\ba_i)  \dfrac{\alpha_i}{H_i} v_i \right)^2 =\int_K \left( \bbx \sum_{i=1}^{d+1} \bar{v}_i - \sum_{i=1}^{d+1} \ba_i\bar{v}_i   \right)^2 
\end{align*}
Since $\text{div}v \neq 0$, it is possible to define:
\begin{align*}
\ba_0 = \dfrac{\sum_{i=1}^{d+1} \ba_i\bar{v}_i }{\sum_{i=1}^{d+1}  \bar{v}_i }
\end{align*}
Therefore:
\begin{align*}
\|v \|_{L^2(K)}^2=\int_K \left( (\bbx-\ba_0)\sum_{i=1}^{d+1} \bar{v}_i    \right)^2 = \int_K  \left(\sum_{i=1}^{d+1} \bar{v}_i    \right)^2 |\bbx-\ba_0|^2=\int_K \left(\sum_{i=1}^{d+1} \bar{v}_i    \right)^2  \sum_{w=x,y,z} (w-a_0^w)^2
\end{align*}
We have:
\begin{align*}
w-a_0^w=w -  \dfrac{\sum_{i=1}^{d+1} a_i^w \bar{v}_i }{\sum_{i=1}^{d+1}  \bar{v}_i }=
\dfrac{w \sum_{i=1}^{d+1}  \bar{v}_i  -\sum_{i=1}^{d+1} a_i^w \bar{v}_i }{\sum_{i=1}^{d+1}  \bar{v}_i }=
\dfrac{\sum_{i=1}^{d+1} (w- a_i^w )\bar{v}_i }{\sum_{i=1}^{d+1}  \bar{v}_i }
\end{align*}
So:
\begin{align*}
\displaystyle
\|v \|_{L^2(K)}^2&=\int_K \left( \sum_{i=1}^{d+1} \bar{v}_i   \right)^2 \sum_{w=x,y,z} \left( \dfrac{\sum_{i=1}^{d+1} (w- a_i^w )\bar{v}_i }{\sum_{i=1}^{d+1}  \bar{v}_i } 
\right)^2 = 
\int_K
 \sum_{w=x,y,z} (d+1)h^2 \left( \sum_{i=1}^{d+1} \bar{v}_i^2\right)\\
&\leq \int_K d(d+1) h^2 \left( \sum_{i=1}^{d+1} \bar{v}_i^2\right)
\end{align*}
\section{Proof for the convergence of the 1-level method}
\begin{enumerate}[label=\Roman*.]
\item $U = \left[ H^1(\Omega) \right]^2 \times \left[H_{\text{div}}(\Omega)\right]^2$ is an Hilbert space, so it is also Banach and reflexive (right?). Furthermore the functional is Gateaux-differentiable, coercive and satisfies the other properties.
\item The property of the convex set is automatically satisfied for the 1-level case. Indeed we define:
\begin{align*}
\phi_J=\phi-w \quad K_J=\{v \in V_{hj}: v \leq \phi_J \}
\end{align*}
\item  \begin{itemize}
\item With $\beta_{jk}=1$:
\begin{align*}
\langle F'(v+v_{ij})-F'(v),v_{kl}\rangle =\langle F'(v_{ij}),v_{kl}\rangle \leq
\beta  \|v_{ij} \|  \|v_{kl}\|
\end{align*}
\item Consider a color $m$ and all $i \in m$. Then it holds:
\begin{align*}
\norm{ \sum_{ i \in m} w_{ji}}^2 = \sum_{i\in m}  \|w_{ji}\|^2 
\end{align*}
Then $C_1=\sqrt{m_j}$:
\begin{align*}
\norm{\sum_{i=1}^{I_j} w_{ji}}^2 =\norm{\sum_{m=1}^{m_j}  \sum_{i\in m}  w_{ji}}^2 \leq m_j \sum_{m=1}^{m_j} \norm{  \sum_{i\in m}  w_{ji}}^2 =   m_j \sum_{m=1}^{m_j} \sum_{i\in m}  \|w_{ji}\|^2 
\end{align*}
\item
\end{itemize}
\end{enumerate}


\section{Bound $\norm{u_{ji}}_{H_{div}}^2 \leq \dfrac{1}{h}\norm{u_{j}}_{H_{div}} $}
We want to show this inequality:
\begin{align*}
\norm{u_{ji}}_{H_{div}}^2 \leq \dfrac{1}{h}\norm{u_{j}}_{H_{div}} 
\end{align*}
It would be desirable to remove the $h$, but it seems difficult (impossible?).\\
Then on each patch $P_i$ we have $u_{ji}$ that has only the internal face-dofs. To recover $u_j|_{P_i}$, we have to add and subtract the border dofs of the patch. Since on each element $K\in P_i$, there is, by definition, only one border face, we are removing and adding the corresponding border dof. We are going to obtain a norm of $u_j$ and one of the external border dofs. The $L^2$ norm of these, since $\RT_0$ is FE space, can be bounded again by $\norm{u_j}_{L^2(P_i)}$.
\begin{align*}
\norm{u_{ji}}_{H_{div}}^2=\norm{\sum_{j \in P_i} \bphi_j u_j+\sum_{k \in \partial P_i} \bphi_k u_k-\sum_{k \in \partial P_i} \bphi_k u_k }_{H_{div}}^2\leq C \norm{u_j}_{H_{div}}^2 + C \abs{\sum_{k \in \partial P_i} \bphi_k u_k }_{H_{div}}^2
\end{align*}
Now we exploit the fact that (\textbf{IS THIS TURE?}):
\begin{align*}
\abs{\sum_{k \in \partial P_i} \bphi_k u_k }_{H_{div}}^2&=\sum_{k \in \partial P_i} \abs{K} \left( \dfrac{d}{H_i}u_i\right)^2\\
&=\sum_{k \in \partial P_i} \dfrac{L_i H_i}{d} \left( \dfrac{d}{H_i}u_i\right)^2\\
&=\sum_{k \in \partial P_i} \dfrac{d}{H_i} L_i u_i^2\\
& \leq \dfrac{C}{h} \norm{u_{j,n}}_{L^2(\partial P_i)}^2 \\
&\leq \dfrac{C C_T}{h}  \norm{u_j}_{H_{div}(P_i)}^2
\end{align*}
Therefore:
\begin{align*}
\norm{u_{ji}}_{H_{div}}^2 \leq \dfrac{C}{h} \norm{u_j}_{H_{div}}^2 
\end{align*}
Regarding the norm $\norm{\boldsymbol{\varepsilon}(\bu)}$, we know that we can bound $\norm{\boldsymbol{\varepsilon}(\bu_{ji})}\leq C \norm{\nabla\bu_{ji}}$. Also Korn's inequality holds if homogeneous Dirichlet bc are satisfied on a non-zero measure set. But all the correction $\bw_{ji}$ have for sure zero Dirichlet bc, because otherwise, adding them to the current iterate, would give rise to a function which is not in the convex set. Also $\bu \in K_j$ and $\bw \in K_j$ imply that $\bu -\bw$ will be zero on the Dirichlet boundary. Therefore we can use the Korn's inequality:
\begin{align*}
\sum_{j=1}^J \sum_{i=1}^{I_j} \norm{\boldsymbol{\varepsilon}(\bu_{ji}) }^2 &\leq C\sum_{j=1}^J \sum_{i=1}^{I_j} \norm{\nabla\bu_{ji} }^2\\
&
\leq C \left( \norm{   \nabla\left(\sum_{j=1}^J \sum_{i=1}^{I_j}\bu_{ji} \right)}^2 +  \sum_{j=1}^J \sum_{i=1}^{I_j} \norm{\nabla \bw_{ji}}^2 \right) \\
&
\leq C \left( \norm{\boldsymbol{\varepsilon}  \left(  \sum_{j=1}^J \sum_{i=1}^{I_j} \bu_{ji} \right)}^2 +  \sum_{j=1}^J \sum_{i=1}^{I_j} \norm{ \boldsymbol{\varepsilon}(\bw_{ji})}^2 \right) 
\end{align*}
Here we are using Badea's theorem. But he uses the full $H^1$ norm, we just use the seminorm. Anyhow, norm and seminorm are the same in case of homogeneous bc.\\\\
\textbf{I DO NOT KNOW IF IT CAN BE USEFUL, BUT ON A PATCH, SINCE WE HAVE ZERO DOFS ON THE BOUNDARY, THE FUNCTION HAS ZERO DIVERGENCE IN AVERAGE.}
\section{Tai}
% \begin{itemize}
% \item $V$ reflexive Banach space
% \item $\mathcal{J}: V \to \mathbb{R}$ convex functional satisfying:
% \begin{align*}
% \langle F'(w)-F'(v),w-v \rangle \geq K \|w-v \|_V^2 \qquad  \| F'(w)-F'(v) \|_{V'}  \leq L \|w-v \|_V
% \end{align*}
% \item $\forall v \in V$, $\exists$ $v_i \in V_i$ and $C_1 >0$ such that:
% \begin{align}
%\displaystyle v=\sum _{i}^{m} v_i \qquad \qquad  \sum _{i}^{m} \|v_i \|_V^2 \leq C_1 \| v \|_V^2
% \end{align}
% \item $\forall w_{ij} \in V$, $u_{i} \in V$,  $v_{j} \in V$, $\exists$ $C_2>0$ such that:
%  \begin{align}
%\displaystyle v=\sum _{i,j}^{m} \langle F'(w_{ij}+u_i)-F'(w_{ij},v_j \rangle \leq C_2 \left( \sum _{i}^{m} \|u_i \|^2 \right)^{1/2} \left( \sum _{i}^{m} \|v_i \|^2 \right)^{1/2}
% \end{align}
% \end{itemize}
%Then $\exists$ $\delta(K,L,C_1,C_2) \in (0,1)$ such that $d_n \leq \delta^n \delta_0$:
%\begin{align}
%d_n=F(u^n)-F(u)
%\end{align}
\begin{itemize}
\item Assumption 2.3\\

\begin{align*}
\| u \|_{1,\text{supp}(u) \cap\text{supp}(v)}  \leq C  \gamma^{d |j - l |} \| u\|_1\\
\| u\: \text{supp}(v) \|_{L^2} \leq \| u\|_{L^2}\: \| \text{supp}(u) \cap\text{supp}(v) \|_{\infty}
\end{align*}
Consider  two levels j,k with $j \leq k$, and the corresponding subspace i, l. We want to estimate $\| \text{div} \bsigma^{j,i}\|_{\text{supp}(\bsigma^{j,i}) \cap\text{supp}(\bsigma^{k,l})}$. 
We define $\omega_{j,i}=\text{supp}(\bsigma^{j,i})$, $\omega_{k,l}=\text{supp}(\bsigma^{k,l})$, $K_{j/k}$ as an element belonging to $\omega_{j,i}$/ $\omega_{k,l}$, $ \text{div}\bsigma_i=\text{div}\bsigma^i|_{K_i}$, $\text{div}\bsigma_j= \text{div}\bsigma_i$ if $K_j \in K_i$:
\begin{align*}
\displaystyle
\|\text{div} \bsigma^{j,i}\|_{\text{supp}(\bsigma^{k,l}) \cap\text{supp}(\bsigma^{j,i})}^2= \sum_{K_{k} \in \omega_{j,i} \cap \omega_{k,l}} |K_{k}| |\text{div}\bsigma|_{K_k}^2 
=\\
 \sum_{K_{j} \in \omega_i} |\text{div}\bsigma|_{K_{j}}^2  \sum_{K_k \in K_j \cap \omega_j} |K_k|
\leq C  \left(\dfrac{h_j}{h_i}\right)^{d |j-i|}  \| \text{div} \bsigma^{j,i} \|_{\text{supp}(\bsigma^i)}^2
\end{align*}
The constant $C=C(d)$ depends on the dimension of the problem. For $d=2$, $C=6$; for $d=3$, $C=5$ (CREDO, NON RIESCO A VEDERE IL REFINEMENT DI TETRAEDRI 3D). Indeed in 2D a fine patch is contained in a coarser triangle respectively.
We have used the fact that  $\sum_{K_k \in K_j \cap \omega_j} |K_j|
\leq C  \left(\dfrac{h_k}{h_j}\right)^{d |j-i|}  |K_i|$
\end{itemize}
\section{ARNOLD Hdiv PROJECTION}
we want to show:
\begin{align*}
\sum_{n \in N} \Lambda(\phi_n,\phi_n)= \Lambda(\sum_{n \in N} \phi_n,\sum_{n \in N}\phi_n)= \Lambda(\phi,\phi)
\end{align*}
We know this holds for 
\section{Contact boundary}
If the domain is locally Lipschitz, we can define local coordinates such that:
\begin{align*}
\gamma_{loc}=\{ (y_1,y_2,y_3): \: y_3 = \eta_{loc}(y_1,y_2), |y_1|<\alpha_1,\varepsilon, |y_2|<\alpha_2,\varepsilon, \varepsilon \:\:\varepsilon \text{sufficiently small}  \}
\end{align*}
So that locally we can represent the points : 
\begin{align*}
x_3 = \eta_x (x_1,x_2)
\end{align*}
Nota bene. Direi che il sistema locale di coordinate vede la "z" come la coordinata normale. \\
Sto dicendo che l'altezza (dall'alto) del punto spostato X deve essere inferiore a quella dell'ostacolo valutata nel punto spostato X. \\
ASSUNZIONE IMPLICITA: normali esterne sono tra loro vicine. Forse intende dire che se cosi non fosse, potrei avere un ostacolo con diverse concavita' e convessita', per cui nello spostare il punto, questo andrebbe a pentrare una protuberanza intermedia.
\begin{align*}
\eta_x(x_1,x_2)+u_3(x_1,x_2,\eta_x(x_1,x_2)) \leq \eta_y(x_1+u_1(x_1,x_2,\eta_x(x_1,x_2)),x_2+u_2(x_1,x_2,\eta_x(x_1,x_2)))
\end{align*}
Per piccoli spostamenti:
\begin{align*}
\eta_x(x_1,x_2)+u_3 \leq \eta_y(x_1,x_2)+\left( \dfrac{\partial \eta_y}{\partial y_1},\dfrac{\partial \eta}{\partial y_2}\right) \cdot \left(u_1,u_2\right)^T
\end{align*}
che porta a:
\begin{align*}
\bn_y \cdot (u_1,u_2,u_3)^T \leq G(x)
\end{align*}
dove la normale e' quella sull'ostacolo. Per piccoli spostamenti, la si confonde con quella del corpo.


\section{Poisson equation}
\begin{align*}
\begin{cases}
\bu+\nabla p=0\\
\text{div} \bu = f
\end{cases}
\end{align*}
\begin{align*}
\begin{cases}
\int_{\Omega}\bu \bv-\int_{\Omega}  p \text{div} \bv +\int_{\partial \Omega} p \bv \cdot \bn=0 \\ 
\int_{\Omega} \text{div} \bu  q= \int_{\Omega} f q\\
\end{cases}
\end{align*}
\begin{itemize}
\item $u_x$:
\begin{align*}
J(u_x,u_x)= \int \phi_u test_u  \qquad J(u_x,u_y)= 0 \quad J(u_x,p)=  -\int \phi_p test_{u,x}
\end{align*}
\item $u_y$:
\begin{align*}
J(u_y,u_x)= 0  \qquad J(u_y,u_y)= \int \phi_u test_u \quad J(u_y,p)=  - \int \phi_p test_{u,y}
\end{align*}
\item $p$:
\begin{align*}
J(p,u_x)= \int \phi_{u,x} test_p   \qquad J(p,u_y)= \int \phi_{u,y} test_p \quad J(p,p)=  0
\end{align*}
\end{itemize}
\section{Dual Linear Elasticity}
\begin{align*}
\begin{cases}
\mathcal{A} \bsigma - \boldsymbol{\varepsilon}(\bu)=0\\
\text{div} \bsigma + \bff=0
\end{cases}
\end{align*}
\begin{align*}
\begin{cases}
\int_{\Omega} \mathcal{A} \bsigma : \btau - \int_{\Omega}\boldsymbol{\varepsilon}(\bu): \btau=0\\
\int_{\Omega}  \left( \text{div} \bsigma + \bff \right) \bv=0
\end{cases}
\end{align*}
\begin{align*}
\begin{cases}
\int_{\Omega} \mathcal{A} \bsigma : \btau -\int_{\Omega}\nabla \bu : \frac{1}{2}(\btau+\btau^T)=0\\
\int_{\Omega}  \left( \text{div} \bsigma + \bff \right) \bv=0
\end{cases}
\end{align*}
\begin{align*}
\begin{cases}
\int_{\Omega} \mathcal{A} \bsigma : \btau +\frac{1}{2}\int_{\Omega} \bu\: \text{div}\left(\btau+\btau^T \right)- \frac{1}{2}\int_{\partial \Omega} \bu \cdot \left(\btau+\btau^T \right)\bn =0\\
\int_{\Omega}  \left( \text{div} \bsigma + \bff \right) \bv=0
\end{cases}
\end{align*}
In case the simmetry is strongly enforced in the space:
\begin{align*}
\begin{cases}
\int_{\Omega} \mathcal{A} \bsigma : \btau +\int_{\Omega} \bu\: \text{div}\btau- \int_{\partial \Omega} \bu \cdot \btau \bn =0\\
\int_{\Omega}  \left( \text{div} \bsigma + \bff \right) \bv=0
\end{cases}
\end{align*}
Being:
\begin{align*}
 \mathcal{A} \bsigma : \btau =  \beta \bsigma : \btau +  \alpha \text{tr}(\bsigma)\text{tr}(\btau) 
\end{align*}
\begin{itemize}
\item $\btau_{raw1}$:\\
\begin{align*}
\int_{\Omega}  \beta \bsigma_{raw1} \cdot \btau_{raw1} +  \alpha \left(\bsigma_{raw1,1}+\bsigma_{raw2,2} \right) \btau_{raw1,1}
+\int_{\Omega} \bu_1\: \text{div}\btau_{raw1}\\
\end{align*}
\item $\btau_{raw1}$:\\
\begin{align*}
\int_{\Omega}  \beta \bsigma_{raw2} \cdot \btau_{raw2} +  \alpha \left(\bsigma_{raw1,1}+\bsigma_{raw2,2} \right) \btau_{raw2,2}
+\int_{\Omega} \bu_2\: \text{div}\btau_{raw2}\\
\end{align*}
\item  $v_{1}$:\\
\begin{align*}
\int_{\Omega} \bv_1\: \text{div}\bsigma_{raw1}\\
\end{align*}
\item  $v_{2}$:\\
\begin{align*}
\int_{\Omega} \bv_2\: \text{div}\bsigma_{raw2}\\
\end{align*}
\end{itemize}
\section{LSFEM Linear Elasticity}
\begin{align*}
\begin{cases}
& \left( \boldsymbol{\varepsilon} (\bu) -\mathcal{A} \bsigma , \boldsymbol{\varepsilon} (\bu_h ) \right)=0\\
&  \left( \text{div} \bsigma + \bff,  \text{div} \bsigma_h \right)
+ \left( \mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu), \mathcal{A} \bsigma_h \right)=0
\end{cases}
\end{align*}
We obtain the Jacobian:
\begin{itemize}
\item  $v_x$:\\
\begin{align*}
u_{x,x}v_{x,x} + \dfrac{u_{x,y}+u_{y,x}}{2} v_{x,y} - \beta \left(\sigma_{xx} v_{x,x} +\dfrac{\sigma_{xy}+\sigma_{yx}}{2} v_{x,y} \right)-\alpha \left(\sigma_{xx}+\sigma_{yy}\right) v_{x,x}
\end{align*}
\item  $v_y$:\\
\begin{align*}
u_{y,y}v_{y,y} + \dfrac{u_{x,y}+u_{y,x}}{2} v_{y,x} - \beta \left(\sigma_{yy} v_{y,y} +\dfrac{\sigma_{xy}+\sigma_{yx}}{2} v_{y,x} \right)-\alpha \left(\sigma_{xx}+\sigma_{yy}\right) v_{y,y}
\end{align*}
\item  $\tau_{raw,1}$:\\
\begin{align*}
\left( \text{div} \bsigma_{raw1} f_1\right) \text{div} \btau_{raw1} +
\beta^2 \bsigma_{raw1} \btau_{raw1} +
\tau_{raw1,1} \left( \sigma_{raw1,1} + \sigma_{raw2,2} \right)  \left( 2 \alpha \beta + \text{dim}\: \alpha^2 \right)\\
-\beta \left( u_{x,x} \tau_{raw1,1} +\dfrac{u_{x,y}+u_{y,x}}{2} \tau_{raw1,2} \right)
-\alpha \left( u_{x,x} +u_{y,y} \right) \tau_{raw1,1}
\end{align*}
\item  $\tau_{raw,2}$:\\
\begin{align*}
\left( \text{div} \bsigma_{raw2} f_2\right) \text{div} \btau_{raw2} +
\beta^2 \bsigma_{raw2} \btau_{raw2} +
\tau_{raw2,2} \left( \sigma_{raw1,1} + \sigma_{raw2,2} \right)  \left( 2 \alpha \beta + \text{dim}\: \alpha^2 \right)\\
-\beta \left( u_{y,y} \tau_{raw2,2} +\dfrac{u_{x,y}+u_{y,x}}{2} \tau_{raw2,1} \right)
-\alpha \left( u_{x,x} +u_{y,y} \right) \tau_{raw2,2}
\end{align*}
\end{itemize}
The Jacobian is (in 11 add $\text{div}{\sigma_1}
\text{div}{\tau_1}$, in 22 add $\text{div}{\sigma_2}
\text{div}{\tau_2}$):
{\tiny
\begin{align*}
\begin{bmatrix}
\beta^2 \sigma_{xy}\tau_{xy} + \sigma_{xx}\tau{xx}  (\alpha^2 +(\alpha+\beta)^2) & 
2  \alpha (\alpha+\beta) \sigma_{yy} \tau_{xx}  &
 - (\tau_{xx} u_{x,x}(\alpha+\beta) + 0.5 \beta \tau_{xy} u_{x,y})&
  -(\alpha \tau_{xx} u_{y,y} + 0.5 \beta \tau_{xy} u_{y,x}\\
 2  \alpha (\alpha+\beta) \sigma_{xx} \tau_{yy} & 
 \beta^2 \sigma_{yx}\tau_{yx} + \sigma_{yy}\tau_{yy}  (\alpha^2 +(\alpha+\beta)^2)&
  - (\alpha \tau_{yy} u_{x,x} + 0.5 \beta  \tau_{yx} u_{x,y} &
   - ((\alpha+\beta) \tau_{yy} u_{y,y} + 0.5 \beta \tau_{yx} u_{y,x}) ) \\
 - (\sigma_{xx} v_{xx} (\alpha + \beta) +0.5 \beta \sigma_{xy} v_{x,y}) )&                 
 -( \alpha \sigma_{yy} v_{x,x} +0.5  \beta \sigma_{yx} v_{x,y})&     
 u_{x,x} v_{x,x} + 0.5 (u_{x,y}v_{x,y})&
 0.5 u_{y,x} v_{x,y}\\
 - (\alpha \sigma_{xx} v_{y,y} +0.5 \beta \sigma_{xy} v_{y,x})&
-( \sigma_{yy}v_{y,y} (\alpha + \beta) + 0.5 \beta \sigma_{yx} v_{y,x})&                              
 0.5 u_{x,y} v_{y,x}&
0.5 u_{y,x} v_{y,x}+ u_{y,y} v_{y,y}
\end{bmatrix}
\end{align*}
}
\subsection{Linear elasticity equations}

\section{Signorini problem: strong formulation}
\begin{align*}
\begin{cases}
\text{div} \bsigma + \bff=0 & \Omega  \qquad \text{momentum balance equation}\\
\mathcal{A} \bsigma - \boldsymbol{\varepsilon}(\bu)=0 &\Omega \qquad \text{constitutive law}\\
\bu = \bu^D & \Gamma_D\\
\bsigma \cdot \bn = \bt^N & \Gamma_N\\
\end{cases} 
\end{align*}
with the constraints:
\begin{align*}
\begin{cases}
\bu \cdot \bn - g  \leq 0 & \Gamma_D \qquad \text{impenetrability}\\
(\bsigma \bn) \cdot \bn \leq 0 &\Gamma_N \qquad \text{direction of the surface pressure}\\
\langle \bu \cdot \bn -g, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_c}=0 & \Gamma_C \subset \Gamma_N \qquad \text{complementarity condition}
\end{cases}
\end{align*}
Between the two possible formulations, the one that uses $\mathcal{A} \bsigma -\boldsymbol{\varepsilon}=0$ is preferred to $\bsigma - \mathcal{C} \boldsymbol{\varepsilon}=0$. In this way, we can also deal with near incompressible or incompressible materials ($\lambda \gg 1$, $\lambda \to \infty$).
\begin{align*}
&\mathcal{A} \bsigma= \dfrac{1}{2 \mu} \left(\bsigma-\dfrac{\lambda}{d \lambda + 2 \mu } \text{tr} \bsigma \bI\right)\\
&\mathcal{C} \boldsymbol{\varepsilon}=\lambda \text{tr} \boldsymbol{\varepsilon}(\bu) \bI+ 2 \mu \boldsymbol{\varepsilon}(\bu)
\end{align*}


\section{Spaces}
\begin{align*}
&W^{m,p}(\Omega) =\{v \in L^p(\Omega) ; \quad \partial^{\alpha}  v \in L^p(\Omega) \quad \forall |\alpha| \leq m \}\\
&\| v \|_{m,p,\Omega}=\left( \sum_{|\alpha| \leq m \int_{\Omega} } | \partial^{\alpha} v(x) |^{\alpha} \right)^{1/p}
\end{align*}
where $W^{-m,p'}(\Omega)$, with $1/p+1/p'=1$ is the dual space normed by:
\begin{align*}
&\| f \|_{-m,p',\Omega}=\sup_{v \neq 0  v \in W_0^{m,p}(\Omega)} \dfrac{ \langle f, v \rangle }{\| v\|_{m,p,\Omega} }
\end{align*}
A lemma characterizes the functionals of $W^{-m,p'}(\Omega)$:
\begin{align*}
f \in W^{-m,p'}(\Omega) \quad \iff \quad \exists  f_{\alpha}=  \sum_{|\alpha| \leq m  }| \partial^{\alpha} f_{\alpha}|\in L^{p'}(\Omega)  
\end{align*}
For $p=2$,  $W^{m,2}(\Omega)$ can be defined in a different way by using the Fourier transforms. For real $s >0$:
\begin{align*}
&H^{s}(\mathbb{R}^N) =\{v \in L^2(\mathbb{R}^N) ; \quad (1+\| x\|^{2} )^{1/2}  \hat{v}(x) \in L^2(\mathbb{R}^N) \quad \forall |\alpha| \leq m \}\\
&\| v \|_{s,\mathbb{R}^N}=\left(|| v ||_{L^{2}(\mathbb{R} )}^{2} +|| (1+ || x||^{2} )^{s/2} \hat{v}(x)||_{L^{2}(\mathbb{R} )}^{2}  \right)^{1/2}\\
&H^{s}(\Omega) =\{ v \in L^2\: \quad \exists \tilde{v} \in H^{s}\left(\mathbb{R}^N\right) \:\: \text{with} \: \tilde{v} =v  \: \text{on} \:\Omega \}\\
&||v||_{s,\Omega} = \inf_{ \tilde{v} \in H^{s}\left(\mathbb{R}^N\right), \tilde{v} = v \: \text{in} \: \Omega} || \tilde{v}||_{s,\mathbb{R}^N}
\end{align*}
Let $\Omega$ be a Lipschitz-continuous bounded open subset of $\mathbb{R}^N$. Then:
\begin{align*}
&H^{s}(\Omega) = W^{s,2}\left(\Omega \right)
\end{align*}
\textbf{Theorem}\\
Let $\Omega$ be like in the previous definition (WHICH ONE) and let $p \geq 1$, $s \geq 0$ be two real numbers such that $s \leq  k + 1$, $s-1/p= l + \sigma$ where $l \geq 0$ is an integer and $\sigma < 1$. Then the mapping $ u \to \gamma_0 u$ defined on $\mathcal{D}(\bar{\Omega})$ has a unique linear continuous extension as an operator from: 
\begin{align*}
W^{s,p}\left(\Omega \right) \quad \text{onto} \quad W^{s-1/p,p}\left(\Gamma \right) 
\end{align*}
Moreover, in $W^{1,p}$ we have Ker$(\gamma_0)=W_0^{1,p}$. The norm that can be used is:
\begin{align*}
&||f||_{s-1/p,p,\Gamma} = 
\inf_{ \tilde{v} \in W^{s,p}\left(\Omega \right), \gamma_0 \tilde{v} = f \: \text{in} \: \Omega } 
|| \tilde{v}||_{s,p,\Omega}
\end{align*}

\begin{align*}
&H_d^1(\Omega) = \{q \in H^1 \left(\Omega \right): \:q=0 \:\text{on}\:\Gamma_d \}\\ 
&H_d^1(\text{div},\Omega) = \{\bw \in H^1 \left(\text{div},\Omega \right): \: \bw \cdot \bn=0 \:\text{on}\:\Gamma_N \}\\
&V_h \subset H_d^1(\Omega)\\
&\Sigma_h \subset H_d^1(\text{div},\Omega)\\
&H_d^{1/2}(\Gamma_d) = \{v \in L^2 \left(\Gamma_d \right): \: \exists u \in H_d^1(\Omega), \:v=\text{tr} (u) \}\\ 
&H_d^{-1/2}(\Gamma_N) \: \: \qquad H_d^{1/2}(\Gamma_N)  \text{ dual space}\\  
&|| f ||_{-1/2,\Omega}=\sup_{v \neq 0 , \: v \in W_0^{1/2}(\Omega)} \dfrac{ \langle f, v \rangle }{|| v||_{1/2,\Omega} }
\end{align*}



\section{Least squares functionals}
The least squares functional:
\begin{align*}
\mathcal{F}_C(\bu,\bsigma)=||\text{div} \bsigma+\bff||^2+||\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu)||^2 \geq 0
\end{align*}
and the augmented least squares functional (positive thanks to the constraints):
\begin{align*}
\mathcal{F}(\bu,\bsigma)=||\text{div} \bsigma+\bff||^2+||\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu)||^2+\langle \bu \cdot \bn, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C} \geq 0  
\end{align*}
Searching for an approximate solution $(\bu_h^{\perp},\bsigma_h^{\perp})$ such that exactly satisfy:
\begin{align*}
\langle \bu \cdot \bn, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C}=0
\end{align*}
means that (being the set of $(\bu_h^{\perp},\bsigma_h^{\perp})$ smaller than the one in which we search for $(\bu_h^{},\bsigma_h^{})$):
\begin{align*}
\mathcal{F}(\bu_h,\bsigma_h) \leq \mathcal{F}_C(\bu_h,\bsigma_h) \leq \mathcal{F}_C(\bu_h^{\perp},\bsigma_h^{\perp}) =\mathcal{F}(\bu_h^{\perp},\bsigma_h^{\perp}) 
\end{align*}


\section{Solving the functional}
We have to minimize the augmented functional:
\begin{align*}
\mathcal{F}_C(\bu,\bsigma)=||\text{div} \bsigma+\bff||^2+||\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu)||^2+\langle \bu \cdot \bn, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C} \geq 0  
\end{align*}
with respect to the finite element subspaces $V_h \subset  H_d^1(\Omega)^d $ and $\Sigma_h \subset H_{N,C}(\text{div},\Omega)$. So we search for a solution $(\bu_h,\bsigma_h) = (\bu^D,\bsigma^N)+(\hat{\bu}_h,\hat{\bsigma}_h) $ with $\hat{\bu}_h$ and $\hat{\bsigma}$ in $V_h$ and $\Sigma_h$, such that the inequality constraints are satisfied. \\
This is equivalent to the system of variational inequalities:
\begin{align}
\begin{cases}
-2 \left( \mathcal{A} \bsigma_h-\boldsymbol{\varepsilon}(\bu_h),\boldsymbol{\varepsilon}(\bv_h-\bu_h) \right)+
\langle \bn \cdot (\bsigma_h  \bn), \bn \cdot (\bv_h-\bu_h)\rangle_{\Gamma_C} \geq 0\\
2 \left( \text{div} \bsigma_h + \bff, \text{div} (\btau_h-\bsigma_h)\right)+
2\left( \mathcal{A} \bsigma_h - \boldsymbol{\varepsilon}(\bu_h), \mathcal{A}(\btau_h-\bsigma_h \right)+
\langle \bn \cdot \bu_h  -g, \bn \cdot (\btau_h-\bsigma_h)\rangle_{\Gamma_C} \geq 0\\
\end{cases}
\end{align}


\section{A posteriori error estimator}
\textbf{Theorem}\\
Let $(\bu_h,\bsigma_h) \in H^1(\Omega)^d \times H(\text{div},\Omega)^d$ be the exact solution of the constrainted problem. Then $\forall (\bu_h,\bsigma_h) \in (\bu_h^D,\bsigma_h^D)  + V_h \times \Sigma_h$ which satisfy the constraints, $\exists \: C_R>0$ ($\lambda$ indipendent) such that:
\begin{align*}
&\mathcal{F}_C(\bu,_h\bsigma_h) \geq C_R
\left(|| (\boldsymbol{\varepsilon}(\bu_h)-\boldsymbol{\varepsilon}(\bu))||^2+||\text{div} (\bsigma-\bsigma_h)||^2+||\mathcal{A}(\bsigma-\bsigma_h)||^2 \right)  
\end{align*}

\textbf{Korn's inequality}\\
Let $\Gamma_d \subset \Gamma$ be a set of positive measure $|\Gamma_d| > 0$. Then $|| \boldsymbol{\varepsilon}\left( \bv \right)||$ is an equivalent norm to $||v||_{1,\Omega} $ in $H_d^1(\Omega)^d$
\begin{align*}
&||v||_{1,\Omega} \leq C_l || \boldsymbol{\varepsilon}\left( \bv \right)|| \qquad \forall \bv \in H_d^1(\Omega)^d
\end{align*}
\textbf{Lemma 1}\\
Let $\Gamma_N \subset \Gamma$ be a set of positive measure $|\Gamma_N| > 0$. Then:
\begin{itemize}
\item \begin{align*}
|| \btau|| \leq C \left( ||\text{dev} (\btau)||+|| \text{div} (\btau) || \right) \qquad
 \forall \btau \in H_N(\text{div}, \Omega)^d
\end{align*}
\item \begin{align*}
|| \text{dev} \btau|| \leq 2 \mu  ||\mathcal{A} (\btau) ||  \qquad
|| \text{dev} \btau|| \leq  \sqrt{2 \mu}  \left( \mathcal{A} (\btau),\btau \right) 
\end{align*}
\item Trace inequalities: \begin{align*}
||v||_{1/2,\Gamma_C} \leq C_T \mu  ||v||_{1,\Omega} \quad \forall \: \bv \in H^1(\Omega) \qquad  \qquad
|| \bsigma \cdot \bn||_{-1/2,\Gamma_c} \leq  C_T || \bsigma||_{\text{div},\Omega} \quad \forall \: \bsigma  \in H_N(\text{div},\Omega)
\end{align*}
\end{itemize}
\begin{align*}
&\mathcal{F}(\bu,\bsigma)=||\text{div} \bsigma + \bff||^2 + || \mathcal{A} \bsigma-\boldsymbol{\varepsilon}(\bu)||^2\\
&\mathcal{F}_C(\bu,\bsigma)=||\text{div} \bsigma + \bff||^2 + || \mathcal{A} \bsigma-\boldsymbol{\varepsilon}(\bu)||^2+ \langle \bn \cdot \bu -g, \bn \cdot (\bsigma \cdot \bn)\rangle
\end{align*}


\section{The least squares functional as an a posteriori estimator: reliability and efficiency}
Let be $e$ the error. Then we want a functional that is both reliable and efficient. In this way, the functional is equivalent to the norm of the error.
\begin{align}
&C_1 ||\text{e}||  \overbrace{\leq}^{ \text{reliability} }  \mathcal{F} \overbrace{\leq}^{\text{efficiency}} C_2 ||\text{e}||  \qquad \to \qquad
 ||\text{e}||   \to 0 \qquad \iff \qquad \mathcal{F} \to 0
\end{align}
If both are taken singularly, we are no sure of the fact that the functional is a good estimator. Indeed it could happen:\\
Reliability: $||\text{e}|| \approx 0$, $\mathcal{F} \gg 1$ (refinement, although is not necessary $\to$ wasting time)\\
Efficiency:     $\mathcal{F} \approx 0$, $||\text{e}|| \gg 1$ (not refinement, but it is necessary $\to$ bad solving) 


\subsection{Reliability}
$(\bu_h,\bsigma_h) \in (\bu^D,\bsigma^N)+\bV_h \times \bSigma_h$.
\begin{align*}
&\mathcal{F}_C(\bu_h,\bsigma_h) \geq C_R \left( 
\overbrace{||\text{div}(\bsigma-\bsigma_h)||^2}^{a)}+
\overbrace{||\mathcal{A}(\bsigma-\bsigma_h)||^2}^{b)}+
\overbrace{|| \boldsymbol{\varepsilon}(\bu-\bu_h)||^2}^{c)}
 \right)
\end{align*}


\begin{itemize}
\item a)  \begin{align*}
 ||\text{div}(\bsigma-\bsigma_h)||^2\overset{\pm \bff}{=}||\text{div}(\bff+\bsigma_h)||^2 \leq \mathcal{F}_C(\bu_h,\bsigma_h)
\end{align*}
\item b) Remarking the fact that, for a scalar product, $(\text{sym}(a),b)+(a,\text{as}(b))=(a,b)$ and using Greens formula:
\begin{align*}
(\boldsymbol{\varepsilon}(\bu-\bu_h),\bsigma-\bsigma_h)=&
\langle (\bsigma-\bsigma_h)\cdot \bn , \bu -\bu_h\rangle_{\Gamma_C}-(\text{div}(\bsigma-\bsigma_h),\bu-\bu_h)-(\text{as}(\bsigma-\bsigma_h),\nabla(\bu-\bu_h))\\
\overset{\text{lemma1}}{\leq} &\langle \bsigma_h \bn \cdot  \bn , \bu_h \cdot \bn -g\rangle_{\Gamma_C}+\\
&||\text{div} (\bsigma- \bsigma_h)|| ||\bu- \bu_h||+||\text{as} (\bsigma- \bsigma_h)|| ||\nabla(\bu- \bu_h)||\\
{\leq} &\mathcal{F}_C(\bu_h,\bsigma_h)+ \sqrt{2} \left(
||\text{div} (\bsigma- \bsigma_h)||^{2}+||\text{as} (\bsigma- \bsigma_h)||^{2} \right)^{1/2}||\bu- \bu_h||_1
\end{align*}
where:
\begin{align*}
||\text{div} (\bsigma- \bsigma_h)||^{2}+||\text{as} (\bsigma- \bsigma_h)||^{2} \overset{\text{as}(\bsigma)=0}{=} &||\text{div} (\bsigma_h)+\bff||^{2}+||\text{as} ( \bsigma_h)||^{2}\\
\overset{\text{as}(\boldsymbol{\varepsilon}(\bu_h))=0}{=}& ||\text{div} (\bsigma_h)+\bff||^{2}+(2 \mu)^{2}||\text{as} (\mathcal{A} \bsigma_h-\boldsymbol{\varepsilon}(\bu_h))||^{2}\\
\overset{ ||\text{as}(\cdot)||\leq  ||\text{}(\cdot)||}{=}& ||\text{div} (\bsigma_h)+\bff||^{2}+(2 \mu)^{2}|| (\mathcal{A} \bsigma_h-\boldsymbol{\varepsilon}(\bu_h))||^{2}\\
\leq & \max(1, (2 \mu)^{2})\mathcal{F}_C(\bu_h,\bsigma_h)
\end{align*}
Now we take advantage of this inequality:
\begin{align*}
||\boldsymbol{\varepsilon} (\bu_h- \bu)|| \overset{\pm \mathcal{A} \bsigma}{=} &|| ( \mathcal{A} \bsigma -\boldsymbol{\varepsilon} (\bu) ) + (\boldsymbol{\varepsilon} (\bu_h) - \mathcal{A} \bsigma)||=\\
 \overset{ \mathcal{A} \bsigma -\boldsymbol{\varepsilon} (\bu) = 0 }{=} & ||\boldsymbol{\varepsilon} (\bu_h) - \mathcal{A} \bsigma||\\
   \overset{ \pm \mathcal{A} \bsigma_h} {\leq}   &||\mathcal{A}\bsigma- \mathcal{A} \bsigma_h||_1+||\boldsymbol{\varepsilon} (\bu_h) - \mathcal{A} \bsigma_h||\\
{\leq}     &||\mathcal{A}\bsigma- \mathcal{A} \bsigma_h||+ \mathcal{F}_C(\bu_h,\bsigma_h)^{1/2}
\end{align*}
and using the Korn's inequality:
\begin{align*}
(\boldsymbol{\varepsilon}(\bu-\bu_h),\bsigma-\bsigma_h) \leq & \mathcal{F}_C(\bu_h,\bsigma_h) +  \sqrt{2 \max(1, (2 \mu)^{2})}  \mathcal{F}_C(\bu_h,\bsigma_h)^{1/2} ||\boldsymbol{\varepsilon} (\bu- \bu_h)||\\
{\leq}  & C \mathcal{F}_C(\bu_h,\bsigma_h)^{1/2} ( \mathcal{F}_C(\bu_h,\bsigma_h)^{1/2}+||\mathcal{A}\bsigma- \mathcal{A} \bsigma_h||)\\
 &\overset{ ||\bsigma- \bsigma_h||_{\mathcal{A}} =(\mathcal{A}\bsigma,\bsigma)} {\leq}   C \mathcal{F}_C(\bu_h,\bsigma_h)^{1/2} ( \mathcal{F}_C(\bu_h,\bsigma_h)^{1/2}+||\bsigma- \bsigma_h||_{\mathcal{A}})
\end{align*}
where the last inequality holds with a particular C indipendent on $\lambda$:
\begin{align*}
(\mathcal{A} \bsigma, \bsigma)=& \int_{\Omega} \dfrac{1}{2 \mu}\left[  \sum_{i \neq j} \sigma_{ij}^{2}  + \sum_{i = j} (\sigma_{ij}-\text{tr}\sigma_{ij})\sigma_{ij} \right]+\dfrac{1}{d(d \lambda + 2 \mu)}\left[ \sum_{i = j} \sigma_{ij}\text{tr}  \sigma_{ij} \right]=\\
&\int_{\Omega} \dfrac{1}{2 \mu}\left[  \sum_{i, j} \sigma_{ij}^{2}  - \text{tr} ^{2} \sigma_{ij} \right]+\dfrac{1}{d(d \lambda + 2 \mu)} \text{tr} ^{2} \sigma_{ij}=\\
&\int_{\Omega} \dfrac{1}{2 \mu}  \left[\sum_{i, j} \sigma_{ij}^{2}  +\dfrac{ 2(d-1) \mu+ \lambda d^{2} }{2 \mu d(d \lambda + 2 \mu)} \text{tr}^{2} \sigma_{ij} \right]\\
(\mathcal{A} \bsigma, \mathcal{A} \bsigma)=& \int_{\Omega} \left(\dfrac{1}{2 \mu}\right)^{2} \left[  \sum_{i \neq j} \sigma_{ij}^{2}  + \sum_{i = j} (\sigma_{ij}-\text{tr}\sigma_{ij})^{2}\right]+\dfrac{1}{d(d \lambda + 2 \mu)^{2}}  \text{tr}^{2}\sigma_{ij} \\
& \int_{\Omega} \left(\dfrac{1}{2 \mu}\right)^{2} \left[  \sum_{i, j} \sigma_{ij}^{2}  + \dfrac{(2 \mu)^{2} + (d-2)d (d \lambda + 2 \mu)^{2}}{d(d \lambda + 2 \mu)^{2}}  \text{tr}^{2}\sigma_{ij} \right]\\
\end{align*}
where the costant $C$ can be defined as:
\begin{align*}
C=\dfrac{1}{2 \mu} \sup_{\lambda} \left[1, 
\left(\dfrac{2 \mu d(d \lambda + 2 \mu)} { 2(d-1) \mu+ \lambda d^{2} }\right) 
\left(\dfrac{(2 \mu)^{2} + (d-2)d (d \lambda + 2 \mu)^{2}}{d(d \lambda + 2 \mu)^{2}}  \right)\right]
\end{align*}
Therefore:
\begin{align*}
||\bsigma -\bsigma_h||_{\mathcal{A}}^{2} \overset{\pm \boldsymbol{\varepsilon}(\bu-\bu_h)}{=}&(\mathcal{A}(\bsigma-\bsigma_h)-\boldsymbol{\varepsilon}(\bu-\bu_h), \bsigma-\bsigma_h)+(\boldsymbol{\varepsilon}(\bu-\bu_h),\bsigma-\bsigma_h)\\
\leq &  \sqrt{2} (||\mathcal{A}(\bsigma-\bsigma_h)||^{2}  +  ||\boldsymbol{\varepsilon}(\bu-\bu_h)||^{2})^{1/2}     ||\bsigma -\bsigma_h||+\\ 
&C \mathcal{F}_C(\bu_h,\bsigma_h)^{1/2}(\mathcal{F}_C(\bu_h,\bsigma_h)^{1/2}+||\bsigma -\bsigma_h||_{\mathcal{A}})\\
\leq & \mathcal{F}_C(\bu_h,\bsigma_h)^{1/2}||\bsigma -\bsigma_h||+ C \mathcal{F}_C(\bu_h,\bsigma_h)^{1/2}(\mathcal{F}_C(\bu_h,\bsigma_h)^{1/2}+||\bsigma -\bsigma_h||_{\mathcal{A}})
\end{align*}
Now by using the lemma 2:
\begin{align*}
 ||\bsigma -\bsigma_h|| \leq &\tilde{C}_D ( (\mathcal{A}(\bsigma -\bsigma_h),(\bsigma -\bsigma_h)) + || \text{div} (\bsigma -\bsigma_h) ||)\\
  = & \tilde{C}_D (||\bsigma -\bsigma_h||_{\mathcal{A}} + || \text{div} (\bff +\bsigma_h) ||)\\
  \leq & \tilde{C}_D (||\bsigma -\bsigma_h||_{\mathcal{A}} + \mathcal{F}(\bu_h,\bsigma_h))^{1/2}
\end{align*}
So finally:
\begin{align*}
||\bsigma -\bsigma_h||_{\mathcal{A}}^{2} \leq C \mathcal{F}_C(\bu_h,\bsigma_h)^{1/2} \left( \mathcal{F}_C(\bu_h,\bsigma_h)^{1/2}+||\bsigma -\bsigma_h||_{\mathcal{A}}) \right)
\end{align*}
Then exists a constant $C_R$ such that:
\begin{align*}
C_R ||\bsigma -\bsigma_h||_{\mathcal{A}}^{2} \leq \mathcal{F}(\bu_h,\bsigma_h)
\end{align*}
Imposing $||\bsigma -\bsigma_h||_{\mathcal{A}}=x$ and ${\mathcal{F}}(\bu_h,\bsigma_h)= {C_R} x^{2} $, we obtain $x^{2} \leq C {C_R} x^{2} +C {C_R} x^{2} $ and find that $C_R \geq \dfrac{1}{2 C}$. Even more so, we can state that:
\begin{align*}
C_R ||{\mathcal{A}}(\bsigma -\bsigma_h)||^{2} \leq \mathcal{F}(\bu_h,\bsigma_h)
\end{align*}
\item c) Taking advantage of the previous point:
\begin{align*}
 ||\boldsymbol{\varepsilon}(\bu-\bu_h)||= &||\mathcal{A}(\bsigma-\bsigma_h)+\mathcal{A}(\bsigma_h)-\boldsymbol{\varepsilon}(\bu_h)||=\\
 \leq & ||\mathcal{A}(\bsigma-\bsigma_h)||+||\mathcal{A}(\bsigma_h)-\boldsymbol{\varepsilon}(\bu_h)||\\
  \leq & ||\mathcal{A}(\bsigma-\bsigma_h)||+\mathcal{F}(\bu_h,\bsigma_h)^{1/2}\\
  \leq & \left( \dfrac{1}{C_R}+1 \right) \mathcal{F}(\bu_h,\bsigma_h)^{1/2}
\end{align*}
\end{itemize}





\section{Efficiency}
The element-wise version of the augmented functional:
\begin{align*}
\mathcal{F}_{C,T}(\bu,\bsigma)&=\mathcal{F}_{T}(\bu,\bsigma)+\langle \bu \cdot \bn, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C \cap \delta T}\\
&=||\text{div} \bsigma+\bff||_T^2+||\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu)||_T^2+\langle \bu \cdot \bn, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C \cap \delta T}
\end{align*}
We can minimize $\mathcal{F}_T$ as follows:
\begin{align*}
\mathcal{F}_{T}(\bu_h,\bsigma_h) 
&\overset{\mathcal{A} \bsigma - \boldsymbol{\varepsilon}(\bu)=0 }{=} 
||\text{div} \bsigma_h+\bff \pm \bsigma||_T^2+||(\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu) )+(\boldsymbol{\varepsilon}(\bu_h)-\mathcal{A}\bsigma_h)||_T^2\\
& \overset{\text{div} \bsigma + \bff=0} {\leq}||\text{div} (\bsigma- \bsigma_h)||_T^2+ 2||\mathcal{A}(\bsigma-\bsigma_h) ||_T^2 +2||(\boldsymbol{\varepsilon}(\bu-\bu_h)||_T^2\\
& \quad \:\:\:{\leq} 2\:\:\left(||\text{div}( \bsigma- \bsigma_h)||_T^2+ ||\mathcal{A}(\bsigma-\bsigma_h) ||_T^2 +||(\boldsymbol{\varepsilon}(\bu-\bu_h)||_T^2\right)
\end{align*}
Regarding the boundary term, we take advantage of the following fact:
\begin{align*}
\begin{cases}
\Gamma_{C,a},\: \:\bu \cdot \bn - g=0: &\langle \bu_h \cdot \bn-g, (\bsigma_h \bn) \cdot \bn \rangle_{\Gamma_{C,a}} =\langle  (\bu_h-\bu)\cdot \bn, ((\bsigma_h-\bsigma) \bn) \cdot \bn \rangle_{\Gamma_{C,a}} \\
\Gamma_C \backslash \Gamma_{C,a} ,\:      (\bsigma \bn) \cdot \bn =0: & \langle \bu_h \cdot \bn-g, (\bsigma_h \bn) \cdot \bn \rangle_{\Gamma_C \backslash \Gamma_{C,a}} =\langle \bu_h \cdot \bn-g, ((\bsigma_h -\bsigma_h)\bn) \cdot \bn \rangle_{\Gamma_C \backslash \Gamma_{C,a}}
\end{cases}
\end{align*}
in order to show that:
\begin{align*}
\langle \bu_h \cdot \bn-g, (\bsigma_h \bn) \cdot \bn \rangle_{\Gamma_C }
=&\langle \bu_h \cdot \bn-g, (\bsigma_h \bn) \cdot \bn \rangle_{\Gamma_C \backslash \Gamma_{C,a} }+\langle \bu_h \cdot \bn-g, (\bsigma_h \bn) \cdot \bn \rangle_{\Gamma_{C,a}}\\
=&\langle \bu_h \cdot \bn-g \pm \bu \cdot \bn, ((\bsigma_h -\bsigma)\bn) \cdot \bn \rangle_{\Gamma_C \backslash \Gamma_{C,a}}+\langle  (\bu_h-\bu)\cdot \bn, ((\bsigma_h \pm \bsigma) \bn) \cdot \bn \rangle_{\Gamma_{C,a}} \\
=&\langle \bu\cdot \bn-g, ((\bsigma_h -\bsigma)\bn) \cdot \bn \rangle_{\Gamma_C \backslash \Gamma_{C,a}}+\langle  (\bu_h-\bu)\cdot \bn, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_{C,a}}+\\
&\langle  (\bu_h-\bu)\cdot \bn, ((\bsigma_h-\bsigma) \bn) \cdot \bn \rangle_{\Gamma_{C}} \\
=&\langle \bu\cdot \bn-g, ((\bsigma_h -\bsigma_h)\bn) \cdot \bn \rangle_{\Gamma_C }+\langle  (\bu_h-\bu)\cdot \bn, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_{C}}+\\
&\langle  (\bu_h-\bu)\cdot \bn, ((\bsigma_h-\bsigma) \bn) \cdot \bn \rangle_{\Gamma_{C}} \\
\end{align*}
Using Cauchy-Schwarz and trace inequalities:
\begin{align*}
\langle \bu_h \cdot \bn-g, (\bsigma_h \bn) \cdot \bn \rangle_{\Gamma_C }
=&\langle \bu\cdot \bn-g, ((\bsigma_h -\bsigma_h)\bn) \cdot \bn \rangle_{\Gamma_C }+\langle  (\bu_h-\bu)\cdot \bn, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_{C}}+\\
&\langle  (\bu_h-\bu)\cdot \bn, ((\bsigma_h-\bsigma) \bn) \cdot \bn \rangle_{\Gamma_{C}} \\
\leq &||\bu \cdot \bn-g||_{1/2,\Gamma_C}||(\bsigma_h-\bsigma) \bn||_{-1/2,\Gamma_C}+
||\bu_h-\bu||_{1/2,\Gamma_C}||\bsigma \bn||_{-1/2,\Gamma_C}+\\
&||\bu_h-\bu||_{1/2,\Gamma_C}||(\bsigma_h-\bsigma) \bn||_{-1/2,\Gamma_C}\\
\overset{}{\leq} &C_T||\bu \cdot \bn-g||_{1/2,\Gamma_C}||(\bsigma_h-\bsigma) \bn||_{1,\Omega}+
C_T||\bu_h-\bu||_{1,\Omega}||\bsigma \bn||_{-1/2,\Gamma_C}+\\
& C_T^2 ||\bu_h-\bu||_{1,\Omega}||(\bsigma_h-\bsigma) \bn||_{1,\Omega}
\end{align*}
We cannot rule out that the contact boundary term converges at a slower rate than the error itself.
So we could avoid the presence of this term. Let the least squares functional be:
\begin{align*}
\mathcal{F}_C(\bu,\bsigma)=||\text{div} \bsigma+\bff||^2+||\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu)||^2 \geq 0
\end{align*}
and the augmented least squares functional (positive thanks to the constraints) be:
\begin{align*}
\mathcal{F}(\bu,\bsigma)=||\text{div} \bsigma+\bff||^2+||\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu)||^2+\langle \bu \cdot \bn, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C} \geq 0  
\end{align*}
Searching for an approximate solution $(\bu_h^{\perp},\bsigma_h^{\perp})$ such that exactly satisfy:
\begin{align*}
\langle \bu \cdot \bn, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C}=0
\end{align*}
means that (being the set of $(\bu_h^{\perp},\bsigma_h^{\perp})$ smaller than the one in which we search for $(\bu_h^{},\bsigma_h^{})$):
\begin{align*}
\mathcal{F}(\bu_h,\bsigma_h) \leq \mathcal{F}_C(\bu_h,\bsigma_h) \leq \mathcal{F}_C(\bu_h^{\perp},\bsigma_h^{\perp}) =\mathcal{F}(\bu_h^{\perp},\bsigma_h^{\perp}) 
\end{align*}
Namely, this solution $(\bu_h^{\perp},\bsigma_h^{\perp})$ is worse than $(\bu_h,\bsigma_h)$, but converges more quickly. However in practical cases, the boundary contribution is not so relevant with respect to the whole domain one, so we can search for $(\bu_h,\bsigma_h)$.

\textbf{Lemma 1}\\
Let $(\bu,\bsigma) \in H^1(\Omega)^2 \times H(\text{div},\Omega)^d$ be the exact solution of the contact problem. Moreover, let $(\bu_h,\bsigma_h) \in (u^D+\bU_h) \times \Sigma_h$ be such that the constraints are satisfied. Then:
\begin{align*}
\langle \bu-\bu_h,(\bsigma-\bsigma_h) \bn \rangle_{\Gamma_c}=\langle (\bu-\bu_h) \cdot \bn,(\bsigma-\bsigma_h)\bn \cdot \bn \rangle_{\Gamma_c} \leq 
\langle \bn \cdot \bu_h-g, \bsigma_h \bn \cdot \bn\rangle_{\Gamma_c}
\end{align*}

\textbf{Proof}\\
\begin{align*}
\begin{cases}
\Gamma = \Gamma_{c,d} \cup \Gamma_{c,s}\\
\bn \cdot \bu - g \leq 0 &\Gamma_c\\
(\bsigma \bn) \cdot \bn \leq 0 &\Gamma_c\\
\bu \cdot \bn - g=0 & \Gamma_{c,d}\\
(\bsigma \bn) \cdot \bn =0 & \Gamma_{c,s} 
\end{cases}
\quad \to \quad
\begin{cases}
\langle \bn \cdot \bu_h-g, \bsigma \bn \cdot \bn\rangle_{\Gamma_c,d} \geq 0\\
\langle \bn \cdot \bu-g, \bsigma_h \bn \cdot \bn\rangle_{\Gamma_c,s} \geq 0 
\end{cases}
\end{align*}
Therefore:
\begin{align*}
\langle \bn \cdot \bu_h-g,& (\bsigma_h \bn) \cdot \bn\rangle_{\Gamma_c} \geq\\
&\langle \bn \cdot \bu_h-g, ((\bsigma_h-\bsigma) \bn) \cdot \bn\rangle_{\Gamma_{c,d}}+\langle \bn \cdot (\bu_h-\bu), (\bsigma_h \bn) \cdot \bn\rangle_{\Gamma_{c,s}} = \\
&\langle \bn \cdot (\bu_h-\bu), ((\bsigma_h-\bsigma) \bn) \cdot \bn\rangle_{\Gamma_{c,d}}+
\langle \bn \cdot (\bu_h-\bu), (\bsigma_h-\bsigma \bn) \cdot \bn\rangle_{\Gamma_{c,s}} = \\
&\langle \bn \cdot (\bu_h-\bu), ((\bsigma_h-\bsigma) \bn) \cdot \bn\rangle_{\Gamma_c}
\end{align*}
\textbf{Lemma 2}\\
Let be $\Gamma_N$ be a set of positive measure, $|\Gamma_N|>0$. Then:
\begin{align*}
||\btau|| \leq C_D (||\text{dev}( \btau) ||+ || \text{div} (\btau) ||) \qquad \forall \btau \in H_N(\text{div},\Omega)^d
\end{align*}
Since the following inequalities hold:
\begin{align*}
 ||\text{dev}( \btau) ||\leq 2 \mu  || \mathcal{A}\btau || \qquad   ||\text{dev}( \btau) ||\leq \sqrt{2 \mu}  (\mathcal{A}\btau,\btau)  \qquad \forall \btau \in H_N(\text{div},\Omega)^d
\end{align*}
We can also write:
\begin{align*}
||\btau|| \leq \hat{C}_D (|| \mathcal{A}\btau ||+ || \text{div} (\btau) ||) \qquad  ||\btau|| \leq \tilde{C}_D ( (\mathcal{A}\btau,\btau) + || \text{div} (\btau) ||)  \qquad \forall \btau \in H_N(\text{div},\Omega)^d
\end{align*}




\section{The augmented functional is convex}
Let us consider
\begin{align*}
\mathcal{F}(\bu,\bsigma)=&\mathcal{F}_1(\bu,\bsigma)+\mathcal{F}_2(\bu,\bsigma)+\mathcal{F}_3(\bu,\bsigma)=\\
=&||\text{div} \bsigma+\bff||^2+||\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu)||^2+\langle \bu \cdot \bn-g, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C} 
\end{align*}
whose derivative is the combination of:
\begin{align*}
\mathcal{F}_1(\bu,\bsigma)' \left[ \bu_h, \bsigma_h \right]&= 2 \left( \text{div} \bsigma + \bff,  \text{div} \bsigma_h \right)\\
\mathcal{F}_2(\bu,\bsigma)' \left[ \bu_h, \bsigma_h \right]&=2 \left( \mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu),\mathcal{A} \bsigma_h - \boldsymbol{\varepsilon} (\bu_h) \right)\\
\mathcal{F}_3(\bu,\bsigma)' \left[ \bu_h, \bsigma_h \right]&=\langle \bu_h \cdot \bn-g, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C} +\langle \bu \cdot \bn-g, (\bsigma_h \bn) \cdot \bn \rangle_{\Gamma_C}
\end{align*}
so that:
\begin{align*}
\mathcal{F}(\bu,\bsigma)' \left[ \bu_h, \bsigma_h \right]=& 2 \left( \text{div} \bsigma + \bff,  \text{div} \bsigma_h \right)+
2 \left( \mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu),\mathcal{A} \bsigma_h - \boldsymbol{\varepsilon} (\bu_h) \right)+\\
&\langle \bu_h \cdot \bn-g, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C} +\langle \bu \cdot \bn-g, (\bsigma_h \bn) \cdot \bn \rangle_{\Gamma_C}
\end{align*}
We can show that the functional $\mathcal{F}$ is convex by means of the equivalent statement:
\begin{align*}
\mathcal{F}(\bu +\bu_h,\bsigma+\bsigma_h) -\mathcal{F}(\bu_h,\bsigma) \geq \mathcal{F}(\bu,\bsigma)' \left[ \bu_h, \bsigma_h \right]
\end{align*}
In particular we have:
\begin{align*}
\mathcal{F}_1(\bu,\bsigma)' \left[ \bu_h, \bsigma_h \right] \leq & 
2 \left( \text{div} \bsigma + \bff,  \text{div} \bsigma_h \right)+\left( \text{div} \bsigma_h \,  \text{div} \bsigma_h \right)\\
\mathcal{F}_2(\bu,\bsigma)' \left[ \bu_h, \bsigma_h \right] \leq & 
2 \left( \mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu),\mathcal{A} \bsigma_h - \boldsymbol{\varepsilon} (\bu_h) \right)+
\left( \mathcal{A} \bsigma_h - \boldsymbol{\varepsilon} (\bu_h),\mathcal{A} \bsigma_h - \boldsymbol{\varepsilon} (\bu_h) \right) \\
\mathcal{F}_3(\bu,\bsigma)' \left[ \bu_h, \bsigma_h \right] \leq 
& \langle \bu_h \cdot \bn-g, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C} +\langle \bu \cdot \bn-g, (\bsigma_h \bn) \cdot \bn \rangle_{\Gamma_C}+\langle \bu_h \cdot \bn-g, (\bsigma_h \bn) \cdot \bn \rangle_{\Gamma_C}
\end{align*}

where the last inequality holds thanks to the constraints. The gradient of the functional is:
\begin{align*}
\dfrac{\partial  \mathcal{F}(\bu,\bsigma)}{\partial \bu } \left[ \bu_h \right]=& 
-2 \left( \mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu), \boldsymbol{\varepsilon} (\bu_h ) \right)+\langle \bn \cdot \bu_h,  (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C}\\
\dfrac{\partial  \mathcal{F}(\bu,\bsigma)}{\partial \bsigma } \left[ \bsigma_h \right]=& 
2 \left( \text{div} \bsigma + \bff,  \text{div} \bsigma_h \right)
+2 \left( \mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu), \mathcal{A} \bsigma_h \right)
+\langle \bu \cdot \bn-g, ( \bsigma_h \: \bn) \cdot \bn \rangle_{\Gamma_C}
\end{align*}
In order to obtain the normal equations, we need some adjoints operators (DUBBIO: SU GAMMAC HO OPERATORI DI TRACCIA):
\begin{align*}
\left( \mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu), \boldsymbol{\varepsilon} (\bu_h ) \right)&=\int_{\Omega} (\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)) : \boldsymbol{\varepsilon} (\bu_h )=
\int_{\Omega} (\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)) : \frac{1}{2} \left( \nabla \bu_h+ \nabla^T \bu_h \right) \\
&=\int_{\Omega} (\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)) : \frac{1}{2}  \nabla \bu_h + \int_{\Omega} (\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu))^T : \frac{1}{2}  \nabla \bu_h \\
&=\int_{\Omega} (\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)) :  \nabla \bu_h \\
&=\int_{ \Gamma_d} (\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu))  \bn \cdot \bu_h -\int_{\Omega} \text{div} \left(\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu) \right)\cdot \bu_h \ \\
\\ 
\langle \bn \bu_h,  (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C}&=\int_{\Gamma_C}   (\bu_h \cdot \bn )  \left[(\bsigma \bn) \cdot \bn \right] = \int_{\Gamma_C}  (\bu_h \cdot \bn ) \bn  \cdot (\bsigma \bn) \\
\left( \text{div} \bsigma + \bff,  \text{div} \bsigma_h \right)&= \int_{\Omega} ( \text{div} \bsigma + \bff) \text{div} \bsigma_h =\int_{\Gamma_N}  ( \text{div} \bsigma + \bff) \cdot  \bsigma_h \bn -\int_{\Omega} \nabla( \text{div} \bsigma + \bff) : \bsigma_h \\
 \left( \mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu), \mathcal{A} \bsigma_h \right)&=
 \int_{\Omega}\left(\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)\right): \mathcal{A} \bsigma_h\\
 &=\int_{\Omega}\left(\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)\right) : \left(\dfrac{1}{2 \mu} \bsigma_h- \dfrac{\lambda}{d \lambda + 2 \mu} \text{tr} \bsigma_h \bI \right)\\
  &=\int_{\Omega}\dfrac{1}{2 \mu} \left(\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)\right)  : \bsigma_h- \dfrac{\lambda}{d \lambda + 2 \mu} \text{tr}\left( \mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)\right)   \bI :\bsigma_h\\
   &=\int_{\Omega} \left[ \dfrac{1}{2 \mu} \left(\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)\right)  - \dfrac{\lambda}{d \lambda + 2 \mu} \text{tr}\left( \mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)\right)   \bI \right] :\bsigma_h\\
      &=\int_{\Omega} \left[ \mathcal{A}\left(\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)\right) \right] :\bsigma_h\\
 \\
\langle \bn \cdot \bu-g,  (\bsigma_h \bn) \cdot \bn \rangle_{\Gamma_C}&=\int_{\Gamma_C}   (\bu \cdot \bn -g )  \left[(\bsigma_h \bn) \cdot \bn \right] = \int_{\Gamma_C}  (\bu \cdot \bn -g ) \bn  \cdot (\bsigma_h \bn) \\
\end{align*}
So that:
\begin{align*}
\dfrac{\partial  \mathcal{F}(\bu,\bsigma)}{\partial \bu } \left[ \bu_h \right]=& 
-2 \int_{ \Gamma_d} (\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu))  \bn \cdot \bu_h -2 \int_{\Omega} \text{div} \left(\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu) \right)\cdot \bu_h \\
&+\int_{\Gamma_C}  (\bu_h \cdot \bn ) \bn  \cdot (\bsigma \bn) 
\\
\dfrac{\partial  \mathcal{F}(\bu,\bsigma)}{\partial \bsigma } \left[ \bsigma_h \right]=& 
+2\int_{\Gamma_N}  ( \text{div} \bsigma + \bff) \cdot  \bsigma_h \bn -2 \int_{\Omega} \nabla( \text{div} \bsigma + \bff) : \bsigma_h\\
&+2\int_{\Omega} \left[ \mathcal{A}\left(\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)\right) \right] :\bsigma_h\\
&+\int_{\Gamma_C}  (\bu \cdot \bn -g ) \bn  \cdot (\bsigma_h \bn) \
\end{align*}
The operator equation associated to the residual energy functional is:
\begin{align*}
\text{div} \left(\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu) \right)= \textbf{0}\\
- \nabla( \text{div} \bsigma + \bff) +\left[ \mathcal{A}\left(\mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu)\right) \right] = \textbf{0}\\
\end{align*}
DOMANDA: ottengo un problema del tipo Ax=b. La matrice dovrebbe essere non singolare, quindi dovrebbe esistere una unica soluzione. Ora, i vincoli del problema di contatto, dove rientrano in tutto questo?\\\\
A ME NON SEMBRA PUNTO SELLA. Tuttavia se moltiplico la prima per divTAU, la seconda per v, potrei ritrovare una sorta di punto sella ponendo Asigma-eps=C .MMM \\\\
In the first equation, the term on $\Gamma_d$ is zero due to the test function.\\
L'INTEGRALE SU GAMMAC non capisco perche si annulli.\\
Being $\Gamma_C \subset \Gamma_N$ and $\bsigma \bn =$ on $\Gamma_N$ the boundary integrals of the second equation vanish.
\section{Discretization}
\begin{align*}
\dfrac{\partial  \mathcal{F}(\bu,\bsigma)}{\partial \bu } \left[ \bu_h \right]=& 
-2 \left( \mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu), \boldsymbol{\varepsilon} (\bu_h ) \right)+\langle \bn \cdot \bu_h,  (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C} =0 \\
\dfrac{\partial  \mathcal{F}(\bu,\bsigma)}{\partial \bsigma } \left[ \bsigma_h \right]=& 
2 \left( \text{div} \bsigma + \bff,  \text{div} \bsigma_h \right)
+2 \left( \mathcal{A} \bsigma - \boldsymbol{\varepsilon} (\bu), \mathcal{A} \bsigma_h \right)
+\langle \bu \cdot \bn-g, ( \bsigma_h \: \bn) \cdot \bn \rangle_{\Gamma_C} =0
\end{align*}
Let be:
\begin{align*}
\bsigma = \sum_{j=1}^{N_{\sigma}} \sigma_i \bphi_i \qquad \bu= \sum_{n=1}^{N_{\bu}} u_j \bpsi_j \qquad \bsigma_h = \bphi_i \qquad \bu_h= \bpsi_m
\end{align*}
Then:
\begin{align*}
& B_{1,\:ij}= -\int_{\Omega}  \mathcal{A} \bphi_i : \beps (\bpsi_j) & B_1 \in \mathcal{R}^{N_u \times N_{\sigma}}\\ 
& B_{2,\:ij}= \int_{\Omega}  \beps (\bphi_i) : \beps (\bpsi_j) & B_2 \in \mathcal{R}^{N_u \times N_{u}}\\ 
& B_{3,\:ij}= \frac{1}{2} \int_{\Gamma_c}   (\bphi_i \bn) \cdot \bn  (\bn \cdot \bpsi_j)  &B_3 \in \mathcal{R}^{N_u \times N_{\sigma}}\\ 
& B_{4,\:ij}=  \int_{\Omega}  \text{div} \bphi_i \cdot \text{div} \bphi_j  &B_4 \in \mathcal{R}^{N_{\sigma} \times N_{\sigma}}\\ 
& B_{5,\:ij}=  \int_{\Omega}  \mathcal{A} \bphi_i : \mathcal{A} \bphi_j  &B_5 \in \mathcal{R}^{N_{\sigma} \times N_{\sigma}}\\ 
&F_{1,\:ij}=\frac{1}{2} \int_{\Gamma_c}g \:(\bphi_j \bn)\cdot \bn& F_1 \in \mathcal{R}^{N_{\sigma} }\\ 
&F_{2,\:ij}=- \int_{\Omega} \bff \cdot \text{div} \bphi_j & F_2 \in \mathcal{R}^{N_{\sigma} }\\ 
\end{align*}
and (+ constraints??????):
\begin{align*}
\begin{bmatrix}
B_1^T+B_3^T &B_4 + B_5\\
B_2  & B_1+B_3
\end{bmatrix}
\begin{bmatrix}
\bu\\
\bsigma
\end{bmatrix}
=
\begin{bmatrix}
F_1+F2\\
0
\end{bmatrix}
\end{align*}
Substituting:

\begin{align*}
\begin{cases}
\bu= - B_2^{-1}(B_1+B_3) \bsigma\\
-(B_1+B_3)^T B_2^{-1}(B_1+B_3) \bsigma + (B_4+B_5) \bsigma = F_1+F_2\\
C= B_1+B_3\\
D=B_4+B_5
\end{cases}
\quad \to \quad
\begin{cases}
\bsigma= \left( -C^T B_2^{-1} C + D \right)^{-1} (F_1+F_2)\\
\bu= - B_2^{-1}(B_1+B_3) \bsigma
\end{cases}
\end{align*}

\section{Raviart Thomas}
Computation of the divergence in 2D:
\begin{align*}
\text{div}_{\bbx} \bphi(\bbx)&= \text{div}_{\bbx} \left[ \dfrac{1}{\text{detJ}} \bJ\hat{\bphi}(F^{-1}(\bbx)) \right]\\
&=\dfrac{1}{\text{detJ}}  \left[ \dfrac{d}{dx}\left( J_{11} \hat{\phi}_1(\hat{\bbx}) +J_{12} \hat{\phi}_2(\hat{\bbx}) \right)
+\dfrac{d}{dy}
\left(  J_{21} \hat{\phi}_1(\hat{\bbx}) +J_{22} \hat{\phi}_2(\hat{\bbx})\right)
\right]\\
&=\dfrac{1}{\text{detJ}}  \bigg [
 \dfrac{\partial}{\partial \xi}\left( J_{11} \hat{\phi}_1(\hat{\bbx}) +J_{12} \hat{\phi}_2(\hat{\bbx}) \right) \dfrac{d \xi}{dx}+
\dfrac{\partial}{\partial \eta}\left( J_{11} \hat{\phi}_1(\hat{\bbx}) +J_{12} \hat{\phi}_2(\hat{\bbx}) \right) \dfrac{d \eta}{dx} \\
&+\dfrac{\partial}{\partial \xi}
\left(  J_{21} \hat{\phi}_1(\hat{\bbx}) +J_{22} \hat{\phi}_2(\hat{\bbx})\right) \dfrac{d \xi }{dy}
+\dfrac{\partial}{\partial \eta}
\left(  J_{21} \hat{\phi}_1(\hat{\bbx}) +J_{22} \hat{\phi}_2(\hat{\bbx})\right) \dfrac{d \eta }{dy}
\bigg ]
\end{align*}
The shape functions are:
\begin{align*}
\hat{\bphi}_a(\xi,\eta) =\begin{bmatrix}
\xi \\
\eta -1
\end{bmatrix}
\qquad
\hat{\bphi}_b(\xi,\eta) =\begin{bmatrix}
\xi \\
\eta 
\end{bmatrix}
\qquad
\hat{\bphi}_c(\xi,\eta) =\begin{bmatrix}
1-\xi \\
-\eta
\end{bmatrix}
\end{align*}
Using:
\begin{align*}
&\dfrac{\partial \phi_{a,1} }{\partial \xi}= 1 \qquad 
\dfrac{\partial \phi_{a,1} }{\partial \eta}= 0 \qquad 
\dfrac{\partial \phi_{a,2} }{\partial \xi}= 0 \qquad 
\dfrac{\partial \phi_{a,2} }{\partial \eta}= 1 \qquad \\
&\dfrac{\partial \phi_{b,1} }{\partial \xi}= 1 \qquad 
\dfrac{\partial \phi_{b,1} }{\partial \eta}= 0 \qquad 
\dfrac{\partial \phi_{b,2} }{\partial \xi}= 0 \qquad 
\dfrac{\partial \phi_{b,2} }{\partial \eta}= 1 \qquad \\
&\dfrac{\partial \phi_{c,1} }{\partial \xi}= - 1 \qquad 
\dfrac{\partial \phi_{c,1} }{\partial \eta}= 0 \qquad 
\dfrac{\partial \phi_{c,2} }{\partial \xi}= 0 \qquad 
\dfrac{\partial \phi_{c,2} }{\partial \eta}= -1 \qquad \\
\end{align*}
We obtain:
\begin{align*}
\text{div}_{\bbx} \bphi_a(\bbx)
&=\dfrac{1}{\text{detJ}}  \bigg [
 +J_{11}  \dfrac{d \xi}{dx} +J_{12}\dfrac{d \eta}{dx} +
  J_{21}\dfrac{d \xi }{dy}
+ J_{22}  \dfrac{d \eta }{dy}
\bigg ]\\
\text{div}_{\bbx} \bphi_b(\bbx)
&=\dfrac{1}{\text{detJ}}  \bigg [
 +J_{11}  \dfrac{d \xi}{dx} +J_{12}\dfrac{d \eta}{dx} +
  J_{21}\dfrac{d \xi }{dy}
+ J_{22}  \dfrac{d \eta }{dy}
\bigg ]\\
\text{div}_{\bbx} \bphi_c(\bbx)
&=\dfrac{1}{\text{detJ}}  \bigg [
 -J_{11}  \dfrac{d \xi}{dx} -
 J_{12}\dfrac{d \eta}{dx} -
  J_{21}\dfrac{d \xi }{dy}
-J_{22}  \dfrac{d \eta }{dy}
\bigg ]\\
\end{align*}
We know that:
\begin{align*}
J^{-1}=\begin{bmatrix}
\dfrac{\partial \xi}{\partial x} &\dfrac{\partial \xi}{\partial y} \\
\dfrac{\partial \eta}{\partial x} &\dfrac{\partial \eta}{\partial y} \\
\end{bmatrix}=\dfrac{1}{\text{det J}}
\begin{bmatrix}
J_{22} & -J_{12}\\
-J_{21} & J_{11}
\end{bmatrix}
\end{align*}
So that:
\begin{align*}
\text{div}_{\bbx} \bphi_a(\bbx)
&=\dfrac{1}{\text{detJ}^2}  
\bigg [
 +J_{11} J_{22} -J_{12}J_{21} -
  J_{21}J_{12}
+ J_{22}  J_{11}
\bigg ]\\
&=\dfrac{1}{\text{detJ}^2}  
\bigg [
 +2 J_{11} J_{22} -2 J_{12}J_{21} 
\bigg ]\\
&=\dfrac{1}{\text{detJ}^2}  (2 \text{detJ})\\
&=2 \dfrac{1}{\text{detJ}}  \\
\text{div}_{\bbx} \bphi_b(\bbx)
&=\dfrac{1}{\text{detJ}^2}  \bigg [
 +J_{11}  J_{22} -J_{12}J_{21} -
  J_{21}J_{12}
+ J_{22}  J_{11}
\bigg ]\\
&=\dfrac{1}{\text{detJ}^2}  \bigg [
 + 2J_{11}  J_{22} -2 J_{12}J_{21}
\bigg ]\\
&=\dfrac{1}{\text{detJ}^2}  (2 \text{detJ})\\
&=2 \dfrac{1}{\text{detJ}}  \\
\text{div}_{\bbx} \bphi_c(\bbx)
&=\dfrac{1}{\text{detJ}^2}  \bigg [
 -J_{11}  J_{22} -J_{12}J_{21} +
  J_{21}J_{12}
+ J_{22}  J_{11}
\bigg ]\\
&=\dfrac{1}{\text{detJ}^2}  \bigg [
 - 2J_{11}  J_{22}+2 J_{12}J_{21}
\bigg ]\\
&=\dfrac{1}{\text{detJ}^2}  (-2 \text{detJ})\\
&=-2 \dfrac{1}{\text{detJ}}  
\end{align*}








Computation of the divergence in 3D:
\begin{align*}
\text{div}_{\bbx} \bphi(\bbx)=& \text{div}_{\bbx} \left[ \dfrac{1}{\text{detJ}} \bJ\hat{\bphi}(F^{-1}(\bbx)) \right]\\
=\dfrac{1}{\text{detJ}}  &
\bigg [
 \dfrac{d}{dx}
 \underbrace{\left( J_{11} \hat{\phi}_1(\hat{\bbx}) +J_{12} \hat{\phi}_2(\hat{\bbx})  + J_{13} \hat{\phi}_3(\hat{\bbx})\right)}_{F_1}\\
&+\dfrac{d}{dy}
 \underbrace{\left(  J_{21} \hat{\phi}_1(\hat{\bbx}) +J_{22} \hat{\phi}_2(\hat{\bbx})
+ J_{23} \hat{\phi}_3(\hat{\bbx}) \right)}_{F_2}\\
&+\dfrac{d}{dz}
 \underbrace{\left(  J_{31} \hat{\phi}_1(\hat{\bbx}) +J_{32} \hat{\phi}_2(\hat{\bbx})
+ J_{33} \hat{\phi}_3(\hat{\bbx}) \right)}_{F_3}
\bigg ]\\
&=\dfrac{1}{\text{detJ}}  \bigg [
 \dfrac{\partial F_1}{\partial \xi} \dfrac{d \xi}{dx}+
\dfrac{\partial F_1}{\partial \eta} \dfrac{d \eta}{dx}+\dfrac{\partial F_1}{\partial \zeta} \dfrac{d \zeta}{dz}+ \\
&+\dfrac{\partial F_2}{\partial \xi} \dfrac{d \xi }{dy}
+\dfrac{\partial F_2}{\partial \eta} \dfrac{d \eta }{dy}
+\dfrac{\partial F_2}{\partial \zeta} \dfrac{d \zeta}{dz}\\
&+\dfrac{\partial F_3}{\partial \xi} \dfrac{d \xi }{dy}
+\dfrac{\partial F_3}{\partial \eta} \dfrac{d \eta }{dy}
+\dfrac{\partial F_3 }{\partial \zeta} \dfrac{d \zeta}{dz}
\bigg ]
\end{align*}
The shape functions are:
\begin{align*}
\hat{\bphi}_a(\xi,\eta,\zeta) =\begin{bmatrix}
-\xi \\
-\eta\\
1-\zeta
\end{bmatrix}
\qquad
\hat{\bphi}_b(\xi,\eta,\zeta) =\begin{bmatrix}
\xi \\
\eta -1\\
\zeta
\end{bmatrix}
\qquad
\hat{\bphi}_c(\xi,\eta,\zeta) =\begin{bmatrix}
1-\xi \\
-\eta\\
-\zeta
\end{bmatrix}
\qquad
\hat{\bphi}_d(\xi,\eta,\zeta) =\begin{bmatrix}
\xi \\
\eta\\
\zeta
\end{bmatrix}
\end{align*}
So we have:
\begin{itemize}
\item for $\hat{\bphi}_a(\xi,\eta,\zeta)$
\begin{align*}
&\dfrac{\partial F_1 }{\partial \xi}=-J_{11}  \qquad 
\dfrac{\partial F_1 }{\partial \eta}= -J_{12} \qquad 
\dfrac{\partial F_1 }{\partial \zeta}= -J_{13} \\ 
&\dfrac{\partial F_2 }{\partial \xi}= -J_{21} \qquad 
\dfrac{\partial F_2 }{\partial \eta}= -J_{22}  \qquad 
\dfrac{\partial F_2 }{\partial \zeta}= -J_{23}  \\ 
&\dfrac{\partial F_3 }{\partial \xi}= -J_{31}  \qquad 
\dfrac{\partial F_3 }{\partial \eta}= -J_{32}  \qquad 
\dfrac{\partial F_3 }{\partial \zeta}= -J_{33}  \\ 
\end{align*}
\item for $\hat{\bphi}_b(\xi,\eta,\zeta)$
\begin{align*}
&\dfrac{\partial F_1 }{\partial \xi}=J_{11}  \qquad 
\dfrac{\partial F_1 }{\partial \eta}= J_{12} \qquad 
\dfrac{\partial F_1 }{\partial \zeta}= J_{13} \\ 
&\dfrac{\partial F_2 }{\partial \xi}= J_{21} \qquad 
\dfrac{\partial F_2 }{\partial \eta}= J_{22}  \qquad 
\dfrac{\partial F_2 }{\partial \zeta}= J_{23}  \\ 
&\dfrac{\partial F_3 }{\partial \xi}= J_{31}  \qquad 
\dfrac{\partial F_3 }{\partial \eta}= J_{32}  \qquad 
\dfrac{\partial F_3 }{\partial \zeta}=J_{33}  \\ 
\end{align*}
\item for $\hat{\bphi}_c(\xi,\eta,\zeta)$
\begin{align*}
&\dfrac{\partial F_1 }{\partial \xi}=-J_{11}  \qquad 
\dfrac{\partial F_1 }{\partial \eta}= -J_{12} \qquad 
\dfrac{\partial F_1 }{\partial \zeta}= -J_{13} \\ 
&\dfrac{\partial F_2 }{\partial \xi}= -J_{21} \qquad 
\dfrac{\partial F_2 }{\partial \eta}= -J_{22}  \qquad 
\dfrac{\partial F_2 }{\partial \zeta}= -J_{23}  \\ 
&\dfrac{\partial F_3 }{\partial \xi}= -J_{31}  \qquad 
\dfrac{\partial F_3 }{\partial \eta}= -J_{32}  \qquad 
\dfrac{\partial F_3 }{\partial \zeta}= -J_{33}  \\ 
\end{align*}
\item for $\hat{\bphi}_d(\xi,\eta,\zeta)$
\begin{align*}
&\dfrac{\partial F_1 }{\partial \xi}=J_{11}  \qquad 
\dfrac{\partial F_1 }{\partial \eta}= J_{12} \qquad 
\dfrac{\partial F_1 }{\partial \zeta}= J_{13} \\ 
&\dfrac{\partial F_2 }{\partial \xi}= J_{21} \qquad 
\dfrac{\partial F_2 }{\partial \eta}= J_{22}  \qquad 
\dfrac{\partial F_2 }{\partial \zeta}= J_{23}  \\ 
&\dfrac{\partial F_3 }{\partial \xi}= J_{31}  \qquad 
\dfrac{\partial F_3 }{\partial \eta}= J_{32}  \qquad 
\dfrac{\partial F_3 }{\partial \zeta}=J_{33}  \\ 
\end{align*}
\end{itemize}

So:
\begin{align*}
\text{div}_{\bbx} \bphi_{a,b,c,d}(\bbx)&=\pm \dfrac{1}{\text{detJ}}    \bJ : \bJ^{-T}
\end{align*}


We obtain:
\begin{align*}
\text{div}_{\bbx} \bphi_a(\bbx)
&=\dfrac{1}{\text{detJ}}  \bigg [
 +J_{11}  \dfrac{d \xi}{dx} +J_{12}\dfrac{d \eta}{dx} +
  J_{21}\dfrac{d \xi }{dy}
+ J_{22}  \dfrac{d \eta }{dy}
\bigg ]\\
\text{div}_{\bbx} \bphi_b(\bbx)
&=\dfrac{1}{\text{detJ}}  \bigg [
 +J_{11}  \dfrac{d \xi}{dx} +J_{12}\dfrac{d \eta}{dx} +
  J_{21}\dfrac{d \xi }{dy}
+ J_{22}  \dfrac{d \eta }{dy}
\bigg ]\\
\text{div}_{\bbx} \bphi_c(\bbx)
&=\dfrac{1}{\text{detJ}}  \bigg [
 -J_{11}  \dfrac{d \xi}{dx} -
 J_{12}\dfrac{d \eta}{dx} -
  J_{21}\dfrac{d \xi }{dy}
-J_{22}  \dfrac{d \eta }{dy}
\bigg ]\\
\end{align*}
We know that:
\begin{align*}
J^{-1}=\begin{bmatrix}
\dfrac{\partial \xi}{\partial x} &\dfrac{\partial \xi}{\partial y}  & \dfrac{\partial \xi}{\partial z} \\
\dfrac{\partial \eta}{\partial x} &\dfrac{\partial \eta}{\partial y} &\dfrac{\partial \eta}{\partial z} \\
\dfrac{\partial \zeta}{\partial x} &\dfrac{\partial \zeta}{\partial y} &\dfrac{\partial \zeta}{\partial z} \\
\end{bmatrix}=\dfrac{1}{\text{det J}}
\begin{bmatrix}
J_{22} J_{33}- J_{23} J_{32}&
 J_{13} J_{32} - J_{12}J_{33}&
 J_{12}J_{23}-J_{13}J_{22}\\
 J_{23} J_{31}- J_{21} J_{33}&
 J_{11} J_{33} - J_{13}J_{31}&
 J_{13}J_{21}-J_{11}J_{23}\\
 J_{21} J_{32}- J_{22} J_{31}&
 J_{12} J_{31} - J_{11}J_{32}&
 J_{11}J_{22}-J_{12}J_{21}\\
\end{bmatrix}
\end{align*}
So that:
\begin{align*}
\begin{cases}
\text{div}_{\bbx} \bphi(\bbx)=\alpha \dfrac{3}{\text{detJ}}  \\
\alpha =\begin{cases}
+1 \qquad \text{outward normal}\\
-1 \qquad \text{inward normal}\\
\end{cases}
\end{cases}
\end{align*}
For a generic dimension $d=2,3$:
\begin{align*}
\begin{cases}
\text{div}_{\bbx} \bphi(\bbx)=\alpha \dfrac{d}{\text{detJ}}  \\
d=\text{dimension}\\
\alpha =\begin{cases}
+1 \qquad \text{outward normal}\\
-1 \qquad \text{inward normal}\\
\end{cases}
\end{cases}
\end{align*}

\section{Mass Matrix}
Given the reference shape functions:
\begin{align*}
\hat{\bphi}_a(\xi,\eta) =\begin{bmatrix}
\xi \\
\eta -1
\end{bmatrix}
\qquad
\hat{\bphi}_b(\xi,\eta) =\begin{bmatrix}
\xi \\
\eta 
\end{bmatrix}
\qquad
\hat{\bphi}_c(\xi,\eta) =\begin{bmatrix}
1-\xi \\
-\eta
\end{bmatrix}
\end{align*}
We want to compute the reference mass matrix:
\begin{align*}
M_{loc}=\int_{\hat{K}} \hat{\bphi}_i(\xi,\eta) \cdot \hat{\bphi}_j(\xi,\eta) d\hat{K} \qquad i,\:j=a,b,c
\end{align*}
The result is:
\begin{align*}
M_{loc}=
\begin{bmatrix}
\frac{1}{3} & 0 & \frac{1}{6}\\
0 & \frac{1}{6} & 0\\
\frac{1}{6} & 0 & \frac{1}{3}
\end{bmatrix}
\end{align*}
In general we want to compute:
\begin{align*}
M=\int_{{K}} {\bphi}_i(x,y) \cdot{\bphi}_j(x,y) {K} =\dfrac{1}{\text{det}J}
\int_{\hat{K}} (\bJ \hat{\bphi}_i(\xi,\eta))\cdot (\bJ \hat{\bphi}_j(\xi,\eta)) d\hat{K}=\int_{\hat{K}} \psi(\bJ, \xi,\eta,i,j) d\hat{K}
\qquad i,\:j=a,b,c
\end{align*}
where:
\begin{align*}
 \psi(\bJ, \xi,\eta,i,j) =
c_{xx} \hat{\bphi}_{i,x}\hat{\bphi}_{j,x} +
c_{yy} \hat{\bphi}_{i,y}\hat{\bphi}_{j,y} +
c_{xy}(\hat{\bphi}_{i,x}\hat{\bphi}_{j,y} +\hat{\bphi}_{i,y}\hat{\bphi}_{j,x} )\\
c_{xx} =(J_{11}^2+J_{21}^2) \qquad 
c_{yy} =(J_{12}^2+J_{22}^2) \qquad
c_{xy} =(J_{12} J_{11} + J_{21} J_{22}) 
\end{align*}
If we define:
\begin{align*}
a_1= \frac{1}{12|\text{det}|}  \qquad a_2=\frac{1}{4|\text{det}| }
\end{align*}


    
\begin{center}
  \begin{tabular}{ | l |c| c | r | }
    \hline
(i,j) &            & &      \\ \hline
(1,1)&    $a_1$ & $a_2$ &  $-a_2$  \\ \hline
(1,2)&    $a_1$ & $-a_1$ &$-a_1$\\ \hline
(1,3)&    $a_1$ & $a_1$ &$- a_2 $\\ \hline
(2,2)&    $a_1$ & $a_1$ & $a_1 $\\ \hline
(2,3)&    $a_1$ & $-a_1$ &$a_1 $\\ \hline
(3,3)&    $a_2$ & $a_1$ & $-a_2$ \\
    \hline
  \end{tabular}
\end{center}

For the computation of a mixed term ($\phi_j$ shape function for the displacement):
\begin{align*}
\int_{{K}} \text{div}{\bphi}_i(x,y) {\phi}_j(x,y) d{K} =
|\text{det}J| \int_{\hat{K}} \alpha \: \dfrac{\text{dim}}{\text{det}J} \: {\phi}_j(\xi,\eta) d \hat{K} =
\alpha \: \text{dim} \: \text{sign}J \int_{\hat{K}}  {\phi}_j(\xi,\eta) d \hat{K} 
\end{align*}

\section{Essential Boundary Condition}
Neumann conditions become essential in the Raviart-Thomas framework:
\begin{align*}
\bu \cdot \bn = f \qquad \Gamma_N
\end{align*}
Substituing $\bu = \sum_{i=1}^N U_i \bphi_i(x,y)$ and taking advantage of the support of each shape function, we obtain that on the boundayr:
\begin{align*}
\sum_{i=1}^N U_i \bphi_i(x,y) \cdot \bn =  U_i \bphi_i(x,y) \cdot \bn =  U_i  \alpha \dfrac{1}{L} = f \qquad \Gamma_N
\end{align*}
\begin{align*}
\begin{cases}
 U_i  \:\alpha \:\frac{1}{L} = f \qquad \Gamma_N\\
 \alpha=\begin{cases}
 +1 \qquad \text{outward Raviart-Thomas's normal}\\
 -1 \qquad \text{inward Raviart-Thomas's normal}
 \end{cases}\\
 L= \text{length of the side of the triangle}
 \end{cases}
\end{align*}
So that:
\begin{align*}
 U_i  -\alpha \: L \: f=0 \qquad \forall i \: \text{on} \: \Gamma_N
\end{align*}





\section{MultiGrid}
Classical iterative methods are fast in removing high frequency components, while are quite slow in removing the slowest ones.
We define the following norm:
\begin{align*}
||| x|||_s = (x,A^sx)^{\frac{1}{2}}
\end{align*}
It can be shown (nota: la norma L2 la scomponi sui triangoli, trovi i coefficienti moltiplicativi delle componenti di v come integrali, quindi hai due norme in Rn e sono equivalenti...qualcosa del genere):
\begin{align*}
c^{-1}||v||_0 \leq |||v|||_0 \leq c ||v||_0
\end{align*}
For second order elliptic problems, we have:
\begin{align*}
|||v|||_1^2=(v,A v)=a(v,v)
\end{align*}
and so:
\begin{align*}
\alpha ||v||_1 \leq ||| v|||_1 \leq M ||v ||_1
\end{align*}
We have that an eigenvalue is defined with the Rayleigh quotient and by taking advantage of the norm equivalence:
\begin{align*}
\lambda = \dfrac{\left(x, A_hx\right)}{(x,x)}=\dfrac{|||x|||_1}{|||x|||_0}\geq \dfrac{||x||_1}{||x||_0} =\dfrac{||x||_0 + |x|_1}{||x||_0} =1+ \dfrac{ |x|_1}{||x||_0}
\end{align*}
 The bigger the gradients (oscillations), the bigger the eigenvalues.\\\\
 COMUNQUE non capisco perche' uno smoother agisca in questo modo. \\\\
 In the Hackbusch framework, we can express the two-grid iteration as:
 \begin{align*}
 u^{1,k+1}-u_1=M (u^{1,k}-u_1)
 \end{align*}
 The matrix is:
 \begin{align*}
 M=S^{\nu_2}\left( I - p A_{2h}^{-1} r A_h \right)S^{\nu_1}=
 S^{\nu_2}\left( A_h^{-1}- p A_{2h}^{-1} r  \right) A_hS^{\nu_1}
 \end{align*}
 where $S^{\nu_1}$ represents the pre-smoothing,  while $S^{\nu_2}$ is the post-smoothing. The operators $r$ and $p$ are respectively the reduction and prolongation operators. So the term $p A_{2h}^{-1} r A_h $ represents the coarse-grid correction for a two-grid method.\\
 In order to have a convergent method indipendently on the mesh-size, we could use two inequalities of this kind:
 \begin{align*}
 &\text{Smoothing property:} \qquad\qquad ||A S^{\nu}|| \leq \dfrac{c}{\nu} h^{-2}\\
 & \text{Approximation property:} \qquad || A_h^{-1}- p A_{2h}^{-1} r|| \leq c h^2
 \end{align*}
 In order to show the convergence of the multigrid method we always need these two properties.\\\\
 Hypotheses:
\begin{itemize}
\item The boundary value problem is $H^1$ or $H_0^1$ elliptic
\item The boundary value problem is $H^2$ regular
\item The spaces $S_l$ are nested ($S_{l-1} \subset S_l$) and belong to a family of conforming FEM with uniform a triangulation.
\item We use nodal basis
\end{itemize} 
We remark that $u^{l,k,m}$ is the solution at level $l$ and iteration $k$. Furthermore $m=1,2,3$ that stand for: after presmoothing, after coarse-grid correction, after post-smoothing. The case $m=0$ is omitted and is in general equal to $m=3$ of the previous iteration: $u^{l,k+1} = u^{l,k+1,0}= u^{l,k,3}$.\\\\
\textbf{Convergence proof for the two-grid algorithm}\\
Using Jacobi relaxation with $\omega=\lambda_{max}(A_h)$:
\begin{align*}
|| u^{1,k+1}-u_1||_0 \leq \dfrac{c}{\nu_1} || u^{1,k}-u^1|| 
\end{align*}
\textbf{proof}\\
For smoothing with Richardson relaxation, if $u_1=u_{l_1}$ is the desired solution:
\begin{align*}
u^{1,k,1}-u_1=\left(I- \frac{1}{\omega} A_h \right) \left( u^{1,k}-u_1 \right)
\end{align*}
 We are going to use these three different inequalities:
 \begin{itemize}
 \item Smoothing property: 
 \begin{align*}
 ||| u^{1,k,1}-u_1|||_2 \leq \dfrac{c}{\nu_1} h^{-2}||u^{1,k}-u_1 ||
 \end{align*}
 \item Approximation property:
 \begin{align*}
 ||u^{1,k,2}-u_1||\leq c h^2 ||| u^{1,k,1}-u_1||_0
 \end{align*}
 \item \begin{align*}
 || u^{1,k,3}-u_1||_0 \leq c || u^{1,k,2}-u_1||
 \end{align*}
  \end{itemize}
 Taking advantage of the fact that $u^{1,k,3}=u^{1,k+1}$, the proof follows.\\\\
 \textbf{Smoothing Property}\\
 Under the assumption that guarantees the equivalence of the norms $||| \cdot|||_s$ and $|| \cdot ||_0$ and referring to an elliptic problem, exists a c indipendent of h such that:
 \begin{align*}
 &\lambda_{max} (A_h) = \sup \dfrac{(x,A_h x)}{(x,x)}= \sup \dfrac{|||v_h|||_1^2}{|||v_h|||_0^2} \leq c \sup \dfrac{||v_h||_1^2}{||v_h||_0^2} \leq c h^2\\
  &\lambda_{min} (A_h) = \inf \dfrac{(x,A_h x)}{(x,x)}= \inf \dfrac{|||v_h|||_1^2}{|||v_h|||_0^2} \geq c^{-1} \inf \dfrac{||v_h||_1^2}{||v_h||_0^2} \geq c^{-1}\\
 \end{align*}
  Using the next lemma with $t=2$ and $s=0$, we obtain:
  \begin{align*}
  &|||x^{\nu}|||_{s+t} \leq c \nu^{-t/2}|||x^0|||_s\\
  &|||x^{\nu}|||_{2} \leq \dfrac{c}{\nu} h^{-2}|||x^0|||_s
  \end{align*}
  
  \textbf{Lemma}\\
  Let $\omega \geq \lambda_{max}(A_h)$, $s \in \mathbb{R}$, $t>0$ and
  \begin{align*}
  x^{\nu +1} =\left(1 -\dfrac{1}{\omega}A\right) x^{\nu}
  \end{align*}
  Then:
  \begin{align*}
  ||| x^{\nu} |||_{s+t} \leq c \nu^{-t/2} ||| x^0 |||_s
  \end{align*}
  for $c=\left(\dfrac{t \omega}{2 e} \right)^{t/2}$.\\
\textbf{Proof}\\
Expand $x^0=\sum c_i z_i$ and $x^{\nu}=\sum 
(1-\lambda_i/\omega)^{\nu} c_i z_i$. Then it is easy to show:
\begin{align*}
||| x^{\nu} |||_{s+t}^2=
\sum \lambda_i^{s+t} 
\left[ (1-\frac{\lambda_i }{\omega})^{\nu} c_i \right]^2
\leq \omega^t \max_{\xi \in [0,1]} f(\xi) |||x^0|||_s^2
\end{align*}
where $f(\xi)= \xi^t (1-\xi)^{2 \nu}$.
 
\textbf{Approximation Property}\\
We want to use the following lemma:\\
\textbf{Lemma}\\
Given $v_h \in S_h$, let $u_{2h}$ be the solution of the weak equation
\begin{align*}
a(v_h-u_{2h},w)=0 \qquad \forall w \in S_{2h}
\end{align*}
In addition, let $\Omega$ be convex or have a smooth boundary. Then
\begin{align*}
&||v_h-u_{2h}||_1\leq c 2 h ||v_h||_2\\
&||v_h - u_{2h}||_0 \leq c 2 h ||v_h -u_{2h}||
\end{align*}
\text{Proof}\\
The last equation follows from the $H^2$ regularity. The first one from the ellipticity:
\begin{align*}
&\alpha ||v_h - u_{2h}||_1^2 \leq a(v_h-u_{2h},v_h-u_{2h})
=a(v_h-u_{2h},v_h)=(v_h-u_{2h},A_h v_h)\leq \\
&|||v_h-u_{2h}|||_0 |||v_h|||_2 \leq c_1 ||v_h-u_{2h}||_0 ||v_h||_2 
\leq c_1 c_2 2 h ||v_h-u_{2h}||_1 ||v_h||_2 
\end{align*}
Combining both inequality we obtain:
\begin{align*}
&||v_h - u_{2h}||_0 \leq c  h^2 ||v_h||_2
\end{align*}
We can use this in the demonstration of the theorem considering the fact that $u^{1,k,2}=u^{1,k,1}+u_{2h}$ satisfies:
\begin{align*}
a(u^{1,k,1}+u_{2h},w)=(f,w)_0 \qquad \forall w \: \in S_{2h}
\end{align*}
and that $u_1$ satisfies $a(u_1,w)=(f,w)$ $\forall w\: \in S_h$ and so $\forall w\: \in S_{2h} \subset S_h$ . Subtracting the two equations:
\begin{align*}
a(u^{1,k,1}-u
_1+u_{2h},w)=0 \qquad \forall w \: \in S_{2h}
\end{align*}
 Indeed we can consider $v_h= u^{1,k,1}-u_1$.
 \section{Iterative methods by subspace correction[by Xu Jinchao]}
 A decomposition ov $V$ consists of a number of subspaces $V_i \subset V$ (for $1\leq i \leq J$) such that:
 \begin{align*}
 V=\sum_{i=1}^J V_i
 \end{align*}
 Thus for each $v \in V$ there esists $v_i \in V_i$ such that $v=\sum_{i=1}^J v_i$. This representation is not unique.\\
 We can define three different operators:
 \begin{itemize}
 \item $Q_i : V  \to V_i$ orthogonal projection w.r.t. $(\cdot,cdot)$:
 \begin{align*}
 (Q_i u, v_i)=(u,v_i)\qquad u \in V, v_i \in V_i
 \end{align*}
 \item$P_i : V  \to V_i$ orthogonal projection w.r.t. $(\cdot,cdot)_A$
  \begin{align*}
 (P_i u, v_i)_A=(u,v_i)_A \qquad u \in V, v_i \in V_i
 \end{align*}
 \item $A_i : V_i  \to V_i$
   \begin{align*}
 (A_i u_i, v_i)=(A u,v_i) \qquad u \in V, v_i \in V_i
 \end{align*}
 \end{itemize}
 So that:
 \begin{align*}
 A_i P_i = Q_i A
 \end{align*}
 that also implies $A_i u_i = f_i$, with $f_i = Q_i f$ and $u_i=P_i u$, is equivalent to $A u = f$.\\\\
 \textbf{Parallel subspace correction methods: PSC}\\

 \begin{itemize}
 \item $r_{old}= f - A u_{old}$
 \item $A e = r_{old}$
 \item Restrict the residual equation to $V_i$, $ A_i e_i=Q_i r_{old}$, with $e_i =P_i e$
 \item Solve approximately $\hat{e_i}=R_i Q_i r_{old}$
 \item Correct $u_{new}=u_{old} +\sum_{i=1}^{J} \hat{e_i}=u_{old} +B(f-A u_{old})$, with $B=\sum_{i=1}^{J} R_i Q_i $.
 \end{itemize}
 With $V=R^n$, we obtain the Jacobi algorithm.
  \textbf{Successive subspace correction methods: SSC}\\
 \begin{itemize}
 \item Given $u_0 \in V$ and $u_k \in V_k$.
 \item $u_{k+i/J}=u_{k+(i-1)/J}+R_i Q_i (f-A u_{k+(i-1)/J})=u_{k+(i-1)/J}+R_i Q_i f- T_i u_{k+(i-1)/J}$
 \end{itemize}
Note that $T_i=R_i A_i P_i : V \to V_i$ is symmetric with respect to $(,)_A$. We have:
\begin{align*}
u - u_{k+1}=E_j (u -u_k) \qquad E_j=(I-T_J)...(I-T_1)
\end{align*}
since $ u-u_{k+i/J}=(I-T_i)(u-u_{k+(i-1)/J})$.
\section{Multigrid Hdiv}
If $\bu \in \text{Ker}(\text{div})$ and $\bw \in\text{Ker}^{\perp}(\text{div})$, then:
\begin{align*}
(\bu,\bw)_{L^2(\Omega)}=0
\end{align*}
The only way to make appear a $\text{div}\bu=0$ is that $\bw=\nabla a$ and that $\bu \cdot \bn =0$ on $\partial \Omega$. In this way:
\begin{align*}
(\bu,\bw)_{L^2(\Omega)}=
(\bu,\nabla a)_{L^2(\Omega)}=
(\bu \cdot \bn, a)_{L^2(\partial \Omega)}
-(\text{div}\bu, a)_{L^2(\Omega)}=
0
\end{align*}
Similarly, if $\bu \in \text{Ker}(\curl)$ and $\bw \in\text{Ker}^{\perp}(\curl)$, then:
\begin{align*}
(\bu,\bw)_{L^2(\Omega)}=0
\end{align*}
The only way to make appear a $\curl \bu=0$ is that $\bw=\curl \br $ and that $\bu \cdot \bt =0$ on $\partial \Omega$:
\begin{align*}
(\bu,\bw)_{L^2(\Omega)}=
(\bu,\curl \br)_{L^2(\Omega)}=
(\bu \cdot \bt, \br)_{L^2(\partial \Omega)}+
(\curl \bu, \br)_{L^2(\Omega)}=
0
\end{align*}
\subsection{Orthogonal decomposition}
Let us consider an operator $\mathcal{D}$ and the space:
\begin{align*}
H(\mathcal{D},\Omega)=\{ \bu \in L^2(\Omega), \quad \mathcal{D} \bu \in L^2(\Omega)\}
\end{align*}
\begin{itemize}
\item  $\mathcal{D}=\nabla$, $H=H_0^1(\Omega)$, $\mathcal{V}=\text{ Ker}(\mathcal{\nabla})$:
\begin{align*}
\mathcal{V}= \{\bu \in L^2(\Omega), \bu =\text{const}=0 \}\qquad H_0^1(\Omega)=\mathcal{V}^{\perp}
\end{align*}
\item $\mathcal{D}=\text{div}$, $H=H_{0,div}(\Omega)$, $\mathcal{V}=\text{ Ker}(\mathcal{\text{div}})$:
\begin{align*}
\mathcal{V}= \{\bu \in L^2(\Omega), \text{div}\bu =0 \}\neq \{0\} \qquad H_{0,\text{div}}=\{ \bu =\bv+\bw,\quad\bu \in \mathcal{V} ,\quad \bw \in \mathcal{V}^{\perp} \}
\end{align*}
\item $\mathcal{D}=\nabla \times $, $H=H_{0,curl}(\Omega)$, $\mathcal{V}=\text{ Ker}(\mathcal{\text{curl}})$:
\begin{align*}
\mathcal{V}= \{\bu \in L^2(\Omega), \text{curl}\bu =0 \}\neq \{0\} \qquad H_{0,\text{curl}}=\{ \bu =\bv+\bw,\quad\bu \in \mathcal{V} ,\quad \bw \in \mathcal{V}^{\perp} \}
\end{align*}
This suggest an Helmholtz decomposition. For $\bu \in H(\mathcal{D},\Omega)$:
\begin{align*}
\bu =\bv +\bw =\nabla \phi + \nabla \times \br
\end{align*}
where $ \nabla \times \br / \nabla \phi $ 
$\in$ $ \mathcal{V}(\text{div})  /\mathcal{V}(\nabla \times)$,
while $\nabla \phi / \nabla \times \br$ 
$\in$ $ \mathcal{V}(\text{div}) ^{\perp} /\mathcal{V}(\nabla \times)^{\perp}$.
\end{itemize}

\subsection{Hdiv and Hcurl}
Let us consider the following bilinear form:
\begin{align*}
a(\bu,\bv)=(\bu,\bv)+(\text{div} \bu, \text{div}\bv)
\end{align*}
The eingevalues related to this form are:
\begin{align*}
\lambda = \dfrac{a(\bu,\bu)}{(\bu,\bv)}=1 +  \dfrac{(\text{div} \bu, \text{div}\bu)}{(\bu,\bu)}
\end{align*}
Since we can say that:
\begin{align*}
\begin{cases}
(a+b+c)^2 \leq 3 (a^2+b^2+c^2) \\
a=u_x,\:b=v_y,\: c=w_z
\end{cases}
\qquad \to \qquad \text{div} \bu ^2 \leq C |\nabla \bu |^2
\end{align*}
where $a=u_x$, $b=v_y$ and $c=w_z$, then we have that:
\begin{itemize}
\item a function $\bu$ with $\text{div}\bu=0$ can have whatever value of the gradient;
\item a function $\bu$ with $\text{div}\bu \gg 1$ must have large values of the gradient;
\end{itemize} 
By this simple analysis, we discover, without using the Helmoltz decomposition, that for the kernel of the operator $\text{div}$ high eigenvalues do not correspond to high-frequency function. On the other hand, for the complement of the kernel of the operator $\text{div}$, high eigenvalues correspond to high-frequency function.\\
Nevertheless, in a discrete setting, the eigenvalues that a discrete operator captures are the smallest one. Then, by increasing the mesh and so the size of the problem, we can also reach larger eigenvalues.\\
On a very coarse mesh we can only capture components of the error associated to small eigenvalues. In this case, anyhow, smalle eigenvalues can have high frequency components (null divergence, high gradients). And it is obvious that such components cannot be well described on a coarse mesh. \\
By Helmoltz decomposition, $\bu= \nabla \phi + \nabla \times \br$. Then:
\begin{itemize}
\item $\bu = \nabla \phi $, using $\Delta \nabla \phi= \nabla \Delta \phi$ and homogeneous BC:
\begin{align*}
(\bu,\bv)+(\text{div} \nabla \phi, \text{div}\bv)&=
(\bu,\bu)-(\nabla \text{div} \nabla \phi,\bv)\\&=
(\bu,\bv)-( \nabla \Delta  \phi,\bv)\\&=
(\bu,\bv)-( \Delta  \nabla \phi,\bv)\\&=
(\bu,\bv)-( \Delta  \bu,\bv)
\end{align*}
So that the operator is:
\begin{align*}
\mathcal{A}= \bI - \Delta 
\end{align*}
\item $\bu = \nabla \times \br $, using $\text{div}\nabla \times \b= 0$:
\begin{align*}
(\bu,\bv)+(\text{div} \nabla \times \br, \text{div}\bv)=
(\bu,\bu)
\end{align*}
So the operator is:
\begin{align*}
\mathcal{A}= \bI 
\end{align*}
\end{itemize}
Now we consider the following bilinear form:
\begin{align*}
a(\bu,\bv)=(\bu,\bv)+(\nabla \times  \bu, \nabla \times \bv)
\end{align*}
By Helmoltz decomposition, $\bu= \nabla \phi + \nabla \times \br$. Then:
\begin{itemize}
\item $\bu = \nabla \phi $, using $ \nabla \times \nabla\phi= 0$:
\begin{align*}
(\bu,\bv)+(\nabla \times \nabla \phi, \nabla \times \bv)&=
(\bu,\bu)
\end{align*}
So that the operator is:
\begin{align*}
\mathcal{A}= \bI 
\end{align*}
\item $\bu = \nabla \times $, using $\nabla \times \nabla \times \br= \nabla \text{div} -\Delta$, homogeneous BC, the curl-integration by parts formula and $\nabla \times \Delta = \Delta \times \nabla$:
\begin{align*}
(\bu,\bv)+(\nabla \times \nabla \times \br, \nabla \times\bv)&=
(\bu,\bv)+(\nabla \text{div}\br-\Delta \br , \nabla \times\bv)\\&=
(\bu,\bv)+(\nabla \times\nabla \text{div}\br-\nabla \times\Delta \br , \bv)\\&=
(\bu,\bv)-(\nabla \times\Delta \br , \bv)\\&=
(\bu,\bv)-(\Delta \bu , \bv)
\end{align*}
So the operator is:
\begin{align*}
\mathcal{A}=   \bI - \Delta
\end{align*}
\end{itemize}
\subsection{Commuting diagram}
For simply connected domains:
\begin{figure}[htbp!]
		\centering
	\includegraphics[width=0.75\textwidth]{img/commuting_diagram}
		\caption[Commuting diagram]{Commuting diagram.}
		\label{abb_arc}
\end{figure}
where the interpolation operators defined above preserve the kernels of the differential operators:
\begin{align*}
\bbx \in \mathcal{N}_d(\mathcal{D},T_h) \cap  \mathcal{D} \bbx =0 \qquad \iff \qquad \exists \by \in \mathcal{N}_d(\tilde{\mathcal{D}},T_h) ,\quad \bbx = \tilde{\mathcal{D}} \by    \\
\begin{cases}
H(\textbf{curl} ,\Omega)=\nabla H(\nabla,\Omega)\\
H(\text{div} ,\Omega)=\textbf{curl} H(\textbf{curl},\Omega)
\end{cases}
\end{align*}
\begin{table}[htbp!]
\begin{center}
\begin{tabular}[c]{l ||l |}
  $\mathcal{D} $ & $\tilde{\mathcal{D}}$ \\ \hline 	
\textbf{curl}  &$\nabla$ 		\\ \hline
 div & \textbf{curl}  \\ \hline
\end{tabular}
\end{center}
\caption{Table relating $H(\mathcal{D},\Omega)$ and $H(\tilde{\mathcal{D}},\Omega)$ }
\label{commuting_diagram}
 \end{table}\\
 where $\tilde{\mathcal{D}}$ is the potential operator associated with $\mathcal{D}$ satisfying $\mathcal{D} \tilde{\mathcal{D}}=0$.\\\\
 If we define $\mathcal{N}(\mathcal{D})=\text{Ker}(\mathcal{D})$, then $\mathcal{N}(\text{div})=\textbf{curl}H(\textbf{curl},\Omega)$ and  $\mathcal{N}(\textbf{curl})=\nabla H(\nabla,\Omega)$, so that generally we can state, \textit{for domains topologically equivalent to a ball}:
 \begin{align*}
 \mathcal{N}(\mathcal{D})=\tilde{\mathcal{D}}H(\tilde{\mathcal{D}})
 \end{align*}
 Since:
 \begin{align*}
 \mathcal{N}(\mathcal{D})=\tilde{\mathcal{D}}H(\tilde{\mathcal{D}})=\tilde{\mathcal{D}}\left( \mathcal{N}(\tilde{\mathcal{D}}) \oplus \mathcal{N}^{\perp}(\tilde{\mathcal{D}})  \right)=\tilde{\mathcal{D}} \mathcal{N}^{\perp}(\tilde{\mathcal{D}})
 \end{align*}
 So we can assert that:
 \begin{align*}
 H(\mathcal{D},\Omega) = \tilde{\mathcal{D}}\mathcal{N}^{\perp}(\tilde{\mathcal{D}}) \oplus  \mathcal{N}(\mathcal{D})^{\perp}
 \qquad \to \qquad 
 \begin{cases}
 H(\text{div},\Omega) =\textbf{curl} H^{\perp}(\textbf{curl},\Omega) \oplus \mathcal{N}(\text{div})^{\perp}\\
  H(\textbf{curl},\Omega) =\nabla H^{\perp}(\nabla,\Omega) \oplus \mathcal{N}(\textbf{curl})^{\perp}\\
 \end{cases}
 \end{align*}
 \begin{itemize}
 \item We can treat the two components separately of $\bu=\bv+\bw$ separately.
 \item We can move from the continuous to the discretized setting, because, thanks to the above projections operators and the choice of the proper finite elemenet spaces (Raviart-Thomas, Nedelec), a lot of properties are directly inherited.
 \item In order to define a preconditioner for $\mathcal{N}^{\perp}(\mathcal{D})$ it is sufficient only approximate orthogonality.
 \item In order to define a preconditioner of $\mathcal{N}(\mathcal{D})$ we need to switch to discrete potentials using the above relations.
 \end{itemize}
 \subsection{Smoothing algorithm}
 We have to decrease the error of both component, $ \tilde{\mathcal{D}}\mathcal{N}^{\perp}(\tilde{\mathcal{D}}) $ and $ \mathcal{N}(\mathcal{D})^{\perp} $. Regarding the $H_{div}$ space, we have to smooth  $\bu=\bv+\bw$, where $\bv \in \textbf{curl} H^{\perp}(\textbf{curl},\Omega) $ and $\bw \in \mathcal{N}(\text{div})^{\perp}$.
 \begin{itemize}
 \item The $\bw$ component si referred to an elliptic problem. This means that we can take into consideration the usual smoother. We should describe this component in $\mathcal{N}(\text{div})^{\perp}$. Anyhow, since a representation for $\mathcal{N}(\text{div})^{\perp}$ is not easy to build and since $\mathcal{N}(\text{div})^{\perp} \subset H_{div}$, we can use the whole Raviart-Thomas finite element space to define the orthogonal component $\bw$.
 \item On the other hand, to level down the $\bv$ we take advantage of the  potential discretization. In this case, the bilinear form is:
\begin{align*}
a(\bu,\bv)|_{\mathcal{N}(\text{div})}=
(\bu,\bv)=( \textbf{curl} \:\br , \textbf{curl}  \: \bs) =
(f, \textbf{curl} \: \bs) \qquad \forall \bs \in H(\textbf{curl},\Omega)
\end{align*}
We see that $\curl \br \in  \curl H(\curl,\Omega)\subset H(\text{div},\Omega)$. 
 \end{itemize}
 So basically:
 \begin{itemize}
 \item Smooth $A \bu =\bf$ (start with $\bu=0$).
 \item Compute $\br=\bf-A\bu$.
 \item Transfer from Raviart-Thomas to Nedelec space, $\bs=T \br$.
 \item Smoot $B \bsigma =\bs$ (start with $\bsigma=0$).
 \item Go back to the Raviart-Thomas space, $\br=T^* \bs$.
 \item Update the solution $\bu=\bu+\br$.
 \end{itemize}
 \subsection{Relation between Nedelec and Raviart spaces}
 We imopse:
 \begin{align*}
 &\int_{\Omega} U_{ND} \phi_{RT} = \int_{\Omega} U_{RT} \phi_{RT} 
 \quad  \quad  \int_{\Omega} U_{RT} \phi_{ND} = \int_{\Omega} U_{ND} \phi_{ND} 
  \end{align*}
  And get:
  \begin{align*}
&  \sum_j \int_{\Omega}  \phi_{ND,j} U_{ND,j} \phi_{RT,i} = \sum_i \int_{\Omega}  \phi_{RT,j}  U_{RT,j} \phi_{RT,i} 
   \quad \to \quad
  M_{RT,ND}  U_{ND}= M_{RT,RT} U_{RT}\\
  &  \sum_j \int_{\Omega}  \phi_{RT,j} U_{RT,j} \phi_{ND,i} = \sum_i \int_{\Omega}  \phi_{ND,j}  U_{ND,j} \phi_{ND,i} 
   \quad \to \quad
  M_{ND,RT}  U_{RT}= M_{ND,ND} U_{ND}\\
 \end{align*}
  It SEEMS that we have define the prolongation operator $T: \ND \to \RT$. Anyhow in the algorithm we use $T^*$ to transfer the residual, that does not belong to $H_{div}$ but to its dual, into the $\ND$ space. Nevertheless also this residual should belong to the dual of $\curl H(\curl)$ and not to the space itself.
  Then we should transfer back the solution $\bxi$ in  $\curl H(\curl)$, so we would need an operator $\mathcal{B}: \curl H(\curl) \to \ND$. SONO CONFUSO.\\
Furthermore we have $T: \ND \to \RT$. This relation should follow from the embedding $\curl H(\curl)  \subset H_{div}$. But then I should consider  $T: \curl \ND \to \RT$. In this way, $ \curl \ND \subset \RT$. Reasoning in this way, the space $ \curl \ND$ would be reduced to a piecewise constant polynomial space. Indeed the sum of curls of linear functions is a constant on each element. This implies that we can consider, instead of a dof for each edge, only a dof per element. Then we should consider as $L^2$ projection:
 \begin{align*} 
 (\bu_{\curl \ND},\nabla \phi_{i,\ND})&= (\bu_{\RT}  , \nabla \phi_{i,\ND} ) \\
  \sum_j u_{j,\curl \ND} (\nabla \phi_{j,\ND},\nabla \phi_{i,\ND})&=\sum_k \bu_{k,\RT}  (\phi_{j,\RT}  , \nabla \phi_{i,\ND} )
 \end{align*}
 Thus:
  \begin{align*} 
   &\: \:\bK_{\ND}\:\bu_{\curl \ND} =\bA _{\nabla {\ND} , \RT }\\
 &  \begin{cases}
 \bK_{\ND,\:ij } = (\nabla \phi_{i,\ND}, \nabla \phi_{j,\ND})\\
 \bA _{\nabla {\ND} , \RT, \: ij }= (\phi_{j,\RT}  , \nabla \phi_{i,\ND} )
 \end{cases}
 \end{align*}
 The stiffness matrix $\bK$ is singular, while $\bA$ is, in principle, rectangular. The vector $\bu_{\curl \ND}$ belongs to $\curl \ND \approx Q_0$, where $Q_0$ is the piecewise constant space. This means that the problem that we have to solve is:
 \begin{align*}
 \bM_{Q_0} \bu_{\curl \ND}= \bff_{\ND}
 \end{align*}
\textbf{ Nell'articolo usa la matrice C per correggere l'errore del nucleo. MA questa coincide con A trasferita sui Nedelec?\\}
\textbf{NOTA BENE}:\\
Sia X lo spazio fine con il suo duale X'. Sia Y lo spazio coarse con il suo duale Y'.
Trasferiamo il residuo fine $r_f=b_f-A_f x_f  \: \in X'$ nel sottospazio, ottenendo un residuo $r_c = R  r_f \: \in Y'$.   Questo e' quanto succede all'andata. Al ritorno invece trasferiamo la soluzione, che non appartiene ai duali, bensi' agli spazi veri e propri. Di conseguenza:
\begin{align*}
R: X' \to Y'  \qquad P=R^*: Y -> X\\
\langle R r_f, y\rangle_{Y',Y} = \langle r_f, R^* y\rangle_{X',X}
\end{align*}
\textbf{Quindi, nel momento in cui usiamo la $L^2$ projection, noi vogliamo definire l'operatore di interpolazione $P$ in quanto e' definito tra due spazi le cui basi sono note. \\
Si potrebbe ugualmente definire l'operazione di restrizione in modo equivalente a partire dagli spazi duali, ma in tal caso dovrei conoscerne la base. Allora e' evidente che cio' ha senso solo per L2.}
\subsection{Transfer operator}
The $L^2$ projection does not work between the $\ND$ and the $\RT$ spaces. Let us focus on the reference element $K$. Let us define a a function $f_{\ND} \in \ND(K)$, with coefficients $U_{\ND}=[1,1,1]$. Then consider the operator, defined starting from the $L^2$ projection, $T:\ND \to \RT$:
\begin{align*}
T= M_{\RT,\RT} M_{\RT,\ND} =
\begin{bmatrix}
\frac{1}{3} &-\frac{2}{3} & \frac{1}{3}\\
  \frac{2}{3} &  - \frac{1}{3}  &   \frac{1}{3}\\
    1   & -1&  0\\
\end{bmatrix}
\qquad \to \qquad
u_{\RT}=T u_{\ND}=\frac{2}{3}\begin{bmatrix}
-1\\1\\0
\end{bmatrix}
\end{align*} 
So that we go from:
\begin{align*}
f_{\ND}=\phi_{\ND,1} u_1+ \phi_{\ND,2} u_2 + \phi_{\ND,3} u_3=
\begin{bmatrix}
1-y\\x
\end{bmatrix}+
\begin{bmatrix}
-y\\x
\end{bmatrix}+
\begin{bmatrix}
-y\\x-1
\end{bmatrix}
=
\begin{bmatrix}
1-3y\\3x-1
\end{bmatrix}
\end{align*}
to:
\begin{align*}
f_{\RT}=-\frac{2}{3}
\begin{bmatrix}
x\\-1+y
\end{bmatrix}+
\frac{2}{3}
\begin{bmatrix}
x\\y
\end{bmatrix}+0
\begin{bmatrix}
1-x\\y
\end{bmatrix}
=
\begin{bmatrix}
0\\\frac{2}{3}
\end{bmatrix}
\end{align*}
We see that there is no way to use the $L^2$ projection between these two spaces. Therefore another kind of projection has to be considered. In particular we can see that in 3D, if $\RT_i$ is the value of the function on the $i-$face:
\begin{align*}
\RT_i= \int_{F_i} \bff_{\RT} \cdot \bn dF
\end{align*}
By focusing on the kernel of the operator $\text{div}$, we can write $\bff_{\RT}=\nabla \times \bff_{\ND}$ and, by using Stokes theorem:
\begin{align*}
\RT_i= \int_{F_i}\nabla \times \bff_{\ND} \cdot d\bF=\oint \bff_{\ND} \cdot d \br
\end{align*}
where $d\bF = \bn \cdot dF$ and $d \br = \bt  ds$. Thus:
\begin{align*}
\RT_i= \sum_{j=1}^3 \int_{e_j} \ND_j \phi_{\ND,j} \cdot \bt ds =\sum_{j=1}^3 \pm  \ND_j =\ND_a+\ND_b -\ND_c \\ 
\begin{cases}
\RT_i =\ND_a+\ND_b -\ND_c  & \text{if the face  ordered in counterclockwise direction} \\
\RT_i =-\ND_a-\ND_b +\ND_c  &\text{if the face ordered in clockwise direction} 
\end{cases}
\end{align*}
where $L_j$ are the lenghts of the edges, whose sign depends on the orientation of the edge. If the edge is oriented counterclockwise, then we have a plus, otherwise a minus.\\
This means that in 3D the transfer operator $T:\ND\to \RT$ is a matrix $T \in \mathbb{R}^{m \times n}$, with $m=$ number of faces and $n=$ number of edges, and where for each row only thre entries are non trivial.\\
REMARK: Unlike the usual $L^2$ projection, we have a matrix alread defined, without inverting a mass matrix!!!!\\
This interesting fact also holds for the Raviart-Thomas intergrid operators. When we prolongate a function from a coarser space to a finer one, we want that the average fluxes through the sides are the same. Then, $\Pi: \RT_{0,C} m\to \RT_{0,F}$:
\begin{align*}
\int_{e_i} (\Pi \bv - \bv) \cdot \bn  p_k d \bs =0\qquad \forall e_i \in \mathcal{E}_{F}
\end{align*}
The polynomium $p_k=const$, since we are dealing with $\RT_0$. By substituing
$\bv =\sum_{j=1}^{N_F} \phi_{j}^{\RT_{0,F}} v_{j,F}$ and $\bv =\sum_{k=1}^{N_C} \phi_{k}^{\RT_{0,C}} v_{k,C}$:
\begin{align*}
\sum_{j=1}^{N_F} v_{j,F} \int_{e_i}\phi_{j}^{\RT_{0,F}}  \cdot \bn d \bs =
\sum_{k=1}^{N_C} v_{k,C} \int_{e_i}\phi_{k}^{\RT_{0,C}}  \cdot \bn  d\bs        \qquad \forall e_i \in \mathcal{E}_{F}
\end{align*}
where $ \delta_{ij} = \int_{e_i}\phi_{j}^{\RT_{0,F}}  \cdot \bn d \bs$. Thus:
\begin{align*}
v_{i,F} =
\sum_{k=1}^{N_C} v_{k,C} \int_{e_i}\phi_{k}^{\RT_{0,C}}  \cdot \bn  d\bs        \qquad \forall e_i \in \mathcal{E}_{F}
\end{align*}
Also in this case, we do not need to invert a mass matrix. \\\\
In 2D both curl-free and divergence-free fields can be described respectively by gradient potential and their rotation:
\begin{align*}
& \curl \bu=0&{}   &\bu=\nabla  \phi \\
& \tdiv \bu=0&{}   &\bu=\bR \nabla  \phi \qquad 
\bR=\begin{bmatrix}
0 &1\\-1 &0
\end{bmatrix}
\end{align*}
\textbf{	NOTA BENE: QUESTA E LA MATRICE CORRETTA: INFATTI, SE NE FACCIO IL TRASPOSTO, MI MAPPA VETTORI NORMALI IN VETTORI TANGENTI, DOVE LA NORMALE E ROTAZIONE ORARIA RISPETTO A T. INFATTI I RT LI ABBIAMO DEFINITI CON QUESTA CONVENZIONE.}
where $\bR$ is a COUNTERclockwise rotation.\\
This means that the kernel of both $\ND$ and $\RT$ spaces can be described in the same way, thourgh a scalar potential $\phi$ belonging to P1. Thus:
\begin{align*}
\RT_i= \int_{e_i} \bff_{\RT} \cdot \bn d l= \int_{e_i} \bR \nabla  \phi \cdot \bn d l = \int_{e_i} \nabla  \phi \cdot \bR  \bn d l
= \int_{e_i} \nabla  \phi \cdot \bt d l
\end{align*}
For the fundamental theorem of calculus for line integrals:
\begin{align*}
\RT_i=  \phi(\bbx_2)-\phi(\bbx_1)
\end{align*}


{\small{
\begin {center}
\begin {tikzpicture}[-latex ,auto ,node distance =5.5 cm and 5cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white , bottom color = processblue!0 ,
draw,processblue , text=blue , minimum width =1 cm}]
\node[state] (A){$\small{X_L}$};
\node[state] (D) [above =of A] {$X_L'$};
\node[state] (C) [above right =of A] {$Y_L'$};
\node[state] (B) [ right =of A] {$Y_L$};
\node[state] (A2)[ right =of B] {$X_{L-1}$};
\node[state] (D2) [above =of A2] {$X_{L-1}'$};
\node[state] (C2) [above right =of A2] {$Y_{L-1}'$};
\node[state] (B2) [ right =of A2] {$Y_{L-1}$};
\path (A) edge [loop left] node[below=0.30cm] {$\tiny{S_{\text{Gauss-Seidel}}}$} (A);
\path (D) edge [bend left=0]  node[below =0.02 cm]{$T_L':\RT_L \to \ND$} 
node[above =0.02 cm]{$ \br_{Y_{L}}=T_L' \br_{X_L}$} (C);
\path (B) edge [bend left=0]  node[above =0.15 cm]{$T_L: \ND_L \to \RT_L$}node[below =0.15 cm]{$\bbx_L=T_L \by_L$} (A);
\path (C) edge [bend left=0]  node[left =0.15 cm]{\rotatebox{90}{$ \bC_{L} \by_{L} = \br_{Y_{L}}$}} (B);
\path (A) edge [bend left=0]  node[left =0.15 cm]{\rotatebox{90}{$\br_{X_L}=\textbf{b} - A_L \bbx_L $}} (D);

\path (A2) edge [loop left] node[left] {$S_{\text{Gauss-Seidel}}$} (A2);
\path (D2) edge [bend left=0]  node[below =0.35 cm]{$T_{L-1}':\RT_{L-1} \to \ND{L-1}$} 
node[above =0.15 cm]{$\br_{Y_{L-1}}=T_{L-1}' \br_{X_{L-1}}$} (C2);
\path (B2) edge [bend left=0]  node[above =0.15 cm]{$T_{L-1}: \ND_{L-1} \to \RT_{L-1}$}node[below =0.15 cm]{$\bbx_{L-1}=T_{L-1} \by_
{L-1}$} (A2);
\path (C2) edge [bend left=0]  node[left =0.15 cm]{\rotatebox{90}{$ \bC_{L-1} \by_{L-1} = \br_{Y_{L-1}}$}} (B2);
\path (A2) edge [bend left=0]  node[left =0.15 cm]{\rotatebox{90}{$\br_{X_{L-1}}=\textbf{b} - A_{L-1} \bbx_{L-1} $}} (D2);

\path (D) edge [bend left=30]  node[above =0.15 cm]{$ P_X': X_L' \to X_{L-1}' $} node[below =0.15 cm]{$ \br_{X_{L-1}}=P_X' \br_{X_{L}} $}(D2);
\path (A2) edge [bend left=30]  node[above =0.15 cm]{$ P_X:  X_{L-1}\to X_L $} 
node[below =0.15 cm]{$ \bbx_{L}=P_X \bbx_{L-1}$} (A);

\path (C) edge [bend left=30]  node[above =0.15 cm]{$ P_Y
':  Y_{L}'\to Y_{L-1}' $} (C2);
\path (B2) edge [bend left=30]  node[above =0.15 cm]{$ P_Y:  Y_{L-1}\to Y_{L} $} (B);

\end{tikzpicture}
\end{center}
}}
PROBLEM:\\
in staggered version, it does not converge. Guess: If I impose only dirichlet BC on displacement problem, then the stress-problem is boundary-free and cannot converge.
FROM FINE TO COARSE:
\begin{align*}
\begin{bmatrix}
(P_{\RT_C}^{\RT_F})^T & 0\\
0 & (P_{P_{1,C}}^{P_{1,F}})^T \\
\end{bmatrix}
\begin{bmatrix}
A_{11} & A_{12}\\
A_{12}^T & A_{22}\\
\end{bmatrix}
\begin{bmatrix}
P_{\RT_C}^{\RT_F} & 0\\
0 & P_{P_{1,C}}^{P_{1,F}} \\
\end{bmatrix}
=
\begin{bmatrix}
(P_{\RT_C}^{\RT_F})^T  A_{11} P_{\RT_C}^{\RT_F}& (P_{\RT_C}^{\RT_F})^T  A_{12}P_{P_{1,C}}^{P_{1,F}}\\
(P_{P_{1,C}}^{P_{1,F}})^T A_{12}^T P_{\RT_C}^{\RT_F}& (P_{P_{1,C}}^{P_{1,F}})^T A_{22}P_{P_{1,C}}^{P_{1,F}}\\
\end{bmatrix}
\end{align*}
FROM $\RT$+$P_1$ TO $\ND$+$P_1$:
\begin{align*}
\begin{bmatrix}
(P_{\ND}^{\RT})^T & 0\\
0 & I \\
\end{bmatrix}
\begin{bmatrix}
C_{11} & C_{12}\\
C_{12}^T & C_{22}\\
\end{bmatrix}
\begin{bmatrix}
P_{\ND}^{\RT} & 0\\
0 & I\\
\end{bmatrix}
=
\begin{bmatrix}
(P_{\ND}^{\RT})^T  C_{11} P_{\ND}^{\RT} & (P_{\ND}^{\RT})^T  C_{12}\\
 C_{12}^T P_{\ND}^{\RT} & C_{22}\\
\end{bmatrix}
\end{align*}
\subsection{Linear Elasticity $\ND \to \RT$ operator}
If we deal with a block Gauss-Seidel iterative scheme, then the blocks are:
\begin{align*}
&A_{11}=\int_{\Omega}\text{div}  \bsigma \cdot \text{div} \btau + \mathcal{A} \bsigma : \mathcal{A} \btau 
&A_{12}= - \int_{\Omega} \varepsilon(\bu) :  \mathcal{A} \btau \\
&A_{21}=  -\int_{\Omega} (\mathcal{A} \bsigma-) :  \varepsilon(\bv) 
&A_{22}=  -\int_{\Omega} \varepsilon(\bu)  :  \varepsilon(\bv) 
\end{align*}
Then the pure $H_{\text{div}}$ block is the 11. Inserting in $A_{11}$ divergence-free terms, $\bsigma = \bcurl \phi$ and $\btau = \bcurl \psi$, implies:
\begin{align*}
&C_{11}=\int_{\Omega}\text{div}  \bsigma \cdot \text{div} \btau + \mathcal{A} \bsigma : \mathcal{A} \btau =\int_{\Omega}  \mathcal{A} \bcurl \phi : \mathcal{A} \bcurl \psi
\end{align*}
In 2D:
\begin{align*}
\bR=\begin{bmatrix}
0 &1 \\
-1 &0
\end{bmatrix} \qquad \to \qquad \bcurl \phi= \bR \nabla \phi = \begin{bmatrix}
\phi_y \\
-\phi_x
\end{bmatrix}
\end{align*}
Therefore:
\begin{align*}
C_{11}=&\int_{\Omega} \left( \beta \begin{bmatrix}
\phi_y^1  &-\phi_x^1 \\
\phi_y^2  &-\phi_x^2 \\
\end{bmatrix} + \alpha (\phi_y^1\phi_x^2)\begin{bmatrix}
1  &0 \\
0 &1 \\
\end{bmatrix} \right) :
\left( \beta \begin{bmatrix}
\psi_y^1  &-\psi_x^1 \\
\psi_y^2  &-\psi_x^2 \\
\end{bmatrix} + \alpha (\psi_y^1\psi_x^2)\begin{bmatrix}
1  &0 \\
0 &1 \\
\end{bmatrix} \right) =\\&
\int_{\Omega}  \begin{bmatrix}
\beta^2 \nabla \phi^1 \cdot \nabla \psi^1 + 2 \alpha(\beta+\alpha)  \phi_y^1\psi_y^1 & 
-2 \alpha(\beta+\alpha)\phi_x^2  \psi_y^1  \\
-2 \alpha(\beta+\alpha)\phi_y^1  \psi_x^2 &
\beta^2 \nabla \phi^2 \cdot \nabla \psi^2 + 2 \alpha(\beta+\alpha)  \phi_x^2\psi_x^2  
\end{bmatrix} 
\end{align*}

\begin{align*}
C_{11}&=\beta^2 \nabla \phi^1 \cdot \nabla \psi^1 + 2 \alpha(\beta+\alpha)  \phi_y^1\psi_y^1 \\
C_{12}&=-2 \alpha(\beta+\alpha)\phi_x^2  \psi_y^1  \\
C_{13}&= 0.5 \beta \psi_{1,x} u_{x,y} - \psi_{1,y} u_{x,x}(\alpha + \beta)\\
C_{14}&=0.5 \beta \psi_{1,x} u_{y,x} - \alpha \psi_{1,y} u_{y,y}\\
C_{22}&=\beta^2 \nabla \phi^2 \cdot \nabla \psi^2 + 2 \alpha(\beta+\alpha)  \phi_x^2\psi_x^2 \\
C_{23} &=\alpha \psi_{2,x}u_{x,x} - 0.5 \beta \psi_{2,y} u_{x,y}\\
C_{24}&=\psi_{2,x} u_{y,y}(\alpha + \beta) - 0.5 \beta \psi_{2,y} u_{y,x}
\end{align*}
Idea to solve the system in a monolithic way. We just want to correct the $H_{\text{div}}$ part of the system, since for the $H_1$ is sufficient the projection on coarser spaces. This means that, altought we do not use a Picard iteration, we will not project the whole residual, but only its $H_{\text{div}}$ components.\\
\begin{itemize}
\item Do some smoothing steps on the whole system, improving both displacements and stresses
\item With the given displacement and stresses,  compute only the residual corresponding to the $H_{\text{div}}$ part
\item project this residual that will be the new rhs
\item Since we are dealing only with the stresses, we should project only raws of the matrix related to these ones.  But then we would have a rectangular matrix:
\begin{align*}
\begin{bmatrix}
C_{11} &C_{12} &C_{13} &C_{14}\\
C_{21} &C_{22} &C_{23} &C_{24}\\
\end{bmatrix}
\end{align*} 
Anyhow the blocks on the right ($C_{13} ,C_{14},C_{23} ,C_{24}$) can direclty go to the rhs, since the displacement is now fixed and known. 
\end{itemize}
\section{Linear elasticity projected on $\ND$ space}
When we project the linear elasticity system on the $\ND$ space, we are assuming that:
\begin{itemize}
\item The domain is convex (or homotopic to a ball) so that we can use the Helmoltz decomposition. In this way the decomposition is exact and we can use $\bsigma = \bcurl \bphi$, since $\text{Ker}(\text{div},\Omega)=\bcurl H(\bcurl,\Omega)$. Otherwise $\bcurl H(\bcurl,\Omega) \subset \text{Ker}(\text{div},\Omega)$...
\item Remember that nedelec shape functions are spanned in this way:
\begin{align*}
\bphi_{\ND}= (P_0)^3+ (\bbx \wedge (P_0)^3)=
\begin{bmatrix}
a\\ b \\ c\\
\end{bmatrix}+
\begin{bmatrix}
f y - e z\\ -f x + d z \\ -d y + e x\\
\end{bmatrix}
\end{align*}
If the dof is on the edge with vertices $p_0$ and $p_1$, then we know that the shape function will be exactly zero on the opposite edge (the edge connecting $p_2$ and $p_3$).
\item The curl is:
\begin{align*}
\begin{bmatrix}
\bcurl \bphi= -2d\\
-2 e\\
-2 f
\end{bmatrix}
\end{align*}
\end{itemize}
Then boundary condition on the stress becomes:
\begin{align*}
\bsigma \bn = \bg \quad \rightarrow \quad  (\bcurl \bphi ) \bn = \bg
\end{align*}
We can substitute $\bphi=\sum_i \bphi_{i,\ND} C_i$. Then:
\begin{align*}
(\bcurl \sum_i \bphi_{i,\ND} C_i) \bn = \bg  \quad \rightarrow \quad  \sum_i C_i \bcurl  \bphi_{i,\ND} \bn = \bg 
\end{align*}
The $\ND$ shape function can be defined on the element in this way. Given an edge $p_2-p_3$, we know that on the opposite edge $p_0-p_1$ the shape functions is identically zero. We can define the shape function:
\begin{align*}
\bphi_{\ND}= \pm C \left[(\bbx -\bp_0) \wedge (\bbx -\bp_1)\right]
\end{align*}
where the $\pm$ depends on the direction of the tangent vector $\bp_2-\bp_3$ and the constant $C$ must be such that the integral of the shape-function tested against the tangent vector is one.\\
Observe that each point $\by$ on the edge $\bp_2-\bp_3$ can be written as $\by=\bbx+ (\by-\bbx)$. Then let $\ba$ and $\bb$ be the vertices of the opposite edge. We want to show that the projection on the edge $\bp_2-\bp_3$ of the vector shape function evaluated on each point $\by$ is constant.
\begin{align*}
(\bbx -\ba) \times (\bbx -\bb) \cdot (\by-\bbx)
=(\by-\ba) \times ( \by -\bb) \cdot (\by -\bbx)
\end{align*}
We see that:
\begin{align*}
&(\by-\ba) \times ( \by -\bb) \cdot (\by -\bbx)=\\
&(\bbx-\ba+\by-\bbx) \times ( \by -\bb) \cdot (\by -\bbx)=\\
&(\bbx-\ba) \times ( \by -\bb) \cdot (\by -\bbx) +(\by-\bbx) \times ( \by -\bb) \cdot (\by -\bbx)=\\
&(\bbx-\ba) \times ( \bbx -\bb + \by -\bbx) \cdot (\by -\bbx) +(\by-\bbx) \times ( \by -\bb) \cdot (\by -\bbx)=\\
&(\bbx-\ba) \times ( \bbx -\bb )  \cdot (\by -\bbx)  + (\bbx-\ba) \times (\by -\bbx) \cdot (\by -\bbx) +(\by-\bbx) \times ( \by -\bb) \cdot (\by -\bbx)
\end{align*}
We now use the two following properties:
\begin{align*}
&\ba \times \ba =\textbf{0} \\
&(\ba \times \bb) \cdot \bc =
(\bb \times \bc) \cdot \ba=
 (\bc \times \ba) \cdot \bb
\end{align*}
and see that the results hold. This means that the shape functionis:
\begin{align*}
\begin{cases}
\bphi_{\ND, \bp_2-\bp_3}= C (\bbx -\bp_0) \wedge (\bbx -\bp_1)\\
C= \dfrac{1}{(\bp_2-\bp_0)\times (\bp_2-\bp_1)\cdot (\bp_3-\bp_2)}
\end{cases}
\end{align*}
The direction of the edge is in the term $\bp_3-\bp_2$. Inverting $bp_0$ and $\bp_1$ does not change the formula, since numerator and denominator would contemporarily change their sign.\\
Now we compute the formula for the shape function.
\begin{align*}
C (\bbx-\bp)\times(\bbx-\bq)=&C\begin{bmatrix}
&\textbf{i} &\textbf{j} &\textbf{k} \\
&x-p_1 &y-p_2 &z-p_3 \\
&x-q_1 &y-q_2 &z-q_3 \\
\end{bmatrix}=\\
&C\begin{bmatrix}
&(y-p_2)(z-q_3)-(z-p_3)(y-q_2)\\
&(z-p_3)(x-q_1)-(z-q_3)(x-p_1)\\
&(x-p_1)(y-q_2)-(y-p_2)(x-q_1)\\
\end{bmatrix}=\\
&C\begin{bmatrix}
&p_2 q_3 -p_2 z -q_3 y -p_3 q_2 +p_3 y + q_2 z\\
&-q_1 z -p_3x +q_1 p_3 +z p_1 +x q_3 -p_1 q_3\\
&-x q_2 - p_1 y + p_1 q_2 + y q_1 + x p_2 - p_2 q_1\\
\end{bmatrix}=\\
&C\begin{bmatrix}
&p_2 q_3 -p_3 q_2 + y (p_3 -q_3) + z(q_2 -p_2)\\
&p_3 q_1  -p_1 q_3 + x (q_3-p_3) + z(p_1-q_1)  \\
& p_1 q_2  - p_2 q_1 +x (p_2-q_2) + y (q_1-p_1) \\
\end{bmatrix}
\end{align*}
The $\bcurl$ of $\textbf{F}=C (\bbx-\bp)\times(\bbx-\bq)$:
\begin{align*}
\bcurl \textbf{F}=
\begin{bmatrix}
\dfrac{\partial F_z}{\partial y}-\dfrac{\partial F_y}{\partial z}\\\\
\dfrac{\partial F_x}{\partial z}-\dfrac{\partial F_z}{\partial x}\\\\
\dfrac{\partial F_y}{\partial x}-\dfrac{\partial F_x}{\partial y}
\end{bmatrix}=C
\begin{bmatrix}
(q_1-p_1) - (p_1-q_1)\\\\
(q_2-p_2)-(p_2-q_2)\\\\
(q_3-p_3)-(p_3-q_3)
\end{bmatrix}
=
\begin{bmatrix}
2C(q_1-p_1) \\\\
2C(q_2-p_2)\\\\
2C(q_3-p_3)
\end{bmatrix}
\end{align*}
When we apply the condition $\bsigma \bn = \bg \quad \rightarrow \quad  (\bcurl \bphi ) \bn = \bg$, the shape functions that do not contribute are the ones that:
\begin{itemize}
\item have zero support on that tetrahedron;
\item are such that their opposite edge $\bp-\bq$ is orthogonal to the normal.
\end{itemize}
Let $\bp_1$, $\bp_2$, $\bp_3$ the vertices of the face. Then by construction:
\begin{itemize}
\item the curl of the shape functions related to the edges that contain $\bp_0$ do not contribute, because the opposite edge necessarily is one of the edge of the face and so it is orthogonal to the normal.
\item the curl of shape functions related to the edges of the face is different from zero. The only way that this is different from zero is that the tetrahedron is degenerate, so that it is actually a triangle.
\item The normals are defined on each face. Therefore the boundary equations that must be added to the system are:
\begin{align*}
\sum_{\text{e}=1}^3 C_{e}^d 2C (q_e-p_e)_i n_i = g_d \qquad d=1,2,3
\end{align*}
where $(q_e-p_e)_i$, $n_i$ are the $i-th$ component of $\bq-\bp$ (opposite edge associated to edge $e$) and $\bn$, $d$ is the $d-th$ component of the force. In order to find the coefficient of the potential of the stress tensor. We have 3 equations with 9 unknowns, so we cannot solve directly for this boundary conditions. They go into the system.\\
On the other hand this is clear from this relation:
\begin{align*}
\RT_i= \sum_{j=1}^3 \int_{e_j} \ND_j \phi_{\ND,j} \cdot \bt ds =\sum_{j=1}^3 \pm  \ND_j =\ND_a+\ND_b -\ND_c \\ 
\end{align*}
We know that if the residual is zero, $\RT_i=0$. Then we also know that $\phi$ is a potential, so in a point we can fix it as $\phi(\bp)=0$. In 2D this is sufficient to know that, at least on one neumann boundary, $\phi=0$. But in 3D we have too many degrees of freedom. We cannot solve locally for the boundary conditions. 
\item \textbf{PENSO CHE QUEL CHE HO SCRITTO SOPRA SIA FALSO}. We cannot consider the normals of the faces, otherwise we would have an inconsistency. This would mean that the dof is on the face, but now we have dofs only on edges. Therefore the normal considered is on the edge and is the average between the two adjacent normals: $\bn_{e}=\bn_{F_1}+\bn_{F_2}$. Then we want to enforce:
\begin{align*}
\bcurl \bphi (\bn_1 +\bn_2)=0  \quad \iff \quad \bcurl \bphi \bn_1 =- \bcurl \bphi \bn_2
\end{align*}
The above expression involves 5 dofs. Therefore for each bc equation on an edge, we have 5 unknowns.
\end{itemize}
\textbf{NEDELEC projection operator}:
\begin{align*}
\Pi_{C}^{F,\ND_1}: \ND_{1,C} \to \ND_{1,F}: \qquad 
\int_{e_i} (\Pi_C^{F,\ND_1} \bv - \bv) \cdot \bt_{e_i}  d s =0\qquad \forall e_i \in \mathcal{E}_{h}
\end{align*}
By substituing
$\bv =\sum_{k=1}^{N_e} \phi_{k}^{\ND_{1},F} v_{k,e}$ and $\Pi_C^e \bv =\sum_{j=1}^{N_C} \phi_{j}^{\ND_{1},C} v_{j,C}$:
\begin{align*}
\sum_{k=1}^{N_F} v_{k,F} \int_{e_i}\phi_{k}^{\RT_{0,F}}  \cdot \bt_{e_i} d s =
\sum_{j=1}^{N_C} v_{j,C} \int_{e_i}\phi_{j}^{\RT_{0,C}}  \cdot \bt_{e_i}  ds        \qquad \forall F_i \in \mathcal{E}_{h}
\end{align*}
Because $ \delta_{ki} = \int_{F_i}\phi_{k}^{\ND_{1,F}}  \cdot \bt_{e_i} d s$:
\begin{align*}
\bv_{F} =\Pi_C^F \bv_{C}\qquad
\Pi_{C,ij}^F =   \int_{e_i}\phi_{j}^{\ND_{0,C}}  \cdot \bt_{e_i}  ds      
\end{align*}
 \subsection{Questions}
 \begin{enumerate}
 \item Why Gauss-Seidel smooths the high frequencies components? I understand that high eigenvalues are associated to high gradients. But I do not get why, in general, Gauss-Seidel level down quickly eigenfunctions related to high eigenvalues.
 \item The Hdiv MG acts upon RT and Nedelec element. Actually on curl(H(curl)) that is a subspace of Hdiv. But in the discrete setting it would be curl(Nedelec) that is not a subspace of RT. Simply the dofs are on the edges in the NEdelec case, on the face for the RT. So can I define the operator T:Nedelec$\to$Raviart with some sort of L2 projction? $\int u_{ND} \phi_{RT}=\int u_{RT} \phi_{RT|}$. Indeed it is not a subspace...
 \item In a LSFEM approach I have both Hdiv and H1, so do I have to smooth the stress and the displacemenet in a different manner? One with Hdiv multigrid, the other with classic multigrid...?
 \item I have to solve for the displacement and check the constraints on the boundary, then i solve for the stress. Do i have to check the constraints only on the finer level? Furthermore the hybrid Smoothing is such that I "project" the residual of $\mathcal{N}^{\perp}(\text{div},\Omega)$ onto $\mathcal{N}(\text{div})=\textbf{curl}H(\textbf{curl},\Omega)$. So it is a sort of projection, but on the same fine mesh. \\
 If this smoothing acts only on the stress component, I do not have to check for the displacement. But it is not so clear to me.
 \end{enumerate}
 \section{Saddle Point problem}
 Let be $V$ and $Q$ two Hilbert spaces with the norms $|| \cdot ||_V$ and $|| \cdot ||_Q$.\\
 Let $a(u,v)$  be a continuous bilinear form on $V \times V$, not necessarily symmetric, that defines a linear continuous operator $A:V->V'$ by:
 \begin{align*}
 \langle A u, v \rangle_{V' \times V} = a(u,v) \qquad \forall v \in V, \: \forall u \in V
 \end{align*}
 Let $b(v,q)$ be a continuous bilinear form on $V \times Q$, that defines two linear operators $B:V \to Q'$ and $B^t: Q \to V'$:
 \begin{align*}
  \langle B v, q \rangle_{Q' \times Q} =  \langle v, B^t q \rangle_{V \times V'}=b(v,q) \qquad \forall q \in V
 \end{align*}
 Let $f \in V'$, $g \in Q'$. We want to find $u \in V$, $p \in Q$ solutions of:
 \begin{align*}
 \begin{cases}
 & a(u,v)+b(v,p)=\langle f, v \rangle_{V' \times V}  \qquad \forall v \in V \\
 & b(u,q)=\langle g, q \rangle_{q \times Q'} \qquad \forall q \in Q
 \end{cases}
 \end{align*}
 \textbf{Theorem}\\
 Let $g \in Im(B)$, $a(\cdot,\cdot)$ be coercive on $Ker(B)$:
 \begin{align*}
 a(v_0,v_0) \geq \alpha_0 ||v_0||_V^2 \qquad \forall v_0 \in Ker(B)
\end{align*}  
Then there exists a unique $u \in V$ solution of:
\begin{align*}
a(u,v_o)&=\langle f, v_0 \rangle_{V' \times V} \qquad \forall v_0 \in Ker(B)	\\
Bu&=g
\end{align*}
\textbf{Proof}\\
Since $g \in Im(B)$, one can find a corresponding $u_g$ such that $B  u_g =g$. Then, by setting $u=u_0+u_g$ and taking $v=v_0 \in Ker(B)$:
\begin{align*}
a(u_0,v_0)=-a(u_g,v_0)+\langle f, v_0 \rangle \qquad \forall v_0 \in Ker(B)
\end{align*}
Therefore, for Lax-Milgram, a sufficient condition for the existence of $u_0$ is the coercivity of$a(\cdot,\cdot)$ on $Ker(B)$. \\
We have to show that $u=u_0+u_g$ does not depend on the choice of $u_g$. So if $u_2$ satisfies $B u_2=g$ and $u_1$ satisfies $A u_1=f$, we have $u_1-u_2 in Ker(B)$ and $a(u_1-u_2,v_0)=0 \forall v_0 \in Ker(B)$. So $u_1=u_2$.\\\\
So if the system has a solution $(u,p)$, then for this theorem $u$ exists and is unique. Moreover:
\begin{align*}
||u|| \leq ||u_g|| + \frac{1}{\alpha_0} \left[ ||f||_{V'}+||a|| ||u_g|| \right]
\end{align*} 


PETSc SNES example:
\begin{equation}
  F\genfrac{(}{)}{0pt}{}{x_0}{x_1} = \genfrac{(}{)}{0pt}{}{xË2_0 + x_0 x_1
- 3}{x_0 x_1 + xË2_1 - 6}
\end{equation}
\begin{equation}
  F\genfrac{(}{)}{0pt}{}{x_0}{x_1} = \genfrac{(}{)}{0pt}{}{\sin(3 x_0) + x_0}{x_1}
\end{equation}
\section{Subdifferential}
In finite dimension
\begin{align*}
\partial f (x) = \text{conv} \{\psi:\quad f' (x_i) \to \psi, \quad x_i \to x \}
\end{align*}
If the functional f is convex:
\begin{align*}
\partial (f+g) = \partial f + \partial g\\
\partial (f-g) \subset \partial f + \partial g\\
\end{align*}
for the second case use $f=g$.







\subsection{Nodal Auxiliary Space Preconditioning}
\begin{itemize}
\item Find $u \in V$ such that:
\begin{align*}
a(u,v)=f(v) \quad \forall v \in V
\end{align*}
\item  We define a fictitious space $\bar{V}$ with inner product $\bar{a}(\cdot,\cdot)$ that induces the respective norm.
\item  A linear continuous and surjective transfer operator $\Pi:\bar{V} \to V$ such that:
\begin{align*}
B= \Pi \:\bar{A}^{-1} \Pi^* : V' \to V
\end{align*}
\end{itemize}
\textbf{Fictitious Space Lemma}\\
Assume $\Pi$ is surjective. Then:
\begin{enumerate}
\item  \begin{align*}
\exists c_0 >0: \forall v \in V:		\quad \exists \bar{v} \in \bar{V}: \qquad v = \Pi \bar{v} \:\wedge \: ||\bar{v}||_{\bar{A}} \leq c_0 ||v||_A =c_0 ||\Pi \bar{v}||_A	
\end{align*}
\item \begin{align*}
\exists c_1 >0: 	 ||\Pi \bar{v}||_A	 \leq c_1 || \bar{v}||_{\bar{A}} \qquad \forall \bar{v} \in \bar{V}
\end{align*}
\end{enumerate}
Then:\\
\begin{align*}
c_0^{-2} ||v||_A^2 \leq a(BAv,v) \leq C_1^2 ||v||_A^2
\end{align*}
\textit{Proof}\\
Look at the paper. That is clear.\\\\
We can also see that:
\begin{align*}
k(B A) = \dfrac{\lambda_{max}(BA)}{\lambda_{min}(BA)}\leq (c_0 c_1)^2
\end{align*}
So we want $c_0$ and $c_1$ indipendent on $h$. \textbf{RIGHT}?
\textbf{Definition of the auxiliary spaces}\\
We define:
\begin{align*}
\bar{V}= V \times W_1 \times \cdot \cdot \cdot \times W_j
\end{align*}
with the inner products:
\begin{itemize}
\item $s(u,v): V \times V : \to \mathbb{R} $ is the smoother.
\item $\bar{a}(u,v): W_j \times W_j : \to \mathbb{R}$.
\end{itemize}
Then the preconditioner is:
\begin{align*}
B= S^{-1} + \sum_{j=1}^J \Pi_j \bar{A_j}^{-1} \Pi_j^*
\end{align*}
and we have just to proof that:
\begin{itemize}
\item Find bound for the transfer operator:
\begin{align}
||\Pi_j w_j||_A \leq c_{1,j} \bar{a}(w_j,w_j)^{1/2} \qquad w_j \in W_j 
\label{ c1_inequality}
\end{align} 
\item Investigate continuity of $S^{-1}$:
\begin{align}
\exists c_s >0: \qquad ||v||_A \leq c_s s(v,v)^{-1/2} \qquad \forall v \in V
\label{ cs_inequality}
\end{align}
\item $\forall v\in V$, we can write $v=v_0 +\sum_{j=2}^J \Pi_j w_j$ and:
\begin{equation}
\label{ c0_inequality}
s(v_0,v_0)+\sum_{j=1}^J \bar{a}_j(w_j,w_j) \leq c_0^2 ||v||_A^2
\end{equation}
\end{itemize}
Then the inequality on the condition number becomes:
\begin{align*}
k(BA)\leq c_0^2(c_s^2+ c_1^2 + \cdot \cdot \cdot + c_j^2)
\end{align*}
\textbf{Stable Lemma decomposition}\\
\begin{align*}
\exists R \in L(H((D,\Omega),H^1(\Omega)), \qquad
\exists Z \in L(H((D,\Omega),H(D^{-},\Omega)), \qquad
\exists C=C(\Omega) >0
\end{align*}
Then:
\begin{align*}
\begin{cases}
 R + D^{-} Z =I  \qquad \qquad \text{on} \quad H(D,\Omega)\\
||R v||_{H^1} \leq C ||D v||_L^2\\
||Z v ||_{H(D^{-},\Omega)}\leq C ||v||_{H(D,\Omega)}
\end{cases}
\end{align*}
We want to apply the fictitious space lemma to:
\begin{align*}
B= I  (I -\Delta)^{-1} I + D^{-} (I+ (D^{-})^{*} D^{-}) (D^{-})^{*}
\end{align*}
So that:
\begin{itemize}
\item $\bar{a}(u,v)=(u,v)+(D u, D v)$, $\Pi = D: H(D^{-},\Omega) \to H(D,\Omega) $. We want to show that:
\begin{align*}
\dfrac{1}{c_1}  ||D^{-} v ||_{H(D,\Omega)}\leq ||v||_{H(D^{-},\Omega)}\leq c_0 ||D^{-} v||_{H(D,\Omega)}
\end{align*}
We can show that $c_1=1$, since:
\begin{align*}
||D^{-} v ||_{H(D,\Omega)}=||D D^{-} v||_{L^2} +  || D^{-} v||_{L^2} = || D^{-} v||_{L^2}\leq ||v||_L^2 + ||D^{-} v||_L^2=||v||_{H(D^{-},\Omega)}
\end{align*}
For the same reason, we can now simply show that:
\begin{align*}
||v||_{H(D^{-},\Omega)}\leq c_0 || D^{-} v||_{L^2}
\end{align*}
where this inequality derives from the previous lemma, with $c_0=C(\Omega) \perp h$:
\begin{align*}
||Z \bar{v}||_{H(D^{-},\Omega)}\leq ||Z|| \:||\bar{v}||_{H_(D,\Omega)}
\end{align*}
This means that $c_1=1$ and $c_0=||Z||$.
\item $s(u,v)= (u,v)+(\nabla u,\nabla v)$, $\Pi=I: H^1(\Omega) \to H(D,\Omega)$. We want to show that:
\begin{align*}
\dfrac{1}{c_1}  ||I v ||_{H(D,\Omega)}\leq ||v||_{H^1(\Omega)}\leq c_0 || I v||_{H(D,\Omega)}
\end{align*}
For some reason that are not clear to me:
\begin{align*}
  || I v ||_{H(D,\Omega)}\leq ||v||_{H^1(\Omega)} \to c_1=1
\end{align*}
Indeed $c_1=2$. For
\begin{itemize}
\item $D=\nabla$: $c_1=1$.
\item $D=\bcurl$: $c=\sqrt{2}$:
\begin{align*}
\int \bcurl F  \cdot \bcurl F &= \int (F_{z,y}-F_{y,z})^2+
 \int (F_{z,x}-F_{y,x})^2+ \int (F_{x,y}-F_{x,z})^2 \\
 & \leq
 2 \int  \left[ F_{z,y}^2F_{y,z}^2+
F_{z,x}^2+F_{y,x}^2+ F_{x,y}^2+F_{x,z}^2 \right] \\
&\leq 2 \int \nabla F \cdot \nabla F
\end{align*}
\item $D=\text{div}$: $c=\sqrt{3}$:
\begin{align*}
\int \text{div} F  \text{div}  F &= \int (F_{x,x}+F_{y,y}+F_{z,z})^2 \\
 & = \int F_{x,x}^2+F_{y,y}^2+F_{z,z}^2 +2  F_{x,x}  F_{y,y} +2  F_{x,x}  F_{z,z} +2  F_{z,z}  F_{y,y} \\
 &\leq \int (F_{x,x}+  F_{y,y} ) ^2 +(F_{x,x} + F_{z,z} )^2+(F_{z,z} + F_{y,y})^2 - \left[ F_{x,x}^2+F_{y,y}^2+F_{z,z}^2 \right]\\
 & \leq 3 \int \nabla F \cdot \nabla F
\end{align*}
\end{itemize}
Then we could define $c_1=\sqrt{3}$. But what is really important is that is not dependent on $\Omega$ and $h$. For the other inequality:
\begin{align*}
|| R v||_{H^1(\Omega)} \leq ||R|| ||v||_{H(D,\Omega)}
\end{align*}
\end{itemize}
So globally we would have:
\begin{align*}
\lambda(BA) \leq \sqrt{3} \left(||R||^2 + ||Z||^2 \right)
\end{align*}

\textbf{Helmoltz-Decomposition}\\
We have created a preconditioner that works in the infinite-dimensional case, but we have to move to the finite one. In particular we want to use the Helmoltz decomposition:
\begin{align*}
H(D,\Omega)= H(d0,\Omega) \oplus  H(d0,\Omega)^{\perp}
\end{align*}
The $\RT$ and $\ND$ spaces satisfy the exact sequence property as well as their continuous counterpart. This means that for $\Omega$ homotopy equivalent to a ball:
\begin{align*}
V_h(D0)={v_h\in V_h(D): D v_h=0}=D^{-} V_h(D^{-})
\end{align*}
We can also define nodal interpolation operators $\Pi_h^D: H^1(\Omega) \to V_h(D)$. In general the input space could be something else,but for our purpose (NODAL AUXILIARY SPACE) it is sufficient to define it on $H^1(\Omega)$ (CREDO).
\begin{align*}
\begin{cases}
H_h^{\nabla} \bv = \sum_{n \in  \mathcal{N}_h} \bv(\bbx_{n})  \bb_n \\
H_h^{\bcurl} \bv = \sum_{e \in  \mathcal{E}_h} \left( \int_e \bv \cdot \bt \:ds \right) \bb_e \\
H_h^{\text{div}} \bv = \sum_{f \in  \mathcal{F}_h} \left( \int_f \bv \cdot \bn\: ds \right) \bb_f\\
\end{cases}
\end{align*}
The nodal auxiliary space is $\left(P^1,P^1,P^1 \right)=S_h \subset \bH^1(\Omega)$. On the domain of $\Pi_h^D$ it is true that:
\begin{align*}
D \Pi_h^D = \Pi_h^{D^+} D
\end{align*}
Furthermore since $ D S_h \subset D V_h(D)$:
\begin{align*}
D \Pi_h^D \psi_h = \Pi_h^{D^+} D \psi_h = D \psi_h \qquad \psi_h \in S_h
\end{align*}
Their discrete counterpart $\Pi_h^D: S_h \to V_h(D)$ can be defined as follows:
\begin{align*}
H_h^{\bcurl} \bv_h&= \sum_{e \in \mathcal{E}_h} \left(\int_e \bv_h \cdot \bt \right) \bb_e \\&= 
\sum_{e \in \mathcal{E}_h}\left(\int_e\sum_{n \in \mathcal{N}_h}  \bV_n   \cdot \bt \right) \bb_e \\&= 
\sum_{e \in \mathcal{E}_h} \left[\int_e  \left( \bV_{n_1}  + \bV_{n_2}  \right) \cdot \bt \right] \bb_e \\&= 
\sum_{e \in \mathcal{E}_h} E_e \bb_e 
\end{align*}
Since the tangent vector $\bt$ is constant and the vector lagrange shape functions are linear, the result of edge-coefficient becomes:
\begin{align*}
E_e = \dfrac{|e|}{2}\left( \bV_{n_1}  + \bV_{n_2}\right) \cdot \bt
\end{align*}
A similar path of reasoning can be followed for the other projection operator. This means that:
\begin{align*}
H_h^{\text{div}} \bv_h&= \sum_{f \in \mathcal{F}_h} \left(\int_f \bv_h \cdot \bn \right) \bb_f \\&= 
\sum_{f \in \mathcal{F}_h}\left(\int_f \sum_{n \in \mathcal{N}_h}  \bV_n   \cdot \bn \right) \bb_f \\&= 
\sum_{f \in \mathcal{F}_h} \left[\int_f  \left( \bV_{n_1}  + \bV_{n_2} +\bV_{n_3} \right) \cdot \bn \right] \bb_f \\&= 
\sum_{f \in \mathcal{F}_h} F_f \bb_f 
\end{align*}
\begin{align*}
F_f = \dfrac{|F|}{3}\left( \bV_{n_1}  + \bV_{n_2} + \bV_{n_3}\right) \cdot  \bn
\end{align*}
\subsection{Curl-Preconditioner}
The proposed preconditioner for the $\bcurl$-case:
\begin{align*}
\bB_{\bcurl,h}&= \bD _A^{-1}+\bP_{\bcurl} (\bL+ \tau \bM)^{-1}\bP_{\bcurl}^T+ \tau^{-1} \bG (-\boldsymbol{\Delta} )^{-1}\bG^T
\end{align*}
\begin{itemize}
\item $W_1=S_h$; $\bar{a}(\bpsi_h,\bpsi_h)=||\bpsi_h||_{H^1}^2+\tau ||\bpsi_h||_{L^2}^2$; $\Pi_1=\Pi_h^{\bcurl}$
\item $W_2=V_h(D^{-})$; $\bar{a}(p_h,p_h)=\tau |p_h|_{H^1}^2  $; $\Pi_2=D^{-}$
\end{itemize}
\begin{center}
  \begin{tabular}{ | l | l | l |}
    \hline
    $W$ & $\bar{a}(\cdot,\cdot) $& $\Pi$ \\ \hline
    $S_h$ & $ ||\bpsi_h||_{H^1}^2+\tau ||\bpsi_h||_{L^2}^2$ & $\Pi_h^{\bcurl}$ \\ \hline
    $ V_h(D^{-})$ &$\tau |p_h|_{H^1}^2 $  & $D^{-}$ \\
    \hline
  \end{tabular}
\end{center}
So we have to check that:
\begin{align*}
s(\tilde{\bv},\tilde{\bv})+( \psi_h , \psi_h)_{H^1}+\tau (\bcurl p_h ,\bcurl p_h)_{L^2}\lesssim || v_h||_A^2\\
\end{align*}
and this is implied by: 
\begin{align*}
\sum_{\bb \in \mathcal{B}(D)} ||\bv_{\bb} ||_A^2 + || \bpsi_h||_{H^1}^2+||D^{-} p_h||_A^2 \lesssim || v_h||_A^2
\end{align*}
since $||D^{-}p_h||_A=\tau ||D^{-}p_h||_{L^2}+ ||D D^{-}p_h||_{L^2}
=\tau ||D^{-}p_h||_{L^2}=\tau (\bcurl p_h ,\bcurl p_h)_{L^2}$. Therefore we have found that $c_0 \perp h$.\\
Now we have to show that the same holds also for $c_s$, $c_1$, $c_2$. In particular we have to proof that $||\Pi w||_A^2 \leq C ||w||_{\bar{A}}^2$:
\begin{itemize}
\item $||D^{-} p_h||_{A}^2=\tau || D^{-} p_h ||_{L^2}^2 + || D D^{-} p_h  ||_{L^2}^2 \leq  \tau || D^{-} p_h  ||_{L^2}^2 +  ||  p_h  ||_{L^2}^2= || p_h  ||_{\bar{A}}^2 $ (so $c_0=1$)
\item $||\Pi^D \bpsi||_A^2=\tau ||\Pi^D \bpsi||_{L^2}^2 + ||\nabla\Pi^D \bpsi||_{L^2}^2  \lesssim || \nabla \bpsi ||_{L^2}^2+|| \bpsi ||_{L^2}^2= || \bpsi ||_{H^1(\Omega)}^2$ (look at 4.11 of the paper)
\item For the first term the inequality $||\bv_h||^2\leq c_s s(\bv_,\bv_h)$ follows from 7.2. Note that in the continuum Jacobi smoother means to make the scalar product on each basis.
\end{itemize}
\subsection{Div-Preconditioner}
We have:
\begin{align*}
\bv_h=\sum_{\bp \in \mathcal{B}(\text{div})} \bv_{\bb} + \Pi_h^{\text{div}} \bpsi_h + \bcurl p_h=
\sum_{\bp \in \mathcal{B}(\text{div})}\bv_{\bb}+ 
\Pi_h^{\text{div}} \bpsi_h + 
\sum_{\bq \in \mathcal{B}  (\bcurl )  } \bcurl p_{\bq} + \bcurl \bPhi_h
\end{align*}
Therefore the preconditioner is:
\begin{align*}
\bB_{\text{div}} =\bD_A^{-1} + \bP_{\text{div}} (\bL + \tau \bM)^{-1} \bP_{\text{div}}^T +\bC \bD_{\bcurl}^{-1}\bC^T+\tau^{-1} \bC \bP_{\bcurl}  (\bL + \tau \bM)^{-1} \bP_{\bcurl}^T \bC^T
\end{align*}
\section{COARSE SPACE FOR CONTACT BOUNDARY}

In case an edge or a node satisfy the constraints with equality, then no correction needs to be added to such degrees of freedom. We can distinguish two cases, for $P_1$ and $\RT_0$ functions. 
Also in this case, we do not need to invert a mass matrix. \\\\
In 2D both curl-free and divergence-free fields can be described respectively by gradient potential and their rotation:


vefevefvefeffefeevefef
\end{document}  