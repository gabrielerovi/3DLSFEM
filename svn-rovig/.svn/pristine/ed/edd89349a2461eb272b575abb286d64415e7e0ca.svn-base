\documentclass[]{usiinfprospectus}
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{booktabs}
\usepackage {tikz}
\usetikzlibrary {positioning}
\definecolor {processblue}{cmyk}{0.96,0,0,0}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{amsthm}
\usepackage{amsmath}
\captionsetup{labelfont={bf}}

\author{Gabriele Rovi}

\title{Stress-based Methods for variational inequalities in solid mechanics: }
\subtitle{FE discretization and solution by hierarchical optimization}
\versiondate{\today}

\begin{committee}
%With more than 1 advisor an error is raised...: only 1 advisor is allowed!
\researchadvisor[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Rolf}{Krause}
\academicadvisor[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Rolf}{Krause}
\committeemember[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{}{Luca Gambardella}
\committeemember[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{}{Silvia Santini}
%\committeemember[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Committee}{Member3}
%\coadvisor[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Research}{Co-Advisor1}
%\coadvisor[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Research}{Co-Advisor2}
%\coadvisor[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Research}{Co-Advisor3}
\phddirector{Prof.}{Binder}{/Bronstein}
\end{committee}

\abstract {
In many engineering applications, some of the most important quantities of interest are the forces. In particular the forces on a solid structure can be generated by the contact with another body. Possible examples of this phenomenon can be the contact of: a foundation with the soil; a biomedical prothesis with a portion of the human body;  the aortic valves, during their closure; and so on. \\
The aim of this project is modelling contact problems in solid mechanics so that a good approximation of the stresses (strictly related to the forces) is attained. For such a purpose, it is exploited  the huge potential of finite element approaches based on the approximation of stresses as the dual variable for variational inequalities. Approximating stresses directly by suitable finite element spaces allows for algorithmic simplifications and improved accuracy, in particular, in association with plasticity and friction where stress components play a prominent role. \\
Here the Least Square (LS) Finite Element Method (FEM) applied to contact linear elasticity is examined. Such approach is able to compute good approximation of stresses and deal with incompressible solids. The solution space is given by the pairs $H^1 \times H_{div}$ to which displacements and stresses respectively belong. Since the related system is ill-conditioned, fast solvers as multigrid are needed. Actually, at the state of the art, no fast solver for LS linear elasticity, much less for contact problems, has been developed. Such an extension is actually the goal of the present work.
%This includes models of frictional contact as well as elasto-plastic deformations and emphasizes adaptive refinement strategies based on built-in a posteriori error estimators as well as efficient iterative solution methods for the arising non-smooth problems. 
}

\newcommand{\supp}{\operatorname{supp}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\divr}{\operatorname{div}}
\newcommand{\tdiv}{\operatorname{div}}
\newcommand{\ess}{\operatorname{ess}}
\newcommand{\rotore}{\operatorname{rot}}
\newcommand{\curl}{\operatorname{\textbf{curl}}}
\newcommand{\bcurl}{\operatorname{\textbf{curl}}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\bA}{\textbf{A}}
\newcommand{\ba}{\textbf{a}}
\newcommand{\bb}{\textbf{b}}
\newcommand{\bB}{\textbf{B}}
\newcommand{\bc}{\textbf{c}}
\newcommand{\bC}{\textbf{C}}
\newcommand{\bd}{\textbf{d}}
\newcommand{\bD}{\textbf{D}}
\newcommand{\be}{\textbf{e}}
\newcommand{\bE}{\textbf{E}}
\newcommand{\bff}{\textbf{f}}
\newcommand{\bF}{\textbf{F}}
\newcommand{\bg}{\textbf{g}}
\newcommand{\bG}{\textbf{G}}
\newcommand{\bi}{\textbf{i}}
\newcommand{\bI}{\textbf{I}}
\newcommand{\bj}{\textbf{j}}
\newcommand{\bJ}{\textbf{J}}
\newcommand{\bh}{\textbf{h}}
\newcommand{\bH}{\textbf{H}}
\newcommand{\bk}{\textbf{k}}
\newcommand{\bK}{\textbf{K}}
\newcommand{\bl}{\textbf{l}}
\newcommand{\bL}{\textbf{L}}
\newcommand{\bm}{\textbf{m}}
\newcommand{\bM}{\textbf{M}}
\newcommand{\bn}{\textbf{n}}
\newcommand{\bN}{\textbf{N}}
\newcommand{\bp}{\textbf{p}}
\newcommand{\bP}{\textbf{P}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\bPi}{\boldsymbol{\Pi}}
\newcommand{\bo}{\textbf{o}}
\newcommand{\bq}{\textbf{q}}
\newcommand{\bQ}{\textbf{Q}}
\newcommand{\br}{\textbf{r}}
\newcommand{\bR}{\textbf{R}}
\newcommand{\bs}{\textbf{s}}
\newcommand{\bS}{\textbf{S}}
\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\bt}{\textbf{t}}
\newcommand{\bv}{\textbf{v}}
\newcommand{\bV}{\textbf{V}}
\newcommand{\bw}{\textbf{w}}
\newcommand{\bW}{\textbf{W}}
\newcommand{\bu}{\textbf{u}}
\newcommand{\bU}{\textbf{U}}
\newcommand{\by}{\textbf{y}}
\newcommand{\bY}{\textbf{Y}}
\newcommand{\bbx}{\textbf{x}}
\newcommand{\bX}{\textbf{X}}
\newcommand{\bz}{\textbf{z}}
\newcommand{\bZ}{\textbf{Z}}
\newcommand{\beps}{\boldsymbol{\varepsilon}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\aaa}{\`a}
\newcommand{\eee}{\`e}
\newcommand{\iii}{\`i}
\newcommand{\ooo}{\`o}
\newcommand{\uuu}{\`u}
\newcommand{\aaaa}{\'a}
\newcommand{\eeee}{\'e}
\newcommand{\iiii}{\'i}
\newcommand{\oooo}{\'o}
\newcommand{\uuuu}{\'u}
\newcommand{\AAA}{\`A}
\newcommand{\EEE}{\`E}
\newcommand{\III}{\`I}
\newcommand{\OOO}{\`O}
\newcommand{\UUU}{\`U}
\newcommand{\AAAA}{\'A}
\newcommand{\EEEE}{\'E}
\newcommand{\IIII}{\'I}
\newcommand{\OOOO}{\'O}
\newcommand{\UUUU}{\'U}
\newcommand{\ND}{\mathcal{ND}}
\newcommand{\RT}{\mathcal{RT}}

%% argmin argmax
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


%%% theorems
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\renewcommand\qedsymbol{$\blacksquare$}

%%% norm and abs
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\abs[1]{\left\vert#1\right\vert}


\begin{document}
\maketitle
\section{Definition of the problem }
 Let $\Omega$ be an open, bounded, connected subset of $\mathbb{R}^d$, where $d=2,3$ is the dimension of the problem. The boundary $\partial \Omega$, Lipschitz and continuous, is the union of two open disjoint subsets $\partial \Omega= \Gamma_D \cup \Gamma_N$, with  $\Gamma_D \neq \emptyset$ and  $\Gamma_D \cap\Gamma_N = \emptyset$. Then let $\bff=(f_1,...,f_d)^T$ be the body force, $\bu=(u_1,...,u_d)^T$ the displacement field,  $\bsigma=(\sigma_{ij})_{d \times d}$ the stress tensor. The \textit{strong formulation of linear elasticity} is the following:\textit{ find $\bu$, $\bsigma$ such that}:
\begin{align*}
\begin{cases}
\text{div} \bsigma + \bff=0 & \Omega  \qquad \text{momentum balance equation}\\
\mathcal{A} \bsigma - \boldsymbol{\varepsilon}(\bu)=0 &\Omega \qquad \text{constitutive law}\\
\bu = \bu^D & \Gamma_D\qquad \text{Dirichlet BC}\\
\bsigma  \bn = \bt^N & \Gamma_N\qquad \text{Neumann BC}\\
\end{cases} 
\end{align*}
where the linearized strain tensor $\boldsymbol{\varepsilon}(\bu)= \text{sym} (\nabla \bu)$ is the symmetric part of the displacement gradient, the compliance tensor is $\mathcal{A}=\dfrac{1}{2 \mu} \left(\bsigma-\dfrac{\lambda}{d \lambda + 2 \mu } \text{tr} \bsigma \bI\right)$ with $\text{tr}$, $d$, $\lambda$ and $\mu$ denoting respectively the trace operator, the dimension of the problem and the Lam\eeee${}$ parameters.\\
Now let $\Gamma_C$ be the contact boundary such that $ \partial \Omega=\Gamma_C \cup  \Gamma_D \cup  \Gamma_N$, $\Gamma_i \cap  \Gamma_j =\emptyset$ for $i,j=D,N,C, i \neq j $, and $\Gamma_D \neq \emptyset$. Then, by adding the following constraints:
\begin{align*}
\begin{cases}
\bu \cdot \bn_o - g  \leq 0 & \Gamma_C \qquad \text{impenetrability}\\
(\bsigma \bn) \cdot \bn_o \leq 0 &\Gamma_C \qquad \text{direction of the surface pressure}\\
 \left(\bu \cdot \bn_o -g \right) \left( (\bsigma \bn) \cdot \bn_o \right) =0 & \Gamma_C \qquad \text{complementarity condition}
  \\
  (\bsigma \bn) \cdot \bt_o \leq 0 &\Gamma_C \qquad  \text{frictionless condition}
\end{cases}
\end{align*}
the strong formulation of contact for linear elasticity is finally obtained. Here $\bn$ represents the outward normal of the body, while $\bn_o$ and $\bt_o$ respectively represent the normal and the tangent vectors of the obstacle. The gap function $g$ is instead the distance in the normal direction between the obstacle and the body. Here the first condition means that no penetration can occur between the body and the obstacle. The second condition implies that, whenever contact forces arise, they have to be of compression and no adhesion is permitted. The third condition is a classic complementarity condition of the first two. 
The last one states that only normal stresses can arise.
Finally, by confusing the normal and the tangent vectors of the obstacle with the ones of the body, i.e. $\bn \approx\bn_o$ and $\bt \approx \bt_o$, the linearized contact formulation for linear elasticity is recovered. \\
In general, from the strong formulation of the contact linear elasticity, different variants of weak forms can be derived. In the following, a list of motivations that retraces the one in \cite{CS04}, is presented. By substituing the constitutive equation ($\bsigma = \mathcal{C}\boldsymbol{\varepsilon}(\bu)$, with $\mathcal{C}=\mathcal{A}^{-1}$ the elasticity tensor) into the momentum balance one, the displacement formulation is consequently obtained. The displacements $\bu$, belonging to $ H^1(\Omega)$, are the only unknowns and the stresses $\bsigma$, belonging only to $L^2(\Omega)$, are derived \textit{a posteriori} and cannot be carefully approximated. Furthermore locking phenomena can arise for incompressible or nearly incompressible solids ($\lambda \gg 1$ or $\lambda \to \infty$).\\
To achieve a better approximation of the stresses, the dual mixed formulation by Hellinger-Reissner can be used (\cite{BF12}). Given the energy-functional $\mathcal{J}(\bu,\bsigma)=\frac{1}{2}\left(\mathcal{A} \bsigma, \bsigma \right) + \left(\nabla \cdot \bsigma + \bff, \bu \right)  $, both displacements and stresses $(\bu,\bsigma)$ are unknowns of the problem, respectively belonging to $  L^2(\Omega)^d  \times  H_{\text{div},S}(\Omega)^d$ , where $H_{\text{div},S}(\Omega)^d$ is the space of symmetric tensors in $H_{\text{div}}(\Omega)$. In this case, in order to satisfy the inf-sup condition in the discrete setting, a stable combination of finite element spaces is needed. Although such spaces have been built (\cite{CS04}), the number of degrees of freedom they require is very large. Furthermore the corresponding linear system is a saddle point problem, that in general is difficult to solve.\\
The approach that is here presented is based on the LS principle (\cite{Boc09}, \cite{BMM97}, \cite{YL97}). The main idea behind it is to build a fictitious functional as the weighted sum of the squared $L^2$-norms of the residual equations. Unlike the previous cases, now it is required more regularity on both variables: $\bu \in H^1(\Omega)$,$\bsigma \in H(\tdiv,\Omega)$. With respect to the dual mixed formulation, the symmetry of the stress tensor is not needed. Indeed, as it is shown in \cite{CS04}, $\norm{
\bsigma-\bsigma^T}\leq C \norm{\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu)}$. Therefore, by reducing the residual of the constitutive law, the asymmetry is reduced as well. Altough in the discrete setting exact 
The full symmetry can be recovered only in the continuum setting, where the exact solution 
In particular, spaces that would be useful for the analysis are the following: 
\begin{align*}
&H_D^1(\Omega)=\left\lbrace \bv \in \left[ H^1(\Omega)\right]^d, \quad \bv|_{\Gamma_D}=\bu^D   \:\: \text{on} \: \Gamma_D \right\rbrace &&
H_{D,0}^1(\Omega)=\left\lbrace \bv \in \left[ H^1(\Omega)\right]^d, \quad \bv|_{\Gamma_D}=\textbf{0}    \:\:  \text{on} \: \Gamma_D \right\rbrace \\
&H_N(\tdiv,\Omega)=\left\lbrace \btau \in \left[ H(,\tdiv, \Omega)\right]^d, \quad \btau \bn |_{\Gamma_N}=\bt^N   \:\:  \text{on} \: \Gamma_N
\right\rbrace &&
H_{N,0}(\tdiv,\Omega)=\left\lbrace \btau \in \left[ H(,\tdiv, \Omega)\right]^d, \quad \btau \bn |_{\Gamma_N}=\textbf{0}
 \:\: \text{on} \: \Gamma_N \right\rbrace 
\end{align*}
Then, relying on the formulation given in \cite{KMS17}, we define the linear elasticity LS functional $\mathcal{F}$ and the corresponding augmented LS functional $\mathcal{J}$ for contact:
\begin{align*}
&\mathcal{F}(\bu,\bsigma;\bff)=C_{eq} ||\text{div} \bsigma+\bff||_{L^2(\Omega)^d}^2+C_{const}||\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu)||_{L^2(\Omega)^d}^2 \\
&
\mathcal{J}(\bu,\bsigma;\bff,g)=\mathcal{F}(\bu,\bsigma;\bff)+C_{compl} \langle \bu \cdot \bn -g, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_c}\\
\end{align*}
So that the problem can be formulated in this way: find $(\bu,\bsigma)$ such that
 \begin{align}
 &(\bu,\bsigma)=\argmin \limits_{(\bu,\bsigma) \in K}\mathcal{J}(\bu,\bsigma;\bff,g)\\
 &K=
 \left\lbrace 
  \left(\bu,  \bsigma \right)  \in  
 H_D^1(\Omega)\times H_N(\tdiv,\Omega)  : \quad 
 \bu \cdot \bn - g  \leq 0, \:\:  (\bsigma \bn) \cdot \bn \leq 0, \:\: (\bsigma \bn) \cdot \bt =\textbf{0}  \:\: \text{on}  \: \: \Gamma_C
\right\rbrace 
\label{MinimizationProblem}
\end{align}
The complementarity condition on $\Gamma_C$ is a non-linear term. Defining the convex set $K$ by adding also this requirement would be cumbersome, at least from a computational perspective. On the other hand, augmenting the functional with this term seems a more natural choice. \\
The augmented functional is Gateaux-differentiable and strongly convex, as we will show. Therefore the problem (\ref{MinimizationProblem}) can be reformulated in the following way:
\begin{align*}
\begin{cases}
&
\left\langle \dfrac{\partial \mathcal{J}(\bu,\bsigma;\bff,g)}{\partial \bu }, \bv-\bu \right\rangle =
-2 \left(\mathcal{\bsigma}-\boldsymbol{\varepsilon}(\bu), \boldsymbol{\varepsilon}(\bv-\bu) \right)+\langle \bn \cdot \left( \bsigma \bn\right), \bn \cdot (\bv -\bu)\rangle_{\Gamma_C}
\geq 0 \\\\
&
\left\langle \dfrac{\partial \mathcal{J}(\bu,\bsigma;\bff,g)}{\partial \bsigma }, \btau-\bsigma \right\rangle =
2\left(\text{div}\bsigma +\bff, \tdiv (\btau-\bsigma) \right)
+ 2 \left( 
\mathcal{\bsigma}-\boldsymbol{\varepsilon}(\bu),\mathcal{\btau-\bsigma}
\right) +\langle \bn \cdot \bu-g, \bn \cdot 
\left(\btau- \bsigma \right)\bn \rangle_{\Gamma_C}
\geq 0 
\end{cases}
\qquad  \forall (\bv,\btau) \in K
\end{align*}
\section{Existence and uniqueness of the solution}
Now we want to prove that the minimization problem has a unique solution $(\bu,\bsigma)$. To this aim,  let us define the following norm $M(\bs,\bw): H_D^1(\Omega) \times H_N(\tdiv,\Omega) \to \mathbb{R}$:
\begin{align*}
&M(\bs,\bw)= \|  \boldsymbol{\varepsilon}(\bw) \|_{L^2} +  \| \btau \|_{H_{div}}^2 =  \|  \boldsymbol{\varepsilon}(\bw) \|_{L^2}^2 +  \| \bs \|_{L^2}^2 +  \| \tdiv \bs \|_{L^2}^2 
\end{align*}
Then we can prove:
\begin{lemma}
$\forall \bu, \bv \in H_D^1(\Omega)$, $\forall \bsigma, \btau \in H_N(\tdiv,\Omega)$, let $\bw=\bu-\bv$ and $\bs=\bsigma-\btau$. Then exist $C_1$ and $C_2$ such that:
\begin{align*}
M(\bs,\bw) C_1 \leq \mathcal{J}(\bw,\bs;0,0) \leq C_2 M(\bs,\bw)
\end{align*}
where:
\begin{align}
C_1= \left(\text{max} \left(4,   \dfrac{  1 + 4 \mu + 10 \mu^2 + 16 \mu^3} {(1+2 \mu) \mu^2} \right) 
\right)^{-1}
\end{align}
\end{lemma}
\begin{proof}
The proof is similar to the the theorem 3.1 in \cite{CS04}. Although the functional $\mathcal{J}$ is defined only with the $L^2$ norms of the residuals, here we also consider non-homogeneous boundary conditions and the contact boundary. \\
The upper bound can be easily shown by using triangle inequality, $\|\mathcal{A}\btau\| \leq \dfrac{1}{2 \mu} \| \btau \|$, Young's and trace inequalities. \\
In order to prove the inequality from below, it is sufficient to bound all the terms in $M$ with the functional. Knowing that $\|\mathcal{A}\btau\| \leq \dfrac{1}{2 \mu} \| \btau \|$:
\begin{align*}
 \| \text{div}\bs\|_{L^2}^2  + \| \boldsymbol{\varepsilon}(\bw) \|^2 \leq  \| \text{div}\bs\|_{L^2}^2 +2 \| \boldsymbol{\varepsilon}(\bw) - \mathcal{A}\bs\|^2 + 2 \| \mathcal{A}\bs\|^2  \leq 2 \mathcal{F}(\bw,\bs;0) +  \dfrac{1}{2 \mu }\| \bs\|^2
\end{align*} 
Therefore it is now sufficient to bound $\dfrac{2 \mu +1}{2 \mu} \| \bs \|_{L^2} $. 
We just exploit $(\mathcal{A}\bs,\bs )\geq \dfrac{1}{2 \mu}  \| \bs \|^2$:
\begin{align*}
(\mathcal{A}\bs,\bs ) &= (\mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw),\bs ) + (\boldsymbol{\varepsilon}(\bw),\bs)=  (\mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw),\bs ) + (\bs - \frac{\bs -\bs^T}{2},\nabla \bw)\\
&=(\mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw),\bs )-(\text{div} \bs,\bw) +\int_{\partial \Omega} \bs \bn \cdot \bw - \left(\bs-\bs^T, \nabla \bw \right)\\ 
& \leq \|  \mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw) \| \| \bs\| +\|\text{div} \bs\| \| \bw \| +\|\frac{\bs-\bs^T}{2}\| \|\nabla \bw \|+\int_{\partial \Omega} \bs \bn \cdot \bw \\
& \leq \mu \|  \mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw) \|^2+    \dfrac{1}{4 \mu} \| \bs\|^2
+  \|\boldsymbol{\varepsilon}(\bw) \| \left( \|\text{div} \bs\| + \|\frac{\bs-\bs^T}{2}\|\right)+\int_{\partial \Omega} \bs \bn \cdot \bw 
\end{align*}
And by using $\| \bs -\bs^T\| \leq 4 \mu \|\mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw) \|$, $\| \bs \|^2 \leq 2 \mu (\mathcal{A} \bs, \bs)$, 
$\| \mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw)\| \leq  \mathcal{F}(\bw,\bs;0)^{1/2}$, $\| \bs \| \leq  \mathcal{F}(\bw,\bs;0)^{1/2}$, $\| \bs -\bs^T\| \leq 4 \mu \|\mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw) \|$:
\begin{align*}
(\mathcal{A}\bs,\bs ) & \leq 2 \mu \|  \mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw) \|^2+ 2 \left( 1+ 2 \mu \right) \mathcal{F}(\bw,\bs;0)^{1/2}  \|\boldsymbol{\varepsilon}(\bw) \| 
+\int_{\partial \Omega} \bs \bn \cdot \bw 
\end{align*}
Now it is exploited again $ \|  \mathcal{A}\bs - \boldsymbol{\varepsilon}(\bw) \|^2\leq \mathcal{F}(\bw,\bs;0) $ and $ \| \boldsymbol{\varepsilon}(\bw) \|  \leq  \mathcal{F}(\bw,\bs;0)^{1/2} + \| \mathcal{A} \bs\| \leq \mathcal{F}(\bw,\bs;0)^{1/2} +  \dfrac{1}{2 \mu }\| \bs\|$, so that:
\begin{align*}
(\mathcal{A}\bs,\bs ) & \leq (2 +6 \mu)\mathcal{F}(\bw,\bs;0) 
+\dfrac{\left( 1+ 2 \mu \right) }{\mu}\mathcal{F}(\bw,\bs;0)^{1/2}  \| \bs \| 
+2 \int_{\partial \Omega} \bs \bn \cdot \bw 
\end{align*}
By Young's inequality:
\begin{align*}
(\mathcal{A}\bs,\bs ) & \leq 2 (2 +6 \mu)\mathcal{F}(\bw,\bs;0) 
+2 \dfrac{ \left( 1+ 2 \mu \right)^2 }{2 \mu^2}\mathcal{F}(\bw,\bs;0)
+4 \int_{\partial \Omega} \bs \bn \cdot \bw\\
& \leq \dfrac{4 \mu^2 + 12 \mu^3 + 1 + 4 \mu^2 + 4 \mu}{\mu^2} \mathcal{F}(\bw,\bs;0) +4 \int_{\partial \Omega} \bs \bn \cdot \bw  
\end{align*}
By rearranging the terms:
\begin{align*}
M(\bs,\bw)&=  \|  \boldsymbol{\varepsilon}(\bw) \|_{L^2}^2 +  \| \bs \|_{L^2}^2 +  \| \text{div} \bs \|_{L^2}^2\\
& \leq 2 \mathcal{F}(\bw,\bs;0) +  \dfrac{4 \mu^2 + 12 \mu^3 + 1 + 4 \mu^2 + 4 \mu}{(1+2 \mu) \mu^2}  \mathcal{F}(\bw,\bs;0)  +4 \int_{\partial \Omega} \bs \bn \cdot \bw \\
&\leq \dfrac{1}{C_1} \left( \mathcal{F}(\bw,\bs;0) + \int_{\partial \Omega} \bs \bn \cdot \bw\right) \\
&=\dfrac{1}{C_1} \left( \mathcal{F}(\bw,\bs;0) + \langle \bs \bn \cdot \bn,  \bw \cdot \bn \rangle_{\Gamma_C}\right) 
\end{align*} 
where we have exploited $\bw|_{\Gamma_D}= \textbf{0}$, $\bs \bn|_{\Gamma_N} =\textbf{0}$, $\left(\bs \bn\right)\cdot \bt=0$
\end{proof}
\begin{lemma}
The augmented LS functional is strongly convex and coercive.
\end{lemma}
\begin{proof}
The functional $\mathcal{F}(\bu,\bsigma;\bff) $ is convex, but the complementarity term is not. However the whole functional $\mathcal{G}(\bu,\bsigma,
\bff,g) $ is strongly convex:
\small
\begin{align*}
\mathcal{J}( t \bu + (1-t) \bv, t \bsigma + (1-t) \btau; \bff,g)
&= t \mathcal{J}( \bu ,\bsigma ; \bff,g) + (1-t)\mathcal{J}(  \bv,  \btau; \bff,g)+ t(t-1) \mathcal{J}(\bu-\bv,\bsigma-\btau;0,0)\\
& \leq t  \mathcal{J}( \bu ,\bsigma ; \bff,g)+ (1-t) \mathcal{J}( \bv ,\btau ;\bff,g)+ t(t-1)C_1M(\bu-\bv,\bsigma-\btau)
\end{align*}
\normalsize
where the last inequality holds due to the previous result. It is also known the $\mathcal{J}(   t \bu + (1-t) \bv, t \bsigma + (1-t) \btau; \bff,g)\geq0$ $\forall \bsigma, \btau, \bu, \bv$ in the adimissible set. Then fix $t \in (0,1)$, choose $\bv$, $\btau$ as the minimizer of the problem, so that $\mathcal{J}(  \bv,  \btau, \bff,g)=0$, and let $\norm{\bu}_{H^1(\Omega)}\to \infty$, $\norm{\bsigma}_{H^N(\tdiv,\Omega)} \to \infty$:
\begin{align*}
\dfrac{1}{C_1(1-t)}\mathcal{J}( \bu ,\bsigma , \bff,g)& \geq  M(\bu-\bv,\bsigma-\btau) =\\
&( \|  \boldsymbol{\varepsilon}(\bu ) \|_{L^2}^2  +  \|  \boldsymbol{\varepsilon}(\bv ) \|_{L^2}^2 -2 \| \boldsymbol{\varepsilon}(\bu )\|_{L^2} \| \boldsymbol{\varepsilon}(\bv )\|_{L^2} +
  \| \btau \|_{H_{div}}^2+ \| \bsigma \|_{H_{div}}^2 - 2  \| \btau \|_{H_{div}}  \| \bsigma \|_{H_{div}}) \to \infty
\end{align*}
The right handside is a quadratic expression in $ \| \bsigma \|_{H_{div}}$ and $\|  \boldsymbol{\varepsilon}(\bu ) \|_{L^2}$. By the generalized Korn's inequality, if $\norm{\bu}_{H^1(\Omega)}\to \infty$, also $\norm{\boldsymbol{\varepsilon}(\bu)}_{L^2(\Omega)}\to \infty$.
\end{proof}
\begin{lemma}
It exists a unique minimizer $(\bu,\bsigma) \in K$ of the augmented LS functional $ \mathcal{J}( \bu ,\bsigma , \bff,g)$.
\end{lemma}
\begin{proof}
The proof follows by strong convexity and coercivity of the functional.
\end{proof}

\section{Monotone Multilevel}
Let $\mathcal{T}_1$ be a partition of $\Omega$ into finite elements $\tau$ (triangles in 2D or tetrahedra in 3D), with meshwidth parameter $h_1=\max_{T\in \mathcal{T}_1}\text{diam}(T)$. Then recursively, for $j=2,...,J$,  define $\mathcal{T}_j$, with the corresponding $h_j$, as the uniform refinement  of $\mathcal{T}_{j-1}$. We also denote by $\mathcal{N}_j$, $\mathcal{E}_j$, $\mathcal{F}_j$ the sets of vertices, edges and faces of the mesh $\mathcal{T}_j$. and with $N_j$, $E_j$, $F_j$ their cardinality. Furthermore, let us consider two levels $j$, $k$ and the corresponding vertices $\nu_j$, $\nu_k$, edges $\varepsilon_j$, $\varepsilon_k$, faces $\phi_j$, $\phi_k$ and elements $\tau_j$, $\tau_k$. We define the $\bigtriangleup$-patch of $\#$ as: 
\begin{align*}
P_{\#}^{\bigtriangleup}=
\left\lbrace
\bigtriangleup: \:
\# \in \bigtriangleup \
    \right\rbrace
    \qquad \#=\nu_j,\varepsilon_j,\phi_j,\tau_j, \quad \bigtriangleup =\nu_k,\varepsilon_k,\phi_k,\tau_k
\end{align*}
Now let $P^1(\mathcal{T}_j)$  and $\text{RT}_0(\mathcal{T}_j)$ be respectively the continuous piecewise linear functions space and the lowest order Raviart-Thomas space defined on the tassellation $\mathcal{T}_j$. The related interpolation operators from level $j$ to level $j+1$ are defined as follows:
\begin{align*}
&P_j^{j+1}: P^1(\mathcal{T}_j) \to P^1(\mathcal{T}_{j+1})
\qquad
 (P_j^{j+1} u - u )|_{\nu_{j+1}} =0\qquad \forall \nu_{j+1} \in \mathcal{N}_{j+1}, \:\: \forall \bu \in P^1(\mathcal{T}_j)
\\
&\Pi_j^{j+1}: \text{RT}_0(\mathcal{T}_j) \to \text{RT}_0(\mathcal{T}_{j+1}) \qquad
\int_{\phi_{j+1}} (\Pi_j^{j+1} \sigma - \sigma) n_{j+1}  ds =0\qquad \forall \phi_{j+1} \in \mathcal{F}_{j+1}, \:\: \forall \sigma \in  \text{RT}_0(\mathcal{T}_j)
\end{align*}
For $j=1,...,J$ we also define the following finite element spaces:
\begin{align*}
& U_j=\left\lbrace \bu \in \left[
P^1(\mathcal{T}_j) \right]^d
  \right\rbrace \qquad
\Sigma_j=\left\lbrace \bsigma \in \left[
\text{RT}_0(\mathcal{T}_j)\right]^d
  \right\rbrace
    \qquad
  X_j =U_j \times \Sigma_j
\end{align*}
whose basis functions belong to $\Lambda_{U_j}$, $\Lambda_{\Sigma_j}$, $\Lambda_{X_j}$. The $P^1$ fand RT$_0$ functions are defined respectively on each node $\nu \in \mathcal{N}_j$ and face $\phi \in \mathcal{F}_j$. Therefore:
\begin{align*}
\Lambda_{U_j}
=
\left\lbrace
\lambda_{j,\nu}, \quad \nu=1,...,{N}_j
  \right\rbrace ,
  \quad
  \Lambda_{\Sigma_j}
=
\left\lbrace
\lambda_{j,\phi}, \quad \phi=1,...,F_j
  \right\rbrace ,
  \quad 
  \Lambda_{X_j}=
  \Lambda_{U_j} \cup   \Lambda_{\Sigma_j}
\end{align*}
We can also build the multi-dimensional interpolation operators:
\begin{align*}
&\bP_j^{j+1}=\left[ P_j^{j+1} \right]^d : \left[P^1(\mathcal{T}_j)\right]^d \to \left[P^1(\mathcal{T}_{j+1})\right]^d
\qquad
\bPi_j^{j+1}=\left[ \Pi_j^{j+1}\right]^d 
: \left[\text{RT}_0(\mathcal{T}_j)\right]^d \to \left[\text{RT}_0(\mathcal{T}_{j+1})\right]^d 
\end{align*}
Furthermore let denote the convex set as:
\begin{align*}
K_J=\left\lbrace
  \bbx_J=(\bu_J, \bsigma_J) \in X_J: \quad \bu_J|_{\Gamma_D}=\bu_J^D , \:  \bsigma_J|_{\Gamma_N}=\bt_J^N, \: \bu_J\cdot \bn_J|_{\Gamma_C}\leq g_J, \: \bn^T(\bsigma_J \bn)  \leq 0, \: \bt_J^T(\bsigma \bn_J) =0
  \right\rbrace 
\end{align*}
where $\bu_J^D$, $\bt_J^N$, $\bn_J$, $g_J$ are suitable approximations in the discrete finite element space and geometry of $\bu^D$, $\bt^N$, $\bn$ and $g$. Of course, due to this approximation, in general $K_J \nsubseteq K$.\\
In addition, also the functional has to be discretized properly. So let $\bff_J$ be a proper approximation of $\bff$. We can write $\mathcal{J}_J(\bu,\bsigma;\bff,g)=\mathcal{J}(\bu_J,\bsigma_J;\bff_J,g_J)$. But, for the sake of simplicity, from now on we will omit the subscript $J$ from the functional. Also we would use the notation $\mathcal{J}(\bbx):=\mathcal{J}(\bu,\bsigma;\bff,g)$.\\
The discrete minimization problem is then: find $(\bu_J,\bsigma_J) \in K_J$ such that:
\begin{align*}
\mathcal{J}(\bu_J,\bsigma_J;\bff_J,g_J)  \leq \mathcal{J}(\bv_J,\btau_J;\bff_J,g_J) \quad \forall \: (\bv_J,\btau_J) \in K_J
\end{align*}
The standard projected Gauss-Seidel successively minimizes the functional $\mathcal{J}(\bbx)$ in the directions $\lambda_J \in \Lambda_{X_J}$. However the rate of convergence of this method deteriorates for $h_J \to 0$. Such an inconvenient promoted the analysis of multilevel methods that involve coarse grid corrections as well. In particular, the monotone multilevel idea is to extend the minimization process also to low frequency components of the spectrum. Therefore  $\mathcal{J}$ is minimized with respect to all $\lambda_j \in \Lambda_{X_j}$, for $j=1,...,J$.\\
A prerequisite of multilevel methods is that eigenfunctions associated to small eigenvalues can be well represented on coarse meshes. This is automatically satisfied in $H^1$, since the kernel of the gradient operator $\nabla$ boils down to costants functions. However the kernel of the divergence operator $\nabla \cdot$ is very large. All free-divergence functions, also the ones with a large gradient, are admissible. To circumvent this drawback, different strategies have been proposed (\cite{Hip97}, \cite{HX07}). Here we will focus on the one described in \cite{AW97},~\cite{AFW00},~\cite{Arn}, but since the present is a mixed formulation, an extension to primal and dual variables as in \cite{Sta00} is carried out.
\\
For each vertex $\nu \in \mathcal{T}_j$, we define the patch $\Omega_{j,\nu}$ as the union of all the elements of the same mesh $\tau_j$ that share the vertex $\nu \in \mathcal{N}_j$:
\begin{align*}
\Omega_{j,\nu} = P_{\nu_j}^{\tau_j}=
\left\lbrace
\tau_j \in \mathcal{T}_j: \nu_j \in \tau_j
  \right\rbrace 
\end{align*} 
Then the degrees of freedom of interest are the vertex itself for the displacement and all the internal faces for the stress. We denote by $\lambda_{j,\nu}$ the collection of these degrees of freedom. As usual, on the coarser level $j=1$, the direction is the whole space:
\begin{align*}
&\lambda_{j,\nu}=\left\lbrace
\lambda_{U_j, n} \in \Lambda_{U_j}, \: n  \in P_{\nu}^{\nu}
  \right\rbrace 
  \cup
  \left\lbrace
\lambda_{\Sigma_j, f} \in \Lambda_{\Sigma_j}, \: f  \in P_{\nu}^{\phi}
  \right\rbrace   && j=2,...,J\\
&  \lambda_{1,\nu}=\left\lbrace
\lambda_{U_1, n} \in \Lambda_{U_1}, \: n=1,...,N_1
  \right\rbrace 
  \cup
  \left\lbrace
\lambda_{\Sigma_1, f} \in \Lambda_{\Sigma_j}, \: f  =1,...,F_1
  \right\rbrace   && j=1\\
\end{align*}
Consequently $\mathcal{J}$ has to be minimized with respect to all $\lambda_{j,\nu} \in \Lambda_{X_j}$, for $j=1,...,J$. Since the functional is strongly convex and differentiable, the problem can be reformulated in the following form:
\begin{align*}
\begin{cases}
&
\left\langle \dfrac{\partial \mathcal{J}(\bu,\bsigma;\bff,g)}{\partial \bu }, \bv-\bu \right\rangle
\geq 0 \\\\
&
\left\langle \dfrac{\partial \mathcal{J}(\bu,\bsigma;\bff,g)}{\partial \bsigma }, \btau-\bsigma \right\rangle
\geq 0 
\end{cases}
\qquad  \forall (\bv,\btau) \in K_j
\end{align*}
Let $\bbx_{J,k} \in K_J$ be the $k$-th iterate. Then, for $j=J,...,1$ and $\nu=1,...,N_j$, we define $\bbx_{j,0}=\bbx_{j+1,N_{j+1}}$ and compute a sequence of intermediate iterates $\bbx_{j,\nu} =\bbx_{j,\nu-1}+\bc_{j,\nu}$ by solving:
\begin{align*}
\mathcal{J}(\bbx_{j,\nu}^k+\bc_{j,\nu}) \leq \mathcal{J}(\bbx_{j,\nu}^k+\by) \quad \forall \by \in K_{j,\nu}^k
\end{align*}
where the local closed convex set $K_{j,\nu}^k$ is defined as follows:
\begin{align*}
K_{j,\nu}^k(\bbx_{j,\nu}^k)=\left\lbrace
\by \in \text{span}\{\lambda_{j,\nu}\}: \quad \by +\bbx_{j,\nu}^k \in K_J
  \right\rbrace 
\end{align*}
Of course the exact solution of these local problems is the optimal one. But for its computation, a comparison with the gap functions on the fine level is needed.
However a fundamental feature of multilevel algorithms for having linear complexity is that the number of each operations on a given level $j$ is proportional to the number of degrees of freedom on the same level $j$ (\textbf{is this true for the level 1? I do not think so}).
It is then obvious that a check of coarse corrections with respect to fine constraints would imply a violation of this condition and, consequently, a suboptimal complexity. To recover an optimal complexity, only an approximate solution, instead of the exact one,  can be taken into consideration for coarser levels. To this aim, approximate convex sets $K_j^k$ have to be define and, consequently, proper coarse gap functions have to be constructed by using certain non-linear interpolation operators. \\
Therefore the local closed convex set $K_{j,\nu}^k$ is now redefined:
\begin{align*}
K_{j,\nu}^k(\bbx_{j,\nu}^k)=\left\lbrace
\by \in \text{span}\{\lambda_{j,\nu}\}: \quad \by +\bbx_{j,\nu}^k \in K_j
  \right\rbrace 
\end{align*}
where the main difference with respect to the previous definition is that we consider $K_j$ instead of $K_J$. Of course, it is necessary to build these new convex sets on level $j$. Level-dependent gap functions are hence crucial.\\
The main unknown $\bbx$ is characterized by two components, the displacement $\bu$ and the stress $\bsigma$. For each of the two, a specific coarse constraint, and so a specific interpolation operator, has to be considered. Indeed, since $\bu\cdot \bn|_{\Gamma_C} \in H^{1/2}(\Gamma_C)$ and $\bn^T \bsigma \bn \in H^{-1/2}(\Gamma_C)$, their discrete representation is also different. In the finite element framework previously introduced, the trace operator maps $P^1(\mathcal{T}_j$ into piecewise linear funtions on each element of $\Gamma_C$; and RT$_0$ functions into piecewise constant functions on $\Gamma_C$. Therefore also two different interpolation operators have to be defined.

\section{Gap function on coarser levels}
Given an approximation $(\bar{\bsigma}, \bar{\bu})$ of the solution $(\bsigma,\bu)$, a necessary and sufficient condition that a correction $(\bc_{\bsigma}, \bc_{\bu})$ has to satisfy to make $(\bar{\bsigma}+\bc_{\bsigma}, \bar{\bu}+ \bc_{\bu})$ admissible is :
\begin{align*}
\begin{cases}
\bn^T \left(\bar{\bsigma}+\bc_{\bsigma}\right) \bn \leq 0  &\text{on} \: \Gamma_C\\
 \left(\bar{\bu}+\bc_{\bu} \right) \cdot \bn \leq g &\text{on}  \: \Gamma_C 
 \end{cases}
 \qquad
 \iff
 \quad
 \begin{cases}
\bn^T \bc_{\bsigma}\bn \leq g_{J,\bsigma}(\bar{\bsigma}, \bar{\bu}) = - \bn^T \bar{\bsigma}  \bn &\text{on} \: \Gamma_C\\
\bc_{\bu}  \cdot \bn \leq g_{J,\bu}(\bar{\bsigma}, \bar{\bu}) =g - \bar{\bu}\cdot \bn &\text{on}  \: \Gamma_C 
 \end{cases}
\end{align*}
In a multilevel setting, these conditions have to be fulfilled on the fine mesh, but most of the corrections are computed on the coarser ones. 
\\\\
However a fundamental feature of multilevel algorithms for having linear complexity is that the number of each operations on a given level $j$ is proportional to the number of degrees of freedom on the same level $j$ (\textbf{is this true for the level 1? I do not think so}).
It is then obvious that a check of coarse corrections with respect to fine constraints would imply a violation of this condition and, consequently, a suboptimal complexity. To recover an optimal complexity, instead of the previous requirement, only a sufficient condition can be taken into consideration for coarser levels. To this aim, proper gap functions on the coarser levels have to be constructed by using certain non-linear interpolation operators. \\
Let $(\bsigma_k, \bu_k)$ be the $k-$th iterate. Then we compute corrections $(\bc_{j,\nu,\bsigma}, \bc_{j,\nu,u})$ in the directions $\lambda_{j,\nu}$ for $j=J,...,1$ and $\nu=1,...,N_j$.  Assuming that $(\bc_{j,0,\bsigma}, \bc_{j,0,u})=(\textbf{0},\textbf{0})$, we can define:
\begin{align*}
(\bsigma_k, \bu_k) + \sum_{j=J}^i  \sum_{\nu=0}^{\mu-1} 
(\bc_{i,\nu,\bsigma}, \bc_{i,\nu,\bu}) +
\end{align*}
Let $(\bc_{j,\bsigma}, \bc_{j,\bu})$ be a correction computed on the level j. For consistency with respect to the finer level $J$, it is enough that the constraint on the level $j$ satisfies:
\begin{align*}
 \begin{cases}
\bn^T \bc_{j,\bsigma}\bn \leq g_{j,\bsigma}(\bar{\bsigma}, \bar{\bu}) 
&\text{on} \: \Gamma_C\\
\bc_{j,\bu}  \cdot \bn \leq  g_{j,\bsigma}(\bar{\bsigma}, \bar{\bu})
 &\text{on}  \: \Gamma_C 
 \end{cases}
 \quad \text{with}
 \quad
 g_{j,\bsigma}(\bar{\bsigma}, \bar{\bu}) 
\leq g_{J,\bsigma}(\bar{\bsigma}, \bar{\bu}) \\
=
g_{j,\bsigma}(\bar{\bsigma}, \bar{\bu})
\leq 
\Pi_{j}^{j+1} g_{J,\bu}(\bar{\bsigma}, \bar{\bu}) 
\end{align*}
\textbf{Since the present is only one-side obstacle problem, it is enough that the coarser gap function is always lesser than the fine one (with correcttionssss!!!.}\\

\subsection{Interpolation for $P_1$ on $\Gamma_C$}
Let $\mathcal{T}_H=\mathcal{T}_j$ be a given mesh and $\mathcal{T}_h=\mathcal{T}_{j+1}$ its uniform refinement. Given a coarse edge $e_H \in \mathcal{T}_H \cap \Gamma_C$, which contains two coarse nodes $p_{H,1}$, $p_{H,2}$, on its ends, and a fine midpoint $p_h$. Let $g_h$ and $g_H$ be the gap function on level h and H. Since $g_h$ is linear, a sufficient condition so that $g_H \leq g_h$ is the following:
\begin{align}
\label{coarsegapfunctionconditionP1}
\begin{cases}
& g_H(p_{H,1}) \leq g_h(p_{H,1})\\
&  g_H(p_{H,2}) \leq g_h(p_{H,2})\\
&  \dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2})) \leq g_h(p_{h})
\end{cases}
\end{align}
It is easy to see that, on $e_H$, the two following values satisfy the three conditions above. We call this kind of i\textbf{nterpolation Linear Monotone Interpolation (just to let you know that I used it for the square case)}.
\begin{align}
\label{coarsegapfunctionconditionP1LinearInterpolation}
\begin{cases}
g_H({p_{H,1}})= \min( g_h(p_{H,1}),\max( g_h(p_{h}), 2 g_h(p_{h}) - g_h(p_{H,2})) )\\
g_H({p_{H,2}})=\min( g_h(p_{H,2}),\max( g_h(p_{h}), 2 g_h(p_{h})- g_h(p_{H,1}) ) )
\end{cases}
\end{align}
There are two other ways of defining the gap function and are based on the minimum value on the support of the shape function. Furthermore it is easy to check that both definitions satisfy (\ref{coarsegapfunctionconditionP1}).
 Since at the moment we are referring to only one edge, this means searching for the minimum of $g_h$, on $e_H$ intersected with the support of the shape function. If the shape function is the coarse one, then $g_H(q)=\min(g_h(p_{H,1}),g_h(p_h),g_h(p_{H,2}))$, with $q=p_{H,1},p_{H,2}$. But this approach is very roguh (I GUESS IT IS THE ONE IN BADEA? IT DEPENDS ON THE DEFINITION OF SUPPORT FOR HIM. IF IT IS THE CLOSURE OR NOT). On the other hand, if the shape function of $p_{H,i}$ is the fine one,  $g_H(p_{H,i})=\min(g_h(p_{H,i}),g_h(p_h))$, for $i=1,2$. This last approach does not differ so much from the first one given, altough in general is still rougher.\\
Now we want to show that (\ref{coarsegapfunctionconditionP1}) is actually true for (\ref{coarsegapfunctionconditionP1LinearInterpolation}). Let $a=g_h(p_{H,1})$, $b=g_h(p_h)$, $c=g_h(p_{H,2})$. Then:
\begin{enumerate}
\item 
\begin{enumerate}
\item $ a \geq c \geq b$, $a\geq 2b -c$ ($b \geq 2 b -c$, $ b\geq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,2b -c)+\min(c,b))=\dfrac{1}{2}(2b-c+c) = b
\end{align*}
\item  $ a \geq c \geq b$, $a \leq 2 b - c$  ($b \geq 2 b -c$, $ b\geq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,2b -c)+\min(c,b))=\dfrac{1}{2}(a+c) \leq b
\end{align*}
\end{enumerate}
\item $ a \geq c \geq b$ ($b \geq 2 b -c$, $ b\geq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,b)+\min(c,b))=\dfrac{1}{2}(b+b) = b
\end{align*}
\item 
\begin{enumerate}
\item $ c \geq b \geq a$, $2 b - a \geq c$  ($b \geq 2 b -c$, $ b\leq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,b)+\min(c,2b-a))=\dfrac{1}{2}(a+c) \leq b
\end{align*}
\item $ c \geq b \geq a$, $2 b - a \leq c$ ($b \geq 2 b -c$, $ b\leq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,b)+\min(c,2b-a))=\dfrac{1}{2}(a+2b-a) = b
\end{align*}
\end{enumerate}
\item  $ c \geq a \geq b$  ($b \geq 2 b -c$, $ b\geq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,b)+\min(c,b))=\dfrac{1}{2}(b+b) = b
\end{align*}
\item $ b \geq a \geq c$ or $b \geq c \geq a$ ($b \leq 2 b -c$, $ b\leq 2 b -a$):
\begin{align*}
\dfrac{1}{2}( g_H(p_{H,1}) + g_H(p_{H,2}))=\dfrac{1}{2} (\min(a,b)+\min(c,b))=\dfrac{1}{2}(a+c) \leq b
\end{align*}
\end{enumerate}
This requirement has to be true for all edges $e_H$ belonging to its patch $E_H=\{e_H \in P_{p_H, e,H} \}$.
It is possible to define a projection operator such that these conditions are fulfilled. Let $p_{H,i}$ be a node of the coarse mesh $T_H$ and $\phi_{H,i}$ the respective coarse basis function. Then:
\begin{align*}
\displaystyle
&g_H(p_{H,1})= \min_{e_H \in E_H} \left[ \min( g_h(p_{H,1}),\max( g_h(p_{h}), 2 g_h(p_{h}) - g_h(p_{H,2})) )  \right]\\
&g_H=R_{P^{1,H}}^{P^{1,h}} g_h= \sum_{p_{H,i} \in T_H} \phi_{H,i
} \:g_H(p_{H,i})
\end{align*}
Since $g_H$ is a piecewise linear function such that (\ref{coarsegapfunctionconditionP1}) is always fulfilled, it is evident that:
\begin{align*}
0 \leq R_{P^{1,H}}^{P^{1,h}} g_h(x)=g_H(x) \leq g_h(x) \quad \text{if} \quad g_h\geq 0
\end{align*}
Consequently the function:
\begin{align*}
\theta_g(x)= \begin{cases}
\dfrac{R_{P^{1,H}}^{P^{1,h}} g_h(x)}{g_h(x)} & g_h \neq 0 \\
0 & g_h =0
\end{cases}
\end{align*}
satisfies $0 \leq \theta_g \leq 1 $.
\subsection{Interpolation for $\RT_0$ on $\Gamma_C$}
Given a mesh $T_H$ and its uniform refinement $T_h$, consider a face $f_H \in T_H \cap \Gamma_C$ and all the fine faces belonging to it, i.e. patch $F_{h,H}=\{f_h \in f_H \}$. Then, since a discrete $H^{-1/2}$ function is constant on each face of $\Gamma_C$, it is sufficient to define:
\begin{align*}
\displaystyle
R_{\RT_{0,h}}^{\RT_{0,H}} (v)|_{f_H}= \min_{F_{h,H}} v
\end{align*} 
 Let $f_{H,i}$/$f_{h,j}$ be the midpoint of the face $i$/$j$ of the coarse/fine mesh $T_H$/$T_h$. Then let $\phi_{H,i}$ be the respective coarse basis function:
\begin{align*}
\displaystyle
&g_H(f_{H,i})= \min_{f_{h} \in f_{H,i}} g_h(f_h)\\
&g_H=R_{RT_0^{1,H}}^{RT_0^{1,h}} g_h= \sum_{f_{H,i} \in T_H} \phi_{H,i
} \:g_H(f_{H,i})
\end{align*}
Also in this case we can prove:
\begin{align*}
\theta_g(x)= \begin{cases}
\dfrac{R_{RT_0^{1,H}}^{RT_0^{1,h}} g_h(x)}{g_h(x)} & g_h \neq 0 \\
0 & g_h =0
\end{cases}
\end{align*}
satisfies $0 \leq \theta_g \leq 1 $ on $\Gamma_C$. 
\\
\textbf{PLEASE NOTE THAT THETA IS STILL A $\RT_0$ FUNCTION. ESSENTIALLY IT IS DEFINED WITH THE AVERAGE VALUE ON THE FACE WHERE I DO NOT HAVE THE CONSTRAINTS AND WITH THE MINIMUM ON $\Gamma_C$. } Therefore later when I need to use $0 \leq \theta_g \leq 1 $, I use this ONLY on $\Gamma_C$. But I need it to show that $u_j$ is in $K_j$.
%For both interpolations, $g_h$ on the fine level is defined as:
%\begin{align*}
%\begin{cases}
%\tilde{g}_h := g_h - c_{j,k,\bp} & k=\bu,\bsigma, \bp \in T_{j+1}\\
%\end{cases}
%\end{align*}



The problem
\begin{align*}
\begin{cases}
&\left\langle \dfrac{\partial \mathcal{J}(\bu,\bsigma;\bff,g)}{\partial \bu }, \bv-\bu \right\rangle \geq 0 \\\\
&\left\langle \dfrac{\partial \mathcal{J}(\bu,\bsigma;\bff,g)}{\partial \bsigma }, \btau-\bsigma \right\rangle \geq 0 
\end{cases}
\qquad  \forall (\bv,\btau) \in K_j
\quad
\iff
\quad
\left\langle \dfrac{\partial \mathcal{J}(\bbx;\bff,g)}{\partial \bbx }, \by-\bbx \right\rangle \geq 0 \qquad  \forall \by \in K_j
\end{align*}
Consequently, starting with the given $k-$th iterate $\by_{J,0}=\bbx^k$, sequentially compute:
\begin{itemize}
\item For $j=J$
\begin{itemize}
\item $\by_{j,i}=\by_{j,i-1}+\bz_{j,i}$, $\forall \nu \in \mathcal{N}_{J}$
\end{itemize}
\item For $j=J-1,...,1$ \begin{itemize}
\item $\by_{j,0}=\by_{j+1,\mathcal{N}_{j+1}}$
\item $\by_{j,i}=\by_{j,i-1}+\bz_{j,i}$, $\forall \nu \in \mathcal{N}_{j}$
\end{itemize}
\end{itemize}
where the $\bz_{j,i}$ is computed as the solution of the following variational inequality:
\begin{align*}
\left\langle \dfrac{\partial \mathcal{J}(\bbx;\bff,g)}{\partial \bbx }, \by-\bbx \right\rangle \geq 0 \qquad  \forall \by \in K_j
\end{align*}
\subsection{Change of coordinate system  }
In order to properly describe contact conditions, it is wise to locally change the coordinate system of the contact boundary $\Gamma_C$. In this way, the scalar constraints have to be checked directly on the normal components and not on some linear combinations of the unknown.\\ In the following the contact normal is assumed to be known in all vertices and all midpoints of the faces belonging to $\Gamma_C$.
Let $\nu \in \mathcal{N}_j$ be a vertex of the tasselation $\mathcal{T}_j$ that also belongs to $\Gamma_C$ and $\bn_{\nu}$ the obstacle normal in $\nu$. Then consider the vector $ \bu_{\nu} \in \mathbb{R}^d$ that contains the degrees of freedom of the displacement in $\nu$. Define the Householder transformation relative to the ouward normal $\bn_{\nu} $ as:
\begin{align}
\bH_{\nu}= \bI- 2\: \bn_{\nu} ^T \bn_{\nu} 
\end{align}
and the local displacement in the normal-tangent coordinate system (the first coordinate is the normale one):
\begin{align*}
\bu_{\nu,nt}= \bH_{\nu} \bu_{\nu}
\end{align*}
A similar argument has to be applied to the stress components. For each face $\phi \in \Gamma_C$, we can express each vector unknown $\bSigma_{\phi}$ in terms of the normal and tangent forces. To this aim, we rewrite the external force $\bsigma \bn_{\phi} $ in terms of the normal and tangent component $\bSigma_{\phi,nt}$ thanks to the Householder transformation. It is not convenient to use direclty the HouseHolder transformation, because we have no control on the sign of $\left( \bphi_{\bp}  \cdot \bn_{\bp}  \right)$. In its place, it is preferrable the transformation $\bQ_{\phi}$.
\begin{align*}
\bsigma \bn_{\phi} = \left( \bphi_{\phi} \cdot \bn_{\phi} \right) \bSigma_{\phi} = \bH_{\phi} \bSigma_{\phi,nt} \quad
 \iff \quad \bSigma_{\phi} = \dfrac{1}{  \left( \bphi_{\phi} \cdot \bn_{\phi} \right)}\bH_{\phi} \bSigma_{\phi,nt} = \bQ_{\phi} \bSigma_{\phi,nt}
\end{align*}
In this way the first component of $\bu_{\nu,nt}$/$\bSigma_{\phi,nt}$ is actually positive in the direction of the normal $\bn_{\nu}$/$\bn_{\phi}$, with $\bH_{\nu}$/$\bQ_{\phi}$ orthogonal. \\
From now on all the degrees of freedom on the contact boundary will be treated as normal or tangent. In this way, we can define $\bPhi_i$, which is the tensor whose rows are the $\RT_0$ shape functions on the face i, and the corresponding $\bPsi_i$:
\begin{align*}
\bPhi_i =
\begin{bmatrix}
\bphi_i\\
\bphi_i\\
\bphi_i\\
\end{bmatrix}
\quad
\bPsi_i =
\begin{bmatrix}
\bpsi_i\\
\bpsi_i\\
\bpsi_i\\
\end{bmatrix}
\quad
\bPsi_i=\begin{cases}
\bPhi_i \bQ_i & i \in \Gamma_C \\
\bPhi_i   & \text{otherwise}
\end{cases}
\end{align*}


\section{The minimization process}
\begin{algorithm}
\caption{\textbf{[x,WS]=NonLinearSmoothing(A,b,x,g,P,j,J,k)}}
\begin{algorithmic}
\State{$\bbx$=solution, $\textbf{WS}$=working set, $\bA$=system matrix, $\bb$=right hand side, $\bg=$ gap}  
\State{$\bP$=projections, $j$=level, $J$=maximum level, $k$=number of smoothing-steps }  
\State{TruncatedBasis: remove degrees of freedom belonging to the WorkingSet WS}
\State{CoarseConstraint: define $\bg_{j-1}$ such that $\bbx_{j}+\bP_{tr}\bbx_{j-1}\leq \bg_j $}
\State{}
     \If{  ($j=1$) }\\
\textbf{[x,WS]=ActiveSet(A,b,x,g)}
   \Else
\begin{enumerate}
\item \For{$\nu=1,...,\mathcal{N}_j$}
        \State $\bbx$=ActiveSet($A_{j,\nu}$, $\bb$)
      \EndFor 
\item Do $k$ non-linear smoothing-steps
\end{enumerate}   
    \EndIf 
  \end{algorithmic}
\end{algorithm}

\begin{align*}
\forall \nu \in \mathcal{N}_j\\
\end{align*}


\begin{algorithm}
\caption{\textbf{[x,WS]=V-CYCLE(A,b,x,g,P,j,J,k)}}
\begin{algorithmic}
\State{$\bbx$=solution, $\textbf{WS}$=working set, $\bA$=system matrix, $\bb$=right hand side, $\bg=$ gap}  
\State{$\bP$=projections, $j$=level, $J$=maximum level, $k$=number of smoothing-steps }  
\State{TruncatedBasis: remove degrees of freedom belonging to the WorkingSet WS}
\State{CoarseConstraint: define $\bg_{j-1}$ such that $\bbx_{j}+\bP_{tr}\bbx_{j-1}\leq \bg_j $}
\State{}
     \If{  ($j=1$) }\\
\textbf{[x,WS]=ActiveSet(A,b,x,g)}
   \Else
\begin{enumerate}
\item Do $k$ non-linear smoothing-steps: compute $\bbx$
\item Compute $ \bP_{tr} =\text{TruncatedBasis}(\bP,j,\textbf{WS}) $
\item Compute $\br_j=\bb-\bA \bbx$
\item Compute $\br_{j-1}=\bP_{tr}^T \br_j$
\item Compute $\textbf{G}_{j}=\bg_j-\bbx$
\item Compute $\bg_{j-1}=\text{CoarseConstraint}(\textbf{G}_{j})$
\item Compute $\bA_{j-1}=\bP_{tr}^T \bA_j \bP_{tr}^T$
\item \textbf{[x,WS]=V-CYCLE($\bA_{j-1},\br_{j-1}, \textbf{0} ,\bg_{j-1},\bP,j-1,J,k$)}
\item Do $k$ non-linear smoothing-steps
\end{enumerate}   
    \EndIf 
  \end{algorithmic}
\end{algorithm}

\begin{align*}
& U_j=\left\lbrace \bu \in \left[
P^1(\mathcal{T}_j) \right]^d, \quad \bu|_{\Gamma_D}=\bu^D 
  \right\rbrace &&
  U_{j,0}=\left\lbrace \bu \in \left[
P^1(\mathcal{T}_j) \right]^d, \quad \bu|_{\Gamma_D}=\textbf{0}
  \right\rbrace \\
& \Sigma_j=\left\lbrace \bsigma \in \left[
\RT_0(\mathcal{T}_j)\right]^d, \quad \bsigma|_{\Gamma_N}=\bt^N
  \right\rbrace &&
  \Sigma_{j,0}=\left\lbrace \bu \in \left[
\RT_0(\mathcal{T}_j) \right]^d, \quad \bu|_{\Gamma_N}=\textbf{0}
  \right\rbrace
\end{align*}
Note that in general $K_j \nsubseteq K$ is not 
Obviously $U_1 \subset ... \subset U_J$, $\Sigma_1 \subset ... \subset \Sigma_J$ and $X_1 \subset ... \subset X_J$.
${}$ \\
The discrete problem can be formulated in this way: find $(\bu, \bsigma) \in K_J$ such that:
 \begin{align*}
\mathcal{J}(\bu,\bsigma;\bff_h,g_h) \leq \mathcal{J}(\bv,\btau;\bff_h,g_h) \quad \forall (\bv,\btau) \in K_J
\end{align*}
where $\bff_h$ and $\bg_h$ are discrete approximations of $\bff$ and $\bg$. For simpicity of notation, from now on, we will write $\bff$ and $\bg$ instead of $\bff_h$ and $\bg_h$.\\
After the vertical decomposition into coarser spaces, an horizontal decomposition of the domain is necessary. For each level $j>1$, we define a decomposition of this kind.
\\ 
\\
To simplify notation, we define $\bbx=(\bu,\bsigma)$ and $\mathcal{J}(\bbx;\bff,g):=\mathcal{J}(\bu,\bsigma;\bff,g)$. Then for $j=J,...,1$, $\nu \in \mathcal{T}_j$, solve:
\begin{align*}
\left\langle \dfrac{\partial \mathcal{J}(\bbx;\bff,g)}{\partial \bbx }, \by-\bbx \right\rangle \geq 0 \qquad  \forall \by \in K_j
\end{align*}
Given the space $X_J$ spanned by the basis functions $\lambda_J \in \Lambda_J$, 
With the previous choice, on the given patch $\Omega_{j,\nu}$ it is possible to minimize with respect to the whole unknown, considering the constraints on the normal displacement and stress.

We want to stress the fact such a choice is necessary in order to obtain meaningfull smoothers also in the linear case. However, in the contact case, it is possible to deal with both constraints, 
The smoother strategy is the following. Loop on all the vertices $\nu$ of the mesh and, for each $\nu$, solve the difference problem restricted to this small patch $\Omega_{j,\nu}$, imposing homogeneous dirichlet bc on the boundary of the patch:
\begin{align*}
\forall \nu:
\qquad
\begin{cases}
\min_{\delta \bu_{\nu} \in \bU_h, \delta \bsigma_{\nu} \in \bSigma_h } \mathcal{F}(\bu_{\nu}^k+ \delta \bu_{\nu} ,\bsigma_{\nu}^k+ \delta \bsigma_{\nu} )\\
\bu_{\nu}^{k+1}=\bu_{\nu}^k+\delta \bu_{\nu},\quad \bsigma_{\nu}^{k+1}=\bsigma_{\nu}^k+\delta \bsigma_{\nu}
\end{cases}
\end{align*}
In this way one can solve for the displacements and the stresses contemporarily. In particular it is important that  $H^{\text{div}}$ and $H^1$ are not distinguished. One can go from a level to another one with the right projection operators and then simply use this smoother for both, displacement and stresses. Essentially it is a smoother for the overall LS-system that does not make any difference between $\bu$ and $\bsigma$. \\
But what is more important of this MG is that its smoother deals contemporarily and locally (on the patch) with free and not-free divergence components. So we do not need projections for the different components of the solution and we tackle the costraints in the solution space. Therefore Arnold's MG properly fulfills the MMG setting.


In order to overcome the above ,
\\\\The respective weak formulation is derived thanks to the LS principle. This strategy is particularly successful  when one wants to deal with an incompressible solid or solve accurately for the stresses.\\\\\\
The approach that is here presented is based on the LS principle (\cite{Boc09}, \cite{YL97}, \cite{BMM97}). 

 Since contact is a non linear problem, one has to take care of the constraints. While the first two  are linear, the complementarity condition is not. For this reason, the LS functional is augmented by a proper boundary term, that is always positive if the first and second conditions are satisfied (\cite{KMS17}).
Then the problem is the following: \textit{ find $\left(\bu,\bsigma \right)$, satisfying essential boundary conditions (bc), such that}:
\begin{align*}
\begin{cases}
 &\min \limits_{(\bu,\bsigma) \in K}\mathcal{J}(\bu,\bsigma)=C_{eq} ||\text{div} \bsigma+\bff||_{L^2(\Omega)^d}^2+C_{const}||\mathcal{A}\bsigma -\boldsymbol{\varepsilon}(\bu)||_{L^2(\Omega)^d}^2   +C_{compl} \langle \bu \cdot \bn -g, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_c}\\
 &K=\{  \left(\bu,  \bsigma \right)  \in  \left[H_{\Gamma_{d}}^1(\Omega) \right]^d \times \left[ H_{\Gamma_N}^{\text{div}}(\Omega) \right]^d    : \quad 
 \bu \cdot \bn - g  \leq 0, \quad  (\bsigma \bn) \cdot \bn \leq 0 \quad  \Gamma_C
 \}
 \end{cases}
\end{align*}
The minimum of the functional can be derived by enforcing the first-order necessary optimality condition on the convex set $K$. The corresponding weak formulation is nothing else but the normal equations. Therefore, instead of solving $\bA \bbx =\bb$, one has to solve $\bA^* \bA \bbx =\bA^* \bb$. The system is now positive definite (actually it is for sure only if $C_{compl}=0$), but unfortunately the new condition number is bounded from the previous one to the power of two: $K(A^* A) \leq K(A)^2$. A good preconditioning technique must be taken into consideration in order to use iterative solvers. In particular  multigrid algorhitms will be then examined.\\
Also the weights $C_{eq}$, $C_{const}$ and $C_{compl}$ have to be determined. Because the LS-functional is fictitious and has no physical meaning, this decision has to be made a posteriori and will influence also the symmetry of the stress tensor (never imposed). From (\cite{CS04}), it is known that the asymmetry of the stresses can be bounded by the constitutive law: $||\sigma-\sigma^T||\leq 4 \mu || \mathcal{A} \bsigma-\boldsymbol{\varepsilon}(\bu)||$. So one could think of using $C_{const} \gg C_{eq}$ to ensure its symmetry. Nevertheless, in this way, the equilibrium  is not well captured and the solution will be not satisfactory. The functional must be somehow modified and special techniques are required (\cite{BCY06}, \cite{SSS11}).  \\
An important remark is that the bc have been imposed directly in the space (essential {bc}), but boundary terms can also be added to volume ones ({natural bc}). See \cite{Boc09} and \cite{Sta99}.  In the last case, a proper weight must be determined. \begin{align*}
\mathcal{F}(\bu,\bsigma)=\mathcal{F}_{\text{homogenous bc}}(\bu,\bsigma)+C_D ||\bu-\bh ||_{[H^{1/2}(\Gamma_D)]^d}^2+ C_N ||\bsigma  \bn - \bg  ||_{[H^{-1/2}(\Gamma_N)]^d}^2
\end{align*}
However it is desirable to reduce the number of free parameters and from now on only essential bc will be dealt with.
Furthermore the LS-functional can be adopted as an efficient and reliable a posteriori error estimator. Both properties are important and guarantee that one can refine/coarsen where the value of the functional is large/small. Let $e$ be the error and $|| \cdot||$ a proper norm. Then one can prove that (\cite{CLM+94}, \cite{CS04}, \cite{KMS17}, \cite{Sta09}):
\begin{align*}
&C_1 ||\text{e}||  \overbrace{\leq}^{ \text{reliability} }  \mathcal{F} \overbrace{\leq}^{\text{efficiency}} C_2 ||\text{e}||  \quad \Rightarrow \quad
 ||\text{e}||   \to 0 \:\: \iff \:\: \mathcal{F} \to 0
\end{align*}
${}$\\
In order to solve the LS functional, its weak formulation has to be discretized on a tassellation $\mathcal{T}_h$ of $\Omega$ of triangles/tetrahedra (2D/3D). Then a proper discrete-space for the approximation of displacements and stresses has to be chosen. In general, in order to discretize displacement and stresses, one uses respectively the spaces $\bU_h$ and $\bSigma_h$, built from the first-order Lagrange FE space $P^1(T_h)$ and the zero-order Raviart-Thomas space  $\RT_0(T_h)$. The space $P^1(T_h)$ is scalar and its dofs coincide with the vertices of the tassellation, while $\RT_0(T_h)$ is a vector-value FEspace whose dofs lie on the edges/faces (2D/3D). For assembling details: \cite{RKL09}.
%\\\\\\
%An important feature of the LS-approach is that, under certain conditions, it can give rise to a positive definite system with any kind of conforming approximation space. What is important is tha the Such a property is also true for contact systems, since the respective functional, defined on the convex set $K$, is always positive definite. So, altough Once the problem has been discretized, we have to solve. To this end, we need suitable solvers.
%
%${}$\\\\\\\\  The problem becomes: search for $(\bu, \bsigma) \in $ $\left[H_{\Gamma_d}^1(\Omega) \right]^d\times \left[ H_{\Gamma_N}^{\text{div}}(\Omega) \right]^d$ such that:
%\begin{align*}
% \begin{cases}
%C_{eq}(\text{div} \bsigma , \text{div} \btau) + C_{const} (\mathcal{A} \bsigma,\mathcal{A} \btau) - C_{const}(\boldsymbol{\varepsilon}(\bu),\mathcal{A} \btau)=-C_{eq}(\bff , \text{div} \btau) & \forall \btau \in \left[ H_{\Gamma_N^0}^{\text{div}}(\Omega) \right]^d \\
%- C_{const}(\boldsymbol{\varepsilon}(\bv),\mathcal{A} \bsigma)+
%C_{const}(\boldsymbol{\varepsilon}(\bu),\boldsymbol{\varepsilon}(\bv))=0 & \forall \bv \in \left[H_{\Gamma_{d}^0}^1(\Omega) \right]^d\\
%\bu = \bu^D & \Gamma_D\\
%\bsigma  \bn = \bt^N & \Gamma_N\\
% \end{cases}
%\end{align*}
%where the spaces with the pedex $\Gamma_d$, $\Gamma_N$ satisfy the bc of the problem, whilst the ones with $\Gamma_d^0$, $\Gamma_N^0$ have homogeneous ones.\\
\section{Geometric Multigrid in $H^{\text{div}}$ for contact problems}
The LSFE approximation consists of a set of equations whose corresponding matrix has a condition number that can almost double the one of the standard formulation. Furthermore such quantity increases with the number of degrees of freedom. Since the behaviour of usual iterative methods, as Jacobi, deteriorates with the increasing of dimension of the problem, a good preconditioner is needed.
Such a preconditioner must be able to tackle $H^1$ and $H_{\text{div}}$ variables, but also the contact constraints. Actually, at the state of the art, no fast solver for LS contact problems has been developed.\\
LS linear elasticity can be solved with a linear $H^1$, $H_{\text{div}}$ multigrid (MG). However, in the contact formulation, the presence of constraints makes the problem non linear. One standard way to deal with the constraints globally is the Lagrange Multiplier method. For each constraint a proper multiplier must be considered and the LS functional has to be modified consequently. Since there are two constraints, impenetrability and negative pressure, also two lagrange multipliers are needed. Unfortunately this strategy makes the system a saddle point and increases the number of variables, from two, to four. This is why it would be desirable to maintain the original setting and tackle the constraints locally. For sure in the local setting one has to solve for constraints. But at least in this way the issue has to be solved only locally and the number of global variables  can be reduced. \\
This approach is exactly carried out by monotone MG (MMG), that has been already studied for the displacement formulation of contact problems (\cite{Kor94} ,\cite{KKS+08}, \cite{Kra09}).  The purpose of this project is actually its extension to the LS formulation for contact.  
Let us consider the following problem:
\begin{align*}
u \in K : \mathcal{J}(u) \leq \mathcal{J}(v) \qquad \forall v \in K \subset U
\end{align*}
MMG aims to solve the above problem by reducing sequentially the energy on different subspaces $U_i \subset U$, $i=1,...,n$. In the presence of constraints, instead of the subspace $U_i$,  the corresponding convex set $K_i \subset U_i$, obviously satisfying the constraints, can be considered. Given the following decomposition $U=U_1+...+U_n$, for the global space, and $K=K_1 +...+K_n$, for the convex set, and given an initial iterate $w_0=u_{\nu}$, the following subproblems must be solved sequentially:
\begin{align*}
\mathcal{J}(w_i) =\min \limits_{v \in U_i, \:  w_{i-1}+v \in K } \mathcal{J}(w_{i-1}+v)   \qquad i=1,...,n
\end{align*}
and then the next iterate can be updated as $u_{\nu+1}=w_n$. By tackling the constraints locally, one can avoid the use of lagrange multipliers in the global system, and so the saddle point framework. Also the number of global variables is unchanged. For these reasons, an $H_{\text{div}}$-MG that can fit the MMG framework should be chosen. \\
The difference with respect to usual schemes is that in the LS approach the variables live respectively in $H^1$ and $H^{\text{div}}$. Therefore classical MG for $H^1$ is necessary, but not sufficient, since also the stress components have to be taken care of. The amount of literature that tackles $H^1$-MG is very large (see \cite{BY10} and its references), so it is preferable to focus on $H^{\text{div}}$-MG.\\
Two are the main charachters of a MG method: the intergrid projections and the smoother operators. The $\RT_0$ projection operator for nested meshes $ \Pi_C^F $ is defined such that the average flux through each face is the same. Then, if $\mathcal{F}_{h}$ is the set of all the faces of the triangulation:
\begin{align*}
\Pi_C^F: \RT_{0,C} \to \RT_{0,F}: \qquad 
\int_{F_i} (\Pi_C^F \bv - \bv) \cdot \bn_{F_i}  d\bs =0\qquad \forall F_i \in \mathcal{F}_{h}
\end{align*}
The  Raviart-Thomas $\Pi_C^F$ projection operator is a sparse matrix and its application does not even require to invert a mass matrix, like other $L^2$ projection operators do. Therefore problems for MG in $H^{\text{div}}$ can regard only the smoother. \\
The trivial first try would be using well-known smoothers for $H^1$. Unfortunately classical $H^1$-MG is not able to solve $H^{\text{div}}$ problems. \\
Indeed the prerequisite of MG is that eigenfunctions associated to small eigenvalues can be well represented on coarse meshes. This is automatically satisfied in $H^1$, since its kernel $\mathcal{N}(\nabla)$ boils down to costants functions. However the kernel of the div operator, $\mathcal{N}(\text{div})$, is very large. All free-divergence functions, also the ones with a large gradient, are admissible. For a better understanding, let us consider the two basic prototype weak forms  for $H^1$ and $H^{\text{div}}$ and the corresponding eigenvalues: 
\begin{align*}
\begin{cases}
\Lambda_{H^1}(\bu,\bv)=(\bu,\bv)+(\nabla \bu,\nabla \bv) \\
\lambda_{H^1}=1+\frac{|\nabla \bu|_1^2}{||\bu||_{L^2}^2}
\end{cases}
\qquad 
\begin{cases}
\Lambda_{H^{\text{div}}}(\bu,\bv)=(\bu,\bv)+(\text{div} \bu,\text{div} \bv)\\
\lambda_{H^{\text{div}}}=1+\frac{|\text{div} \bu|_1^2}{||\bu||_{L^2}^2}
\end{cases}
\end{align*}
It is obvious that the aforementioned essential requirement is fullfilled for the operator $\Lambda_{H^1}$.  The eigenvalue $\lambda_{H^1}$ decreses for less oscillatory functions and equals the unity for constant functions. Therefore on coarse meshes, these functions can be appropriately represented. 
On the other hand, small eigenvalues $\lambda_{H^{\text{div}}}$ can be associated to free-divergence functions with large gradients that a low-quality mesh is unable to characterize. In short, we cannot direclty approximate the elements of $\mathcal{N}(\text{div})$. This complication is reflected in the smoother property. \\
\textit{At the moment, none of the main $H_{\text{div}}$ MG has been applied to LS elasticity, much less to contact problems. The aim of the present project is indeed to extend $H_{\text{div}}$ MG to elasticity and contact. Due to the non linearities of the contact problems, the $H_{\text{div}}$-MG must also be a MMG}. \\
In the following, the two more important $H_{\text{div}}$-MGs are examined.
\begin{enumerate}
\item The Auxiliary Space Preconditioning is a very general formulation that can be used to derive algebraic or geometric MG -in particular, Hiptmair's MG- (\cite{HX07}, \cite{Hip97}, \cite{HT00}, \cite{XCN09}, \cite{Xu96}). \\
The idea is to decompose the solution space $U$ into more subspaces, by treating another space $\bar{U}$ defined as $\bar{U}=U \times W_1 \times.. \times W_J$. Now $U$ is equipped with an inner product $s(\cdot,\cdot)$ whose operator $S:U \to U'$ is called smoother. So basically the solution $\bar{u} \in U$ is splitted as $\bar{u}=u +  \Pi_1 w_1 ...+ \Pi_J w_J$. The error is smoothed separately on $u$ and on each of component $w_j$, that is related to the original space $U$ through the transfer operator $ \Pi_j: W_j \to U$. 
Then an additive or multiplicative Schwarz preconditioner can be defined:
\begin{align*}
B=S^{-1}+\sum_{j=1}^J \Pi_j A_j^{-1} \Pi_j^* \qquad B=(\prod_{j=1}^J \Pi_j A_j^{-1}\Pi_j^* )S^{-1}
\end{align*}
Of course nobody wants to solve explicitly $A_j^{-1}$. Instead a proper preconditioner, such as MG, can be used on that particular subspace $W_j$. 
With the additive version, the subspace correction is not done sequencially (see \cite{Xu92} for MG as subspace correction). This means that is not monotone. The multiplicative Schwarz is intrinsically sequential and so can be a MMG. Nevertheless the splitting of the solution in different components belonging to different types of spaces makes the application of constraints far from being straightforward. The constraints are written in the $U$ space and applying them in a $W_j$ space, thorugh the proper transfer operator, can be really cumbersome. \\
%In fact one can notice that also in standard MG the constraints on the coarser meshes should be applied by means of projection operators, but for the fine level these ones are not needed. Nevertheless they are necessary for the finer level $W_i$, besides its coarser subspaces. So  both fine and coarser subspaces of $W_i$ require the $\Pi_i$. This makes the application of constraints really inconvenient.
\item Arnold'smoother (\cite{AW97}, 
~\cite{Arn}, ~\cite{AFW00}), instead on a single degree of freedom (as Jacobi or Gauss-Seidel), acts on a whole patch of the mesh. Starke shows in \cite{Sta00} that this method can be easily extended for the LS system. For each vertex $\nu \in \mathcal{T}_h$, we define the patch $\Omega_{\nu}$ as the union of all the elements that share the vertex $\nu$. The smoother strategy is the following. Loop on all the vertices $\nu$ of the mesh and, for each $\nu$, solve the difference problem restricted to this small patch $\Omega_{\nu}$, imposing homogeneous dirichlet bc on the boundary of the patch:
\begin{align*}
\forall \nu:
\qquad
\begin{cases}
\min_{\delta \bu_{\nu} \in \bU_h, \delta \bsigma_{\nu} \in \bSigma_h } \mathcal{F}(\bu_{\nu}^k+ \delta \bu_{\nu} ,\bsigma_{\nu}^k+ \delta \bsigma_{\nu} )\\
\bu_{\nu}^{k+1}=\bu_{\nu}^k+\delta \bu_{\nu},\quad \bsigma_{\nu}^{k+1}=\bsigma_{\nu}^k+\delta \bsigma_{\nu}
\end{cases}
\end{align*}
In this way one can solve for the displacements and the stresses contemporarily. In particular it is important that  $H^{\text{div}}$ and $H^1$ are not distinguished. One can go from a level to another one with the right projection operators and then simply use this smoother for both, displacement and stresses. Essentially it is a smoother for the overall LS-system that does not make any difference between $\bu$ and $\bsigma$. \\
But what is more important of this MG is that its smoother deals contemporarily and locally (on the patch) with free and not-free divergence components. So we do not need projections for the different components of the solution and we tackle the costraints in the solution space. Therefore Arnold's MG properly fulfills the MMG setting.
\end{enumerate} 
\section{Conclusion}
Contact problems arise in many engineering applications. In general a good approximation of the stresses and the capacity of dealing with incompressible materials is required. In this direction, LS formulation is very promising for the solution of mechanics. Unfortunately the condition number of the derived system can be very large and, for the solution of real complex problems, a huge number of degrees of freedom is needed. Therefore fast solvers as MG are necessary. Since there are two different variables, the displacement $\bu \in H^1$ and the stress $\bsigma \in H_{\text{div}}$, the corresponding MG has to tackle both of these spaces. Between auxiliary space preconditioning and Arnold's MG, this last one is preferred. Indeed it fits better the setting of monotone MG. This means that one can solve also for contact problems, dealing with the constraints only locally so that the whole system does not boil down to a saddle point. The future work actually points in this direction, aiming at a 2D and 3D implementation of such an algorithm. 
In many engineering applications, some of the most important quantities of interest are the forces. In particular the forces on a solid structure can be generated by the contact with another body. Possible examples of this phenomenon can be the contact of: a foundation with the soil; a biomedical prothesis with the a portion of the human body;  the aortic valves, during their closure; and so on. Also some solid objects can be considered incompressible and this is another important characteristic that has to be taken into account. The displacement formulation, the standard one of solid mechanics, solves for the displacements, while the stresses are computed only a posteriori, as a derived variable, and so with a possible low quality approximation; such an approach also struggles in solving for incompressible solids. For this reason, here an approach that solves for both displacements and stresses, for compressible and incompressible materials, is presented.\\
Indeed this section is dedicated to the weak and strong formulation of contact problems with the Least Squares (LS) Finite Element Method (FEM).
\bibliographystyle{plain} 
\bibliography{biblio}
%\end{thebibliography}{9}
\end{document}
