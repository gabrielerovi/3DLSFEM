\documentclass[11pt, a4paper]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{algpseudocode}
\usepackage{algorithm}

% ref packages
\usepackage{nameref}
% folowing  must be in this order
\usepackage{varioref}
\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{booktabs}
\usepackage {tikz}
\usetikzlibrary {positioning}
\definecolor {processblue}{cmyk}{0.96,0,0,0}




\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{enumitem}   
\usepackage{subcaption}



\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}



\newcommand{\supp}{\operatorname{supp}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\divr}{\operatorname{div}}
\newcommand{\tdiv}{\operatorname{div}}
\newcommand{\ess}{\operatorname{ess}}
\newcommand{\rotore}{\operatorname{rot}}
\newcommand{\curl}{\operatorname{\textbf{curl}}}
\newcommand{\bcurl}{\operatorname{\textbf{curl}}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\bA}{\textbf{A}}
\newcommand{\ba}{\textbf{a}}
\newcommand{\bb}{\textbf{b}}
\newcommand{\bB}{\textbf{B}}
\newcommand{\bc}{\textbf{c}}
\newcommand{\bC}{\textbf{C}}
\newcommand{\bd}{\textbf{d}}
\newcommand{\bD}{\textbf{D}}
\newcommand{\be}{\textbf{e}}
\newcommand{\bE}{\textbf{E}}
\newcommand{\bff}{\textbf{f}}
\newcommand{\bF}{\textbf{F}}
\newcommand{\bg}{\textbf{g}}
\newcommand{\bG}{\textbf{G}}
\newcommand{\bi}{\textbf{i}}
\newcommand{\bI}{\textbf{I}}
\newcommand{\bj}{\textbf{j}}
\newcommand{\bJ}{\textbf{J}}
\newcommand{\bh}{\textbf{h}}
\newcommand{\bH}{\textbf{H}}
\newcommand{\bk}{\textbf{k}}
\newcommand{\bK}{\textbf{K}}
\newcommand{\bl}{\textbf{l}}
\newcommand{\bL}{\textbf{L}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\bLambda}{\boldsymbol{\Lambda}}
\newcommand{\bm}{\textbf{m}}
\newcommand{\bM}{\textbf{M}}
\newcommand{\bn}{\textbf{n}}
\newcommand{\bN}{\textbf{N}}
\newcommand{\bp}{\textbf{p}}
\newcommand{\bP}{\textbf{P}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\bPi}{\boldsymbol{\Pi}}
\newcommand{\bo}{\textbf{o}}
\newcommand{\bq}{\textbf{q}}
\newcommand{\bQ}{\textbf{Q}}
\newcommand{\br}{\textbf{r}}
\newcommand{\bR}{\textbf{R}}
\newcommand{\bs}{\textbf{s}}
\newcommand{\bS}{\textbf{S}}
\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\bt}{\textbf{t}}
\newcommand{\bv}{\textbf{v}}
\newcommand{\bV}{\textbf{V}}
\newcommand{\bw}{\textbf{w}}
\newcommand{\bW}{\textbf{W}}
\newcommand{\bu}{\textbf{u}}
\newcommand{\bU}{\textbf{U}}
\newcommand{\by}{\textbf{y}}
\newcommand{\bY}{\textbf{Y}}
\newcommand{\bbx}{\textbf{x}}
\newcommand{\bX}{\textbf{X}}
\newcommand{\bz}{\textbf{z}}
\newcommand{\bZ}{\textbf{Z}}
\newcommand{\beps}{\boldsymbol{\varepsilon}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\aaa}{\`a}
\newcommand{\eee}{\`e}
\newcommand{\iii}{\`i}
\newcommand{\ooo}{\`o}
\newcommand{\uuu}{\`u}
\newcommand{\aaaa}{\'a}
\newcommand{\eeee}{\'e}
\newcommand{\iiii}{\'i}
\newcommand{\oooo}{\'o}
\newcommand{\uuuu}{\'u}
\newcommand{\AAA}{\`A}
\newcommand{\EEE}{\`E}
\newcommand{\III}{\`I}
\newcommand{\OOO}{\`O}
\newcommand{\UUU}{\`U}
\newcommand{\AAAA}{\'A}
\newcommand{\EEEE}{\'E}
\newcommand{\IIII}{\'I}
\newcommand{\OOOO}{\'O}
\newcommand{\UUUU}{\'U}
\newcommand{\ND}{\mathcal{ND}}
\newcommand{\RT}{\mathcal{RT}}

%% argmin argmax
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


%%% theorems
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{remark}{Remark}
\renewcommand\qedsymbol{$\blacksquare$}

%%% norm and abs
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\abs[1]{\left\vert#1\right\vert}

%\date{}							% Activate to display a given date or no date

\title{Monotone multilevel for FOSLS linear elastic contact  }
\author[1]{Gabriele Rovi\thanks{rovig@usi.ch}}
\author[2]{Bernhard K\"{o}ber\thanks{
bernhard.kober@uni-due.de}}
\author[2]{Gerhard Starke\thanks{gerhard.starke@uni-due.de}}
\author[1]{Rolf Krause\thanks{rolf.krause@usi.ch}}

\affil[1]{Institute of Computational Science, Universit{\aaa}  della Svizzera italiana, Via Giuseppe Buffi 13, 6900, Lugano, Switzerland}
\affil[2]{Fakult\"{a}t  f\"{u}r Mathematik, Universit\"{a}t Duisburg-Essen, Thea-Leymann-Stra{\ss}e 9, 45127 Essen, Germany}

\begin{document}
\maketitle

\begin{abstract}
The first order system least-squares for linear ealstic contact problems is examined. The complementarity term is added to the functional, while the local inequality constraints are inserted in the definition of the convex set. A mixed formulation for both displacement and stress, subject to the contact conditions, is consequently obtained.  This holds for both compressible and incompressible materials. The problem is then discretized by continuous piecewise linear functions for the displacement and by the lowest order Raviart-Thomas for the stress. As a solver, a multilevel method is exploited and in particular, due to the local constraints, a monotone multilevel method.  A linear convergence rate in the limit case is finally shown in numerical experiments.
\end{abstract}
\section{Introduction}
The least-squares system of first order equations (FOSLS) for linear elasticity has been developed in \cite{CS04}, by introducing a functional which is the sum of the squared $L^2$ norms of the residuals of the equilibrium and constitutive equations (\cite{BMM+05}, \cite{Boc09}). In this way, displacement $\bu \in H^1(\Omega)$ and stress $\bsigma \in H(\tdiv,\Omega)$ are considered as independent,  giving rise to a mixed weak form. With respect to the standard primal displacement formulation, several are the advantages of this approach. First, the stress can be directly accessed, which can make the treatment of the elasto-plasticity (see \cite{Sta07}) and friction cases easier. Second, the Lam{\eeee} parameters are not restricted to a limited range of values: indeed, incompressible solids can be treated with no additional effort. Third, the least-squares functional is a reliable and efficient a posteriori error estimator. All these properties suggested the generalization of  \cite{CS04} to contact, as shown in  \cite{KMS17} for the Signorini's problem. The respective discretization is carried out by conforming finite element spaces: continuouse piecewise linear functions for the displacement and  Raviart-Thomas elements of the lowest order for the stress.\\
Different tecniques for solving the constrained optimisation problem, which describes the Signorini problem in the displacement formulation, have been proposed. A typical one is the projected Gau{\ss}-Seidel, whose behaviour unfortunately deteriorates by increasing the size of the problem. Therefore multilevel approaches  are preferred. Here we want to extend to the least-squares setting the monotone multilevel exposed in \cite{KK01}, \cite{KKS+08}, \cite{KRS12}, while for the other methods we refer the reader to the citations therein (for example \cite{BH93},  \cite{GM90}, \cite{Man84}). Peculiarly, the monotone multilevel aims to compute the solution of the discrete problem by adding to the current iterate fine and coarse corrections that actually minimize the energy functional. Effectively the framework of the least-squares linear elasticity for contact problem perfectly adapts to this case. Investigating this strategy is actually the main goal  of the present paper. Nevertheless the primal and the dual variables belong to different spaces, i.e $\bu \in H^1(\Omega)$ and $\bsigma \in H(\tdiv,\Omega)$. And since $H^1 \subset H(\tdiv,\Omega)$, schemes applied to the primal case cannot be transferred straightforwardly to the mixed one. \\
The most important difference between $H(\tdiv,\Omega)$ and $H^1(\Omega)$ consists in the kernels of the divergence, which contains all divergence-free functions, and of the gradient, which consists only of constants. Between the two, the kernel of the gradient operator is smaller and its elements can be well represented on coarse meshes. On the other hand, the kernel of the divergence is very large and its functions can have large gradients, so that their representation on coarse meshes can be very poor. In order to circumvent this drawback, 
some variants of multilevel methods for $H(\tdiv,\Omega)$ have been studied, at least for the linear case. A general overview can be found in \cite{XCN09}.  In \cite{Hip97} a geometric multigrid has been proposed. Then in \cite{HX07} and \cite{Vas12} a more general framework for dealing algebraic and geometric multigrid has been developed. All these approaches are based on the Helmoltz decomposition, which is a tool used not only in theory but also in the implementation. In particular, the different components in the Helmoltz decomposition can be expressed as functions of certain potentials, which are tackled separately. To this aim, various projections into the potential spaces are needed. However in this paper we take advantage of the work proposed in \cite{AW97},~\cite{AFW00},~\cite{Arn}. In this way, no potential space has to be considered, although a proper patch smoother is required.
Moreover this smoother can be extended to the least-squares formulation as proposed in \cite{Sta00}. In this way, we tackle all together not only the different components of the Helmoltz decomposition for the stress, but also the displacement. The price to pay is the solution of local problems which can be larger with respect to the standard ones. \textbf{Nevertheless, high algorithmic density can be exploited for accelerating the program.\\ if all the quantities needed for solving the local problem stay in the cache, then this could be not such a disadvantage.}\\
So far, to the authors' knowledge, only multilevel methods for linear FOSLS problems have been discussed. Nevertheless the Signorini's problem is non-linear. Of course by exploiting the active set method, for each arising linear problem, a linear multilevel method could be used. However in this way the non-linearities would not appear into the multilevel cycles, but only in the external active set. The main advantage of a monotone multilevel method is that it is able to deal with constraints inside the multilevel cycle itself. In the primal case, it has been shown in \cite{Kor94} that, when the number of iterations $k\to \infty$, all the active degrees of freedom are detected, the inequality constraints become equalities and the overall problem is reduced to a linear one. In such a situation, a linear convergence rate can be shown. Altough this formula is independent of the meshwidth, it depends on the number of levels $J$ of the multilevel method. Some years later (see \cite{Bad02}, \cite{Bad14}, \cite{BK12})  the same kind of behaviour has been prooved for all the iterations, and not only in the limit case. \\
In this paper the primal case is generalized to the  FOSLS for contact problems and some numerical experiments will be carried out to show that the linear rate is meaningful also in this situation.
The article is organized in the following way. In the second section, we introduce the problem. In the third one, existence and uniqueness of the solution are shown. In the fourth section a monotone multilevel strategy is proposed, while in the fifth one some non-linear projection operators for the constraints are introduced. In the sixth section, proper truncated multilevel basis are discussed. Finally in the last section some numerical experiments are presented.
\section{Definition of the problem }
In this section the strong formulations of linear elasticity and linear elastic contact for dimension $d=2,3$ are introduced . Different weak formulations are discussed, with their advantages and disadvantages. The main focus of this paper regards the FOSLS formulation for contact. Therefore the LS functional for linear elasticity and the augmented variant for contact problems are defined.\\
 Let $\Omega$ be an open, bounded, connected subset of $\mathbb{R}^d$, where $d=2,3$ is the dimension of the problem. The boundary $\partial \Omega$, Lipschitz and continuous, is the union of two open disjoint subsets $\partial \Omega= \Gamma_D \cup \Gamma_N$, with  $\Gamma_D \neq \emptyset$ and  $\Gamma_D \cap\Gamma_N = \emptyset$. Then let $\bff=(f_1,...,f_d)^T$ be the body force, $\bu=(u_1,...,u_d)^T$ the displacement field,  $\bsigma=(\sigma_{ij})_{d \times d}$ the stress tensor. By bold symbols we denote vectors or tensors. We use bold letters for The \textit{strong formulation of linear elasticity} is the following:\textit{ find $\bu$, $\bsigma$ such that}:
\begin{align*}
\begin{cases}
\text{div} \bsigma + \bff=0 & \Omega  \qquad \text{momentum balance equation}\\
\boldsymbol{\mathcal{A}} \bsigma - \boldsymbol{\varepsilon}(\bu)=0 &\Omega \qquad \text{constitutive law}\\
\bu = \bu^D & \Gamma_D\qquad \text{Dirichlet BC}\\
\bsigma  \bn = \bt^N & \Gamma_N\qquad \text{Neumann BC}\\
\end{cases} 
\end{align*}
where the linearized strain tensor $\boldsymbol{\varepsilon}(\bu)= \text{sym} (\nabla \bu)$ is the symmetric part of the displacement gradient, $\boldsymbol{\mathcal{A}}=\dfrac{1}{2 \mu} \left(\bsigma-\dfrac{\lambda}{d \lambda + 2 \mu } \text{tr} \bsigma \bI\right)$ is the compliance tensor with $\text{tr}$, $d$, $\lambda$ and $\mu$ denoting respectively the trace operator, the dimension of the problem and the Lam\eeee${}$ parameters.\\
Now let $\Gamma_C$ be the contact boundary such that $ \partial \Omega=\Gamma_C \cup  \Gamma_D \cup  \Gamma_N$, $\Gamma_i \cap  \Gamma_j =\emptyset$ for $i,j=D,N,C, i \neq j $, and $\Gamma_D \neq \emptyset$. Then, by adding the following constraints:
\begin{align*}
\begin{cases}
\bu \cdot \bn_o - g  \leq 0 & \Gamma_C \qquad \text{impenetrability}\\
(\bsigma \bn) \cdot \bn_o \leq 0 &\Gamma_C \qquad \text{direction of the surface pressure}\\
 \left(\bu \cdot \bn_o -g \right) \left( (\bsigma \bn) \cdot \bn_o \right) =0 & \Gamma_C \qquad \text{complementarity condition}
  \\
  (\bsigma \bn) \cdot \bt_o = 0 &\Gamma_C \qquad  \text{frictionless condition}
\end{cases}
\end{align*}
the strong formulation of contact for linear elasticity is finally obtained. Here $\bn$ represents the outward normal of the body, while $\bn_o$ and $\bt_o$ respectively represent the normal and the tangent vectors of the obstacle. The gap function $g$ is instead the distance in the normal direction between the obstacle and the body. Here the first condition means that no penetration can occur between the body and the obstacle. The second condition implies that, whenever contact forces arise, they have to be of compression and no adhesion is permitted. The third condition is a classic complementarity condition of the first two. 
The last one states that only normal stresses can arise.
Finally, by confusing the normal and the tangent vectors of the obstacle with the ones of the body, i.e. $\bn \approx\bn_o$ and $\bt \approx \bt_o$, the linearized contact formulation for linear elasticity is recovered (see \cite{Kik88}). \\
In general, from the strong formulation of the contact linear elasticity, different variants of weak forms can be derived. In the following, a list of motivations that retraces the one in \cite{CS04} is presented. By substituing the constitutive equation ($\bsigma = \mathcal{C}\boldsymbol{\varepsilon}(\bu)$, with $\mathcal{C}=\boldsymbol{\mathcal{A}}^{-1}$ the elasticity tensor) into the momentum balance one, the displacement formulation is consequently obtained. The displacement $\bu$, belonging to $ H^1(\Omega)$, is the only unknown and the stress $\bsigma$, belonging only to $L^2(\Omega)$, is derived \textit{a posteriori} and cannot be carefully approximated. Furthermore locking phenomena can arise for incompressible or nearly incompressible solids ($\lambda \gg 1$ or $\lambda \to \infty$).\\
To achieve a better approximation of the stress, the mixed formulation by Hellinger-Reissner can be used (\cite{BF12}). Given the energy-functional $\mathcal{J}(\bu,\bsigma)=\frac{1}{2}\left(\boldsymbol{\mathcal{A}} \bsigma, \bsigma \right) + \left(\nabla \cdot \bsigma + \bff, \bu \right)  $, both displacement and stress $(\bu,\bsigma)$ are unknowns of the problem, respectively belonging to $  L^2(\Omega)^d  \times  H_{\text{div},S}(\Omega)^d$ , where $H_{\text{div},S}(\Omega)^d$ is the space of symmetric tensors in $H_{\text{div}}(\Omega)$. In this case, in order to satisfy the inf-sup condition in the discrete setting, a stable combination of finite element spaces is needed. Although such spaces have been built (\cite{CS04}), the number of degrees of freedom they require is very large. Furthermore the corresponding linear system is a saddle point problem, that in general is difficult to solve.\\
The approach that is here presented is based on the LS principle (\cite{BMM97}, \cite{Boc09}, \cite{YL97}). The main idea behind it is to build a fictitious functional as the weighted sum of the squared $L^2$-norms of the residual equations. Unlike the previous cases, now it is required more regularity on both variables: $\bu \in H^1(\Omega)$, $\bsigma \in H(\tdiv,\Omega)$. With respect to the Hellinger-Reissner formulation, the symmetry of the stress tensor is not demanded. Indeed, as it is shown in \cite{CS04}, $\norm{
\bsigma-\bsigma^T}\leq C \norm{\boldsymbol{\mathcal{A}}\bsigma -\boldsymbol{\varepsilon}(\bu)}$. Thus, by reducing the residual of the constitutive law, the asymmetry is reduced as well. Principally, spaces that would be useful for the analysis are the following: 
\small
\begin{align*}
&H_D^1(\Omega)=\left\lbrace \bv \in \left[ H^1(\Omega)\right]^d, \quad \bv|_{\Gamma_D}=\bu^D   \:\: \text{on} \: \Gamma_D \right\rbrace &&
H_{D,0}^1(\Omega)=\left\lbrace \bv \in \left[ H^1(\Omega)\right]^d, \quad \bv|_{\Gamma_D}=\textbf{0}    \:\:  \text{on} \: \Gamma_D \right\rbrace \\
&H_N(\tdiv,\Omega)=\left\lbrace \btau \in \left[ H(,\tdiv, \Omega)\right]^d, \quad \btau \bn |_{\Gamma_N}=\bt^N   \:\:  \text{on} \: \Gamma_N
\right\rbrace &&
H_{N,0}(\tdiv,\Omega)=\left\lbrace \btau \in \left[ H(,\tdiv, \Omega)\right]^d, \quad \btau \bn |_{\Gamma_N}=\textbf{0}
 \:\: \text{on} \: \Gamma_N \right\rbrace 
\end{align*}
\normalsize
Then, relying on the formulation given in \cite{KMS17}, we define the linear elasticity LS functional $\mathcal{F}$ and the corresponding augmented LS functional $\mathcal{J}$ for contact:
\begin{align}
\label{augmentedfunctional}
&\mathcal{F}(\bu,\bsigma;\bff)=C_{\text{eq}} ||\text{div} \bsigma+\bff||_{L^2(\Omega)^d}^2+C_{\text{const}}||\boldsymbol{\mathcal{A}}\bsigma -\boldsymbol{\varepsilon}(\bu)||_{L^2(\Omega)^d}^2 \\
&
\mathcal{J}(\bu,\bsigma;\bff,g)=\mathcal{F}(\bu,\bsigma;\bff)+C_{\text{compl}} \langle \bu \cdot \bn -g, (\bsigma \bn) \cdot \bn \rangle_{\Gamma_C}
\end{align}
So that the problem can be formulated in this way: find $(\bu,\bsigma)$ such that
 \begin{align}
 \begin{aligned}
 &(\bu,\bsigma)=\argmin \limits_{(\bu,\bsigma) \in K}\mathcal{J}(\bu,\bsigma;\bff,g)\\
 &K=
 \left\lbrace 
  \left(\bu,  \bsigma \right)  \in  
 H_D^1(\Omega)\times H_N(\tdiv,\Omega)  : \quad 
 \bu \cdot \bn - g  \leq 0, \:\:  (\bsigma \bn) \cdot \bn \leq 0, \:\: (\bsigma \bn) \cdot \bt =\textbf{0}  \:\: \text{on}  \: \: \Gamma_C
\right\rbrace 
\end{aligned}
\label{MinimizationProblem}
\end{align}
The complementarity condition on $\Gamma_C$ is a non-linear term. Defining the convex set $K$ by adding also this requirement would be cumbersome, at least from a computational perspective. On the other hand, augmenting the functional with this term seems a more natural choice. Of course in the discrete setting, due to the fact that we do not enforce it strongly and we just add it as a penalty addendum, the complementarity condition will be not fulfilled exactly and so will be only approximated. \\
The augmented functional $\mathcal{J}(\bu,\bsigma;\bff,g)$ is Gateaux-differentiable and strongly convex, as we will show. Therefore the problem (\ref{MinimizationProblem}) can be reformulated in the following way: find $(\bu,\bsigma) \in K$ such that $\forall (\bv,\btau) \in K$:
\small
\begin{align}
\label{LScontactVI}
\begin{cases}
&
\left\langle \dfrac{\partial \mathcal{J}(\bu,\bsigma;\bff,g)}{\partial \bu }, \bv-\bu \right\rangle =
-2 \left(\mathcal{\bsigma}-\boldsymbol{\varepsilon}(\bu), \boldsymbol{\varepsilon}(\bv-\bu) \right)+\langle \bn \cdot \left( \bsigma \bn\right), \bn \cdot (\bv -\bu)\rangle_{\Gamma_C}
\geq 0 \\\\
&
\left\langle \dfrac{\partial \mathcal{J}(\bu,\bsigma;\bff,g)}{\partial \bsigma }, \btau-\bsigma \right\rangle =
2\left(\text{div}\bsigma +\bff, \tdiv (\btau-\bsigma) \right)
+ 2 \left( 
\mathcal{\bsigma}-\boldsymbol{\varepsilon}(\bu),\mathcal{\btau-\bsigma}
\right) +\langle \bn \cdot \bu-g, \bn \cdot 
\left(\btau- \bsigma \right)\bn \rangle_{\Gamma_C}
\geq 0 
\end{cases}
\end{align}
\normalsize
\section{Existence and uniqueness of the solution}
Standard FOSLS functionals are simply quadratic. Thus they are also strongly convex and differentiable, so that the existence and uniqueness of the minimizer immediately follows. Here the inconvenience is represented by the complementarity term, which is not convex in general. Therefore it is not clear whether the whole functional is still convex or not and if such convexity can depend on the weights in front of each term. 
Actually, by showing some intermediate results, we can prove the strong convexity of the functional for non homogeneous boundary conditions which together with the continuity of the functional implies the existence and uniqueness of the solution $(\bu,\bsigma)$. For simplicity, we will assume $C_{\text{const}} = C_{\text{eq}}=C_{\text{compl}}=1$. For the general case, it is just sufficient to rescale the constants obtained. \\
Let us define the following norm $M(\bs,\bw): H_D^1(\Omega) \times H_N(\tdiv,\Omega) \to \mathbb{R}$:
\begin{align*}
&M(\bs,\bw)= \|  \boldsymbol{\varepsilon}(\bw) \|_{L^2} +  \| \btau \|_{H_{\tdiv}}^2 =  \|  \boldsymbol{\varepsilon}(\bw) \|_{L^2}^2 +  \| \bs \|_{L^2}^2 +  \| \tdiv \bs \|_{L^2}^2 
\end{align*}
Then we can prove:
\begin{lemma}
$\forall \bu, \bv \in H_D^1(\Omega)$, $\forall \bsigma, \btau \in H_N(\tdiv,\Omega)$, let $\bw=\bu-\bv$ and $\bs=\bsigma-\btau$. Then exist $C_1$ and $C_2$ such that:
\begin{align*}
M(\bs,\bw) C_1 \leq \mathcal{J}(\bw,\bs;0,0) \leq C_2 M(\bs,\bw)
\end{align*}
where:
\begin{align}
C_1= \left(\max \left(4,   \dfrac{  1 + 4 \mu + 10 \mu^2 + 16 \mu^3} {(1+2 \mu) \mu^2} \right) 
\right)^{-1}
\end{align}
\end{lemma}
\begin{proof}
The proof is similar to the theorem 3.1 in \cite{CS04}. Although the functional $\mathcal{J}$ is defined only with the $L^2$ norms of the residuals, here we also consider non-homogeneous boundary conditions and the contact boundary. \\
The upper bound can be easily shown by using $\|\boldsymbol{\mathcal{A}}\btau\| \leq \dfrac{1}{2 \mu} \| \btau \|$, the triangle, Young and trace inequalities. \\
In order to prove the inequality from below, it is sufficient to bound all the terms in $M$ with the functional. Knowing that $\|\boldsymbol{\mathcal{A}}\btau\| \leq \dfrac{1}{2 \mu} \| \btau \|$:
\begin{align*}
 \| \text{div}\bs\|_{L^2}^2  + \| \boldsymbol{\varepsilon}(\bw) \|^2 \leq  \| \text{div}\bs\|_{L^2}^2 +2 \| \boldsymbol{\varepsilon}(\bw) - \boldsymbol{\mathcal{A}}\bs\|^2 + 2 \| \boldsymbol{\mathcal{A}}\bs\|^2  \leq 2 \mathcal{F}(\bw,\bs;0) +  \dfrac{1}{2 \mu }\| \bs\|^2
\end{align*} 
Thus it is now sufficient to bound $\dfrac{2 \mu +1}{2 \mu} \| \bs \|_{L^2} $. 
We just exploit $(\boldsymbol{\mathcal{A}}\bs,\bs )\geq \dfrac{1}{2 \mu}  \| \bs \|^2$:
\begin{align*}
(\boldsymbol{\mathcal{A}}\bs,\bs ) &= (\boldsymbol{\mathcal{A}}\bs - \boldsymbol{\varepsilon}(\bw),\bs ) + (\boldsymbol{\varepsilon}(\bw),\bs)=  (\boldsymbol{\mathcal{A}}\bs - \boldsymbol{\varepsilon}(\bw),\bs ) + (\bs - \frac{1}{2}\left(\bs -\bs^T\right),\nabla \bw)\\
&=(\boldsymbol{\mathcal{A}}\bs - \boldsymbol{\varepsilon}(\bw),\bs )-(\text{div} \bs,\bw) +\int_{\partial \Omega} \bs \bn \cdot \bw - \left(\bs-\bs^T, \nabla \bw \right)\\ 
& \leq \|  \boldsymbol{\mathcal{A}}\bs - \boldsymbol{\varepsilon}(\bw) \| \| \bs\| +\|\text{div} \bs\| \| \bw \| +\dfrac{1}{2}\norm{{\bs-\bs^T}} \|\nabla \bw \|+\int_{\partial \Omega} \bs \bn \cdot \bw \\
& \leq \mu \norm{  \boldsymbol{\mathcal{A}}\bs - \boldsymbol{\varepsilon}(\bw) }^2+    \dfrac{1}{4 \mu} \| \bs\|^2
+  \|\boldsymbol{\varepsilon}(\bw) \| \left( \|\text{div} \bs\| +\dfrac{1}{2} \norm{\bs-\bs^T}\right)+\int_{\partial \Omega} \bs \bn \cdot \bw 
\end{align*}
And by using $\| \bs -\bs^T\| \leq 4 \mu \|\boldsymbol{\mathcal{A}}\bs - \boldsymbol{\varepsilon}(\bw) \|$, $\| \bs \|^2 \leq 2 \mu (\boldsymbol{\mathcal{A}} \bs, \bs)$, 
$\| \boldsymbol{\mathcal{A}}\bs - \boldsymbol{\varepsilon}(\bw)\| \leq  \mathcal{F}(\bw,\bs;0)^{1/2}$, $\| \bs \| \leq  \mathcal{F}(\bw,\bs;0)^{1/2}$, $\| \bs -\bs^T\| \leq 4 \mu \|\boldsymbol{\mathcal{A}}\bs - \boldsymbol{\varepsilon}(\bw) \|$:
\begin{align*}
(\boldsymbol{\mathcal{A}}\bs,\bs ) & \leq 2 \mu \|  \boldsymbol{\mathcal{A}}\bs - \boldsymbol{\varepsilon}(\bw) \|^2+ 2 \left( 1+ 2 \mu \right) \mathcal{F}(\bw,\bs;0)^{1/2}  \|\boldsymbol{\varepsilon}(\bw) \| 
+\int_{\partial \Omega} \bs \bn \cdot \bw 
\end{align*}
Now it is exploited again $ \|  \boldsymbol{\mathcal{A}}\bs - \boldsymbol{\varepsilon}(\bw) \|^2\leq \mathcal{F}(\bw,\bs;0) $ and $ \| \boldsymbol{\varepsilon}(\bw) \|  \leq  \mathcal{F}(\bw,\bs;0)^{1/2} + \| \boldsymbol{\mathcal{A}} \bs\| \leq \mathcal{F}(\bw,\bs;0)^{1/2} +  \dfrac{1}{2 \mu }\| \bs\|$, so that:
\begin{align*}
(\boldsymbol{\mathcal{A}}\bs,\bs ) & \leq (2 +6 \mu)\mathcal{F}(\bw,\bs;0) 
+\dfrac{\left( 1+ 2 \mu \right) }{\mu}\mathcal{F}(\bw,\bs;0)^{1/2}  \| \bs \| 
+2 \int_{\partial \Omega} \bs \bn \cdot \bw 
\end{align*}
By Young's inequality:
\begin{align*}
(\boldsymbol{\mathcal{A}}\bs,\bs ) & \leq 2 (2 +6 \mu)\mathcal{F}(\bw,\bs;0) 
+2 \dfrac{ \left( 1+ 2 \mu \right)^2 }{2 \mu^2}\mathcal{F}(\bw,\bs;0)
+4 \int_{\partial \Omega} \bs \bn \cdot \bw\\
& \leq \dfrac{4 \mu^2 + 12 \mu^3 + 1 + 4 \mu^2 + 4 \mu}{\mu^2} \mathcal{F}(\bw,\bs;0) +4 \int_{\partial \Omega} \bs \bn \cdot \bw  
\end{align*}
By rearranging the terms:
\begin{align*}
M(\bs,\bw)&=  \|  \boldsymbol{\varepsilon}(\bw) \|_{L^2}^2 +  \| \bs \|_{L^2}^2 +  \| \text{div} \bs \|_{L^2}^2\\
& \leq 2 \mathcal{F}(\bw,\bs;0) +  \dfrac{4 \mu^2 + 12 \mu^3 + 1 + 4 \mu^2 + 4 \mu}{(1+2 \mu) \mu^2}  \mathcal{F}(\bw,\bs;0)  +4 \int_{\partial \Omega} \bs \bn \cdot \bw \\
&\leq \dfrac{1}{C_1} \left( \mathcal{F}(\bw,\bs;0) + \int_{\partial \Omega} \bs \bn \cdot \bw\right) \\
&=\dfrac{1}{C_1} \left( \mathcal{F}(\bw,\bs;0) + \langle \bs \bn \cdot \bn,  \bw \cdot \bn \rangle_{\Gamma_C}\right) 
\end{align*} 
where we have exploited $\bw|_{\Gamma_D}= \textbf{0}$, $\bs \bn|_{\Gamma_N} =\textbf{0}$, $\left(\bs \bn\right)\cdot \bt=0$
\end{proof}
\begin{lemma}
The augmented LS functional is strongly convex.
\end{lemma}
\begin{proof}
The functional $\mathcal{F}(\bu,\bsigma;\bff) $ is convex, but the complementarity term is not. However the whole functional $\mathcal{G}(\bu,\bsigma,
\bff,g) $ is strongly convex:
\small
\begin{align*}
\mathcal{J}( t \bu + (1-t) \bv, t \bsigma + (1-t) \btau; \bff,g)
&= t \mathcal{J}( \bu ,\bsigma ; \bff,g) + (1-t)\mathcal{J}(  \bv,  \btau; \bff,g)+ t(t-1) \mathcal{J}(\bu-\bv,\bsigma-\btau;0,0)\\
& \leq t  \mathcal{J}( \bu ,\bsigma ; \bff,g)+ (1-t) \mathcal{J}( \bv ,\btau ;\bff,g)+ t(t-1)C_1M(\bu-\bv,\bsigma-\btau)
\end{align*}
\normalsize
where the last inequality holds due to the previous result. 
\end{proof}
\begin{corollary}
The augmented LS functional is coercive.
\end{corollary}
\begin{proof}
Consider the inequality from the previous lemma. It is known that $\mathcal{J}(   t \bu + (1-t) \bv, t \bsigma + (1-t) \btau; \bff,g)\geq0$ $\forall \bsigma, \btau, \bu, \bv$ in the adimissible set. Then fix $t \in (0,1)$, choose $\bv$, $\btau$ as the minimizer of the problem, so that $\mathcal{J}(  \bv,  \btau, \bff,g)=0$, and let $\norm{\bu}_{H^1(\Omega)}\to \infty$, $\norm{\bsigma}_{H^N(\tdiv,\Omega)} \to \infty$:
\small
\begin{align*}
\dfrac{1}{C_1(1-t)}\mathcal{J}( \bu ,\bsigma , \bff,g)& \geq  M(\bu-\bv,\bsigma-\btau) =\\
&( \|  \boldsymbol{\varepsilon}(\bu ) \|_{L^2}^2  +  \|  \boldsymbol{\varepsilon}(\bv ) \|_{L^2}^2 -2 \| \boldsymbol{\varepsilon}(\bu )\|_{L^2} \| \boldsymbol{\varepsilon}(\bv )\|_{L^2} +
  \| \btau \|_{H_{div}}^2+ \| \bsigma \|_{H_{div}}^2 - 2  \| \btau \|_{H_{div}}  \| \bsigma \|_{H_{div}}) \to \infty
\end{align*}
\normalsize
The right handside is a quadratic expression in $ \| \bsigma \|_{H_{div}}$ and $\|  \boldsymbol{\varepsilon}(\bu ) \|_{L^2}$. By the generalized Korn's inequality, if $\norm{\bu}_{H^1(\Omega)}\to \infty$, also $\norm{\boldsymbol{\varepsilon}(\bu)}_{L^2(\Omega)}\to \infty$.
\end{proof}
\begin{lemma}
It exists a unique minimizer $(\bu,\bsigma) \in K$ of the augmented LS functional $ \mathcal{J}( \bu ,\bsigma , \bff,g)$.
\end{lemma}
\begin{proof}
The proof follows from strong convexity, which implies strict convexity and coercivity, and from continuity of the functional, which implies the lower semicontinuity (see \cite{ET99}, chapter II, proposition 1.2) .
\end{proof}

\section{Monotone Multilevel}
Using a direct solver for a large sparse system can be very demanding. Therefore iterative solvers are necessary. Nevertheless the rate of convergence of standard projected Gau{\ss}-Seidel deteriorates by reducing the meshwidth. Furthermore the behaviour gets worse with the increasing of the condition number. The FOSLS system gives rise to normal equations, i.e. a symmetric positive linear system, whose condition number can, unfortunately, almost double the one of the original problem. All these reasons suggest to adopt a multilevel method. In particular, due to the local non-linearities, we opt for a monotone multilevel. In this section we introduce nested subspaces for displacement and stress, together with the corresponding interpolation operators. Indeed these ones are the main ingredients for a multilevel method. However, for the sake of simplicity, for the degrees of freedom on $\Gamma_C$, a proper basis transformation from the canonical coordinate system to the normal tangential coordinate system is thereafter exploited. All the relative quantities will be then represented in this new setting. \\
Let $\mathcal{T}_1$ be a partition of $\Omega$ into finite elements $\tau$ (triangles in 2D or tetrahedra in 3D), with meshwidth parameter $h_1=\max_{\tau \in \mathcal{T}_1}\text{diam}(\tau)$ and $\Gamma_{C,1}=\mathcal{T}_1|_{\Gamma_C}$ . Then recursively, for $j=2,...,J$,  define $\mathcal{T}_j$, with the corresponding $h_j$ and $\Gamma_{C,j}$, as the uniform refinement  of $\mathcal{T}_{j-1}$. We also denote by $\mathcal{N}_j$, $\mathcal{E}_j$, $\mathcal{F}_j$ the sets of vertices, edges and faces of the mesh $\mathcal{T}_j$, and with $N_j$, $E_j$, $F_j$ their cardinality. Furthermore, let us consider two levels $j$, $k$ and the corresponding vertices $\nu_j$, $\nu_k$, edges $\varepsilon_j$, $\varepsilon_k$, faces $\phi_j$, $\phi_k$ and elements $\tau_j$, $\tau_k$. We define the $\bigtriangleup$-patch of $\#$ as: 
\begin{align*}
P_{\#}^{\bigtriangleup}=
\left\lbrace
\bigtriangleup: \:
\# \in \bigtriangleup \
    \right\rbrace
    \qquad \#=\nu_j,\varepsilon_j,\phi_j,\tau_j, \quad \bigtriangleup =\nu_k,\varepsilon_k,\phi_k,\tau_k
\end{align*}
Now let $P^1(\mathcal{T}_j)$  and $\text{RT}_0(\mathcal{T}_j)$ be respectively the continuous piecewise linear functions space and the lowest order Raviart-Thomas space defined on the tassellation $\mathcal{T}_j$. If $\bn_{\phi_{j+1}}$ is the normal related to the face $\phi_{j+1}$, the interpolation operators for nested meshes from level $j$ to level $j+1$ are defined as follows:
\begin{align*}
&P_j^{j+1}: P^1(\mathcal{T}_j) \to P^1(\mathcal{T}_{j+1})
\qquad
 (P_j^{j+1} u - u )|_{\nu_{j+1}} =0\qquad \forall \nu_{j+1} \in \mathcal{N}_{j+1}, \:\: \forall u \in P^1(\mathcal{T}_j)
\\
&\Pi_j^{j+1}: \text{RT}_0(\mathcal{T}_j) \to \text{RT}_0(\mathcal{T}_{j+1}) \qquad
\int_{\phi_{j+1}} (\Pi_j^{j+1} \bt - \bt) \bn_{j+1}  ds =0\qquad \forall \phi_{j+1} \in \mathcal{F}_{j+1}, \:\: \forall \bt \in  \text{RT}_0(\mathcal{T}_j)
\end{align*}
For $j=1,...,J$ we also define the following finite element spaces:
\begin{align*}
& U_j=\left\lbrace \bu \in \left[
P^1(\mathcal{T}_j) \right]^d
  \right\rbrace \qquad
\Sigma_j=\left\lbrace \bsigma \in \left[
\text{RT}_0(\mathcal{T}_j)\right]^d
  \right\rbrace
    \qquad
  X_j =U_j \times \Sigma_j
\end{align*}
whose basis functions belong to $\Lambda_{U_j}$, $\Lambda_{\Sigma_j}$, $\Lambda_{X_j}$. The $P^1$ fand RT$_0$ functions are defined respectively on each node $\nu \in \mathcal{N}_j$ and face $\phi \in \mathcal{F}_j$. Thus:
\begin{align*}
\Lambda_{U_j}
=
\left\lbrace
\blambda_{j,\nu}, \:\: \nu \in \mathcal{N}_j 
  \right\rbrace ,
  \quad
  \Lambda_{\Sigma_j}
=
\left\lbrace
\blambda_{j,\phi}, \:\: \phi \in \mathcal{F}_j 
  \right\rbrace ,
  \quad 
  \Lambda_{X_j}=
  \Lambda_{U_j} \cup   \Lambda_{\Sigma_j}
\end{align*}
We can also build the multi-dimensional interpolation operators:
\begin{align}
\label{interpolations}
&\bP_j^{j+1}=\left[ P_j^{j+1} \right]^d : \left[P^1(\mathcal{T}_j)\right]^d \to \left[P^1(\mathcal{T}_{j+1})\right]^d
\qquad
\bPi_j^{j+1}=\left[ \Pi_j^{j+1}\right]^d 
: \left[\text{RT}_0(\mathcal{T}_j)\right]^d \to \left[\text{RT}_0(\mathcal{T}_{j+1})\right]^d 
\end{align}
Moreover in the finite element framework now introduced, it is also possible to consider the discretization of functions of the normal components in  $\left[H^{1/2}(\Gamma_C)\right]^d$ and $\left[H^{-1/2}(\Gamma_C)\right]^d$. The first space is $U_{j,n}$ and consists of continuous functions which are linear on each face $\phi_j \in\Gamma_{C,j}$. The second space is $\Sigma_{j,n}$ and consists of piecewise discontinuous functions which are constant on each face $\phi_j \in \Gamma_{C,j}$. Also for these spaces we can define interpolation operators for nested meshes:
\begin{align}
\label{normalinterpolations}
P_{j,n}^{j+1}:U_{j,n} \to U_{j+1,n} \qquad \qquad
\Pi_{j,n}^{j+1}:\Sigma_{j,n} \to \Sigma_{j+1,n}
\end{align}
Furthermore we denote the convex set as:
\small
\begin{align*}
K_J=\left\lbrace
  \bbx_J=(\bu_J, \bsigma_J) \in X_J: \quad \bu_J|_{\Gamma_D}=\bu_J^D , \:  \bsigma_J|_{\Gamma_N}=\bt_J^N, \: \bu_J\cdot \bn_J|_{\Gamma_C}\leq g_J, \: \bn^T(\bsigma_J \bn)  \leq 0, \: \bt_J^T(\bsigma \bn_J) =0
  \right\rbrace 
\end{align*}
\normalsize
where $\bu_J^D$, $\bt_J^N$, $\bn_J$, $g_J$ are suitable approximations in the discrete finite element space and geometry of $\bu^D$, $\bt^N$, $\bn$ and $g$. Of course, due to this approximation, in general $K_J \nsubseteq K$. Moreover let $\bff_J$ be an approximation of $\bff$. We can write $\mathcal{J}_J(\bbx_J)=\mathcal{J}_J(\bu_J,\bsigma_J;\bff,g)=\mathcal{J}(\bu_J,\bsigma_J;\bff_J,g_J)$. The discrete minimization problem is then: find $(\bu_J,\bsigma_J) \in K_J$ such that:
\begin{align}
\label{discreteminimizationproblem}
\mathcal{J}_J(\bu_J,\bsigma_J;\bff_J,g_J)  \leq \mathcal{J}_J(\bv_J,\btau_J;\bff_J,g_J) \quad \forall \: (\bv_J,\btau_J) \in K_J
\end{align}
Whenever it will be no cause of misuderstanding, the subscript $J$ will be omitted from the functional and the unknowns. \\\\
The standard projected Gau{\ss}-Seidel successively minimizes the functional $\mathcal{J}(\bbx)$ in the directions $\lambda_J \in \Lambda_{X_J}$. However the rate of convergence of this method deteriorates for $h_J \to 0$. Such an inconvenient promoted the analysis of multilevel methods that involve coarse grid corrections as well. Specifically, the monotone multilevel idea is to extend the minimization process also to low frequency components of the spectrum. Therefore  $\mathcal{J}$ is minimized with respect to all $\lambda_j \in \Lambda_{X_j}$, for $j=1,...,J$.\\
A prerequisite of multilevel methods is that eigenfunctions associated to small eigenvalues can be well represented on coarse meshes. This is automatically satisfied in $H^1$, since the kernel of the gradient operator boils down to costants functions. However the kernel of the divergence operator is very large. All free-divergence functions, also the ones with a large gradient, are admissible. To circumvent this drawback, different strategies have been proposed (\cite{Hip97}, \cite{HX07}). Here we will focus on the one described in \cite{AW97},~\cite{AFW00},~\cite{Arn}, but since the present is a mixed formulation, an extension to primal and dual variables as in \cite{Sta00} is carried out. For each vertex $\nu \in \mathcal{T}_j$, we define the patch $P_{\nu}^{\tau_j}$ as the union of all the elements $\tau_j$ of the same mesh that share the vertex $\nu \in \mathcal{N}_j$. Then the degrees of freedom of interest are the vertex itself $\nu$ for the displacement and all the internal faces $P_{\nu}^{\phi_j}$ for the stress. We denote by $\lambda_{j,\nu}$ the collection of these degrees of freedom. As usual, on the coarser level $j=1$, the direction is the whole space. 
\begin{align*}
\displaystyle
&
 \Lambda_{j,\nu}=
  \left\lbrace   
\blambda_{j,\nu}=
   \left\lbrace
  \blambda_{U_j, \nu} \in \Lambda_{U_j}
    \right\rbrace  
  \cup
  \left\lbrace
\blambda_{\Sigma_j, \phi} \in \Lambda_{\Sigma_j}, \: \phi \in \mathcal{F}_j \cap  \in P_{\nu}^{\phi_j}
  \right\rbrace  ,
  \quad
  \nu \in \mathcal{N}_j 
    \right\rbrace && j=2,...,J,\:\:\\
  &
  \blambda_{1} =   \left\lbrace
\bigcup\limits_{\nu \in \mathcal{N}_1}
\{
\blambda_{U_1, \nu} \in \Lambda_{U_1} \}
  \right\rbrace 
  \cup
    \left\lbrace
  \bigcup\limits_{\phi \in \mathcal{F}_1}
  \{
\blambda_{\Sigma_1, \phi} \in \Lambda_{\Sigma_j}
\}
  \right\rbrace   && j=1
%&\blambda_{j,\nu} \in \Lambda_{j,\nu}=
%\begin{cases}
%\blambda_{U_j, \nu} \in \Lambda_{U_j}
%  \cup
%  \left\lbrace
%\blambda_{\Sigma_j, \phi} \in \Lambda_{\Sigma_j}, \: \phi \in \mathcal{F}_j \cap  \in P_{\nu}^{\phi_j}
%  \right\rbrace   & j=2,...,J\\
%    \left\lbrace
%\bigcup\limits_{\nu \in \mathcal{N}_1}
%\{
%\blambda_{U_1, \nu} \in \Lambda_{U_1} \}
%  \right\rbrace 
%  \cup
%    \left\lbrace
%  \bigcup\limits_{\phi \in \mathcal{F}_1}
%  \{
%\blambda_{\Sigma_1, \phi} \in \Lambda_{\Sigma_j}
%\}
%  \right\rbrace   & j=1
%  \end{cases}
\end{align*}
Consequently $\mathcal{J}$ has to be minimized with respect to $\lambda_{j,\nu} \in \Lambda_{j,\nu}$, for $j=J,...,2$ and $\nu=1,...,N_j$, and $\blambda_1$. Since the functional is strongly convex and differentiable, the discrete minimization problem (\ref{discreteminimizationproblem}) can be reformulated as the variational inequality (\ref{LScontactVI}). Let $\bbx_{J,k} \in K_J$ be the $k$-th iterate. Then we define $\bbx_{J,0}=\bbx_{J}^k$ and $\bbx_{j,0}=\bbx_{j+1,N_{j+1}}$, for $j=J-1,...,1$. We compute a sequence of intermediate iterates $\bbx_{j,\nu} =\bbx_{j,\nu-1}+\bc_{j,\nu}$ by solving:
\begin{align}
\label{exactlocalproblem}
&\mathcal{J}(\bbx_{j,\nu}+\bc_{j,\nu}) \leq \mathcal{J}(\bbx_{j,\nu}+\by) \quad \forall \by \in K_{j,\nu}^{*} \qquad j=J,...,2, \quad \nu=1,...,N_j\\
&\mathcal{J}(\bbx_{2,N_2}+\bc_{1}) \leq \mathcal{J}(\bbx_{2,N_2}+\by) \quad \forall \by \in K_{1}^{*} \qquad j=1
\end{align}
where the local closed convex sets $K_{j,\nu}$  and $K_1^*$ are defined as follows:
\begin{align}
\label{exactlocalconvexset}
\begin{aligned}
&K_{j,\nu}^{*}(\bbx_{j,\nu})&=\left\lbrace
\by \in \text{span}\{\blambda_{j,\nu}\}: \quad \by +\bbx_{j,\nu} \in K_J
  \right\rbrace  \\
  &K_{1}^{*}(\bbx_{2,N_2})&=\left\lbrace
\by \in \text{span}\{\blambda_{1}\}: \quad \by +\bbx_{2,N_2} \in K_J
  \right\rbrace 
  \end{aligned}
\end{align}
In order to compute the solution of these local problems,  a comparison with the constraints on the fine level is needed. However evaluating quantities which live on the finer meshes can lead to algorithms with suboptimal complexity. To recover an optimal complexity, only an approximate solution, instead of the exact one,  can be taken into consideration for coarser levels. To this aim, we define approximate convex sets $K_j$ and, thus, proper coarse constraints which will depend on the current iterate and on the corrections on the higher levels. Two specific non-linear projections, one for the normal displacement and the other for the pressure, will be later investigated. 
\begin{remark}[] In minimizing the functional $\mathcal{J}$ along the directions $\lambda_{j,\nu}$, the order has been chosen in this way: $j=J,...,1$ and, for a fixed level $j$, from $\nu=1$ to $\nu=N_j$. This scheme corresponds only to a pre-smoothing plus an active set method on the coarsest level. Anyhow, after this pre-smoothing, the order can be inverted again, i.e. $j=2,...,J$, $\nu=N_j,...,1$, so that a post-smoothing with a overall symmetric cycle is recovered. For simplicity of notation, only a pre-smoothing is presented in the analysis.
\end{remark}
In order to properly describe contact conditions also on coarser levels, it is wise to locally change the coordinate system of the contact boundary $\Gamma_C$. In this way, the scalar constraints have to be checked directly on the normal components and not on some linear combinations of the unknown.
Let $\nu \in \mathcal{N}_j  \cap \Gamma_{C,j}$ and $\bn_{\nu}$ be the obstacle normal in $\nu$. Then consider the vector $ \bu_{\nu} \in \mathbb{R}^d$ that contains the degrees of freedom of the displacement in $\nu$. Define the Householder transformation $\bH_{\nu}$ relative to the ouward normal $\bn_{\nu} $ and the local displacement in the normal tangential coordinate system $\bu_{\nu,nt}$ (the first coordinate is the normal one) as:
\begin{align*}
\bu_{\nu,nt}= \bH_{\nu} \bu_{\nu} 
\qquad \qquad 
\bH_{\nu}= \bI- 2\: \bn_{\nu} ^T \bn_{\nu} 
\end{align*}
A similar argument has to be applied to the stress components. For each face $\phi \in \Gamma_{C,j}$, we can express the vector unknown $\bSigma_{\phi}$ in terms of the normal and tangent forces $\bSigma_{\phi,nt}$. It is not convenient to use direclty the HouseHolder transformation $\bH_{\phi}$ relative to the face normale $\bn_{\phi}$, because we have no control on the sign of $\left( \blambda_{j,\phi}  \cdot \bn_{\phi}  \right)$. In its place, it is preferrable the transformation $\bQ_{\phi}$:
\begin{align*}
\bsigma \bn_{\phi} = \left( \bphi_{\phi} \cdot \bn_{\phi} \right) \bSigma_{\phi} = \bH_{\phi} \bSigma_{\phi,nt} \quad
 \iff \quad \bSigma_{\phi} = \dfrac{1}{  \left( \bphi_{\phi} \cdot \bn_{\phi} \right)}\bH_{\phi} \bSigma_{\phi,nt} = \bQ_{\phi} \bSigma_{\phi,nt}
\end{align*}
In this way the first component of $\bu_{\nu,nt}$/$\bSigma_{\phi,nt}$ is actually positive in the direction of the normal $\bn_{\nu}$/$\bn_{\phi}$, with $\bH_{\nu}$/$\bQ_{\phi}$ orthogonal. Furthermore the constraints can be direclty compared with the coefficients of the functions in the new basis. Computationally speaking, this is a simplification that does not have to be underestimated.  \\
All the degrees of freedom on the contact boundary will be treated as normal or tangent. The relative change of coordinates is equivalent to a change of basis, so that all the previous definitions of $\Lambda_{U_j,\nu}$, $\Lambda_{\Sigma_j,\nu}$, $\Lambda_{j,\nu}$ have to be consequently adapted:
\small
\begin{align*}
&\Lambda_{U_j,nt} =
\left\lbrace
\blambda_{U_j,\nu,nt}
,\quad \nu \in \mathcal{N}_j
  \right\rbrace 
 \qquad \qquad
\blambda_{U_j,\nu,nt}=
\begin{cases}
\blambda_{U_j,\nu} \bH_{\nu} &\nu \in \Gamma_{C,j} \\
\blambda_{U_j,\nu,nt}=\blambda_{U_j,\nu}  & \nu \notin \Gamma_{C,j}
\end{cases}   \qquad   j=1,...,J,\:\:\nu \in \mathcal{N}_j \\
&\Lambda_{\Sigma_j,nt} =
\left\lbrace
\blambda_{\Sigma_j,\phi,nt}
,\quad \phi \in \mathcal{F}_j
  \right\rbrace 
 \qquad \qquad
 \blambda_{\Sigma_j,\phi,nt}=
\begin{cases}
\blambda_{\Sigma_j,\phi,nt} \bQ_{\nu} & \phi \in \Gamma_{C,j}\\
\blambda_{\Sigma_j,\phi,nt}  & \phi \notin \Gamma_{C,j}\\
\end{cases} \qquad    \qquad j=1,...,J,\:\:\nu \in \mathcal{N}_j \\
  &
\blambda_{j,\nu,nt} \in \Lambda_{j,\nu,nt}=
  \left\lbrace
  \blambda_{U_j, \nu,nt} \in \Lambda_{U_j,nt}
    \right\rbrace  
  \cup
  \left\lbrace
\blambda_{\Sigma_j, \phi,nt} \in \Lambda_{\Sigma_j,nt}, \: \phi \in \mathcal{F}_j \cap  \in P_{\nu}^{\phi_j}
  \right\rbrace    \quad     \qquad j=2,...,J,\:\:\nu \in \mathcal{N}_j \\
  &
  \blambda_{1,nt} =   \left\lbrace
\bigcup\limits_{\nu \in \mathcal{N}_1}
\{
\blambda_{U_1, \nu,nt} \in \Lambda_{U_1,nt} \}
  \right\rbrace 
  \cup
    \left\lbrace
  \bigcup\limits_{\phi \in \mathcal{F}_1}
  \{
\blambda_{\Sigma_1, \phi,nt} \in \Lambda_{\Sigma_j,nt}
\}
  \right\rbrace   \qquad  \quad   \:\:\:   \qquad  \qquad j=1
\end{align*}
\normalsize
It is important to notice that the change of basis has a direct impact on the system and on the interpolation operators. Let $\bH$ be the global Householder matrix, which collects all the local matrices $\bH_{\nu}$ and $\bQ_{\phi}$, while it is the identity on interior degrees of freedom. This operator can be used to redefine \textit{all} the quantities in the normal tangential coordinate system. However, for the sake of simplicity of notation, from now on we will omit the relative subscript $_{nt}$ and we will denote the normal or tangent components by the notation: $[\cdot]_i$, for $i=n,t$. 
\section{Non-linear projection operators and coarse constraints}
To obtain optimal complexity, all the quantities of a given level $j$ should have a size that is proportional to the level itself. This means that no comparison with entities belonging to finer levels should be considered. In particular, instead of the constraints on level $J$, new coarse constraints and, consequently, proper convex sets $K_j$ should be introduced. 
We define the convex sets on the fine level $K_J$ and on the coarser level $K_j$, for $j=J-1,...,1$, in the following way:
\small
\begin{align}
\label{approximateconvexsets}
&K_J=\left\lbrace
  \bbx_J=(\bu_J, \bsigma_J) \in X_J: \: \bu_J|_{\Gamma_D}=\bu_J^D , \:  \bsigma_J|_{\Gamma_N}=\bt_J^N, \: \bu_J\cdot \bn_J|_{\Gamma_C}\leq g_J, \: \bn^T(\bsigma_J \bn)  \leq 0, \: \bt_J^T(\bsigma \bn_J) =0
  \right\rbrace \\
&  K_j=\left\lbrace
  \bbx_j=(\bu_j, \bsigma_j) \in X_j: \: \bu_j|_{\Gamma_D}=\textbf{0} , \:  \bsigma_j|_{\Gamma_N}=\textbf{0}, \: \bu_J\cdot \bn_j|_{\Gamma_C}\leq g_{j,u_n}, \: \bn^T(\bsigma_j \bn)  \leq g_{j,\sigma_n}, \: \bt_J^T(\bsigma \bn_J) =0
  \right\rbrace  \qquad 
\end{align}
\normalsize
%The construction of a convex set $K_j$, for each level $j=1,...,J$, is a necessary step away from suboptimal complexity. 
Let $\bc_{j,\nu}=(\bu_{j,\nu},\bsigma_{j,\nu})$ be the correction at level $j$ on the vertex $\nu$. Furthermore let $ \bc_{J,0}= \bbx_J^k $, $ \bc_{j,0}= \textbf{0} $ for $j=J-1,...,1$ and $ \bw_{j,\nu}= \sum_{\mu=0}^{\nu}  \bc_{j,\mu}$ be respectively the current iterate, the first corrections on level $j$ and the sum of all the corrections on the same level $j$ until the vertex $\nu$. Unlike (\ref{exactlocalproblem}), we solve the following approximate local problem: successively find $\bc_{j,\nu} \in K_{j,\nu}(\bw_{j,\nu-1})$  and $  \bc_{1}  \in K_1 $ such that:
\small
\begin{align}
\label{approximatelocalproblem}
\begin{split}
\bullet \: j=J,...,2, \: \: \nu=1,...,N_j:&\\
\mathcal{J}( \bw_{j,\nu-1}+\bc_{j,\nu}) &\leq \mathcal{J}( \bw_{j,\nu-1}+\by) \quad \forall \: \by \in K_{j,\nu}(\bw_{j,\nu-1})=\left\lbrace
\by \in \text{span}\{\lambda_{j,\nu}\}: \: \by +\bw_{j,\nu-1} \in K_j
  \right\rbrace \\
\bullet \:  j=1:\qquad \qquad \qquad\qquad\:\:&\\
  \mathcal{J}( \bc_{1})& \leq \mathcal{J}( \by) \quad   \:   \qquad \qquad
  \forall \: \by  \in K_1
  \end{split}
\end{align}
\normalsize
where, as opposed to (\ref{exactlocalconvexset}), in $K_{j,\nu}(\bw_{j,\nu-1})$ we consider $K_j$ instead of $K_J$. It is evident that, if $K_{j} \subset K_{j+1}$ for $j=1,...,J-1$, then $K_{j,\nu} \subset K_{j,\nu}^{*}$, which also implies that all the intermediate approximations of the solution belong to $K_J$. Thus we must choose the coarse constraints function $ g_{j,u_n}$ and $ g_{j,\sigma_n}$ so that $K_j \subset K_{j+1}$. Of course $ g_{j,u_n}$ and $ g_{j,\sigma_n}$ will respectively depend on $ g_{j+1,u_n}$ and $ g_{j+1,\sigma_n}$ and, to this aim, specific projection operators need to be examined.\\
Let $j=H$, $j+1=h$. Then $\mathcal{T}_H=\mathcal{T}_j$ is a mesh at level $j$ and $\mathcal{T}_h=\mathcal{T}_{j+1}$ its uniform refinement. Let $\varepsilon_H \in \mathcal{E}_H \cap \Gamma_{C,H}$ be a coarse edge which contains two coarse vertices $\nu_{H,1}$, $\nu_{H,2} \in \mathcal{N}_H$, on its ends, and a fine midpoint $\nu_h \in  \mathcal{N}_h$. Let $v_h \in U_{h,n} $ be a linear function defined on $\Gamma_{C,h}$. Its non-linear projection $v_H=I_h^H(v_h) \in U_{H,n}$ must fulfill $v_H \leq v_h$ and, consequently, $P_{H,n}^h v_H \leq v_h$, where $P_{H,n}^h$ is defined in (\ref{normalinterpolations}) (remind that now everything is expressed in the new coordinate system):
\begin{align}
\label{coarsegapfunctionconditionP1}
\begin{aligned}
& v_H(\nu_{H,1}) \leq v_h(\nu_{H,1})\\
&  v_H(\nu_{H,2}) \leq v_h(\nu_{H,2})\\
&  \frac{1}{2}( v_H(\nu_{H,1}) + v_H(\nu_{H,2})) \leq v_h(\nu_{h})
\end{aligned}
\qquad \qquad \forall \varepsilon_H \in \mathcal{E}_H \cap \Gamma_{C,H}
\end{align}
It is easy to see that, on $e_H$, the following values satisfy the three conditions above:
\begin{align}
\label{coarsegapfunctionconditionP1LinearInterpolation}
&
a)\quad\begin{cases}
\tilde{v}_H({\nu_{H,1}})= \min( v_h(\nu_{H,1}),\max( v_h(\nu_{h}), 2 v_h(\nu_{h}) - v_h(\nu_{H,2})) )\\
\tilde{v}_H({\nu_{H,2}})=\min( v_h(\nu_{H,2}),\max( v_h(\nu_{h}), 2 v_h(\nu_{h})- v_h(\nu_{H,1}) ) )
\end{cases}
&& \forall \varepsilon_H \in \mathcal{E}_H \cap \Gamma_C\\
\label{coarsegapfunctionconditionP1MeanInterpolation}
&
b)\quad
\begin{cases}
\tilde{v}_H({\nu_{H,1}})= \min( v_h(\nu_{H,1}), v_h(\nu_{h}) )\\
\tilde{v}_H({\nu_{H,2}})=\min( v_h(\nu_{H,2}),v_h(\nu_{h} ) )
\end{cases}
&& \forall \varepsilon_H \in \mathcal{E}_H \cap \Gamma_C \\
\label{coarsegapfunctionconditionP1MinInterpolation}
&
c)\quad
\begin{cases}
\tilde{v}_H({\nu_{H,1}})= \min( v_h(\nu_{H,1}),v_h(\nu_{h}),  v_h(\nu_{H,2}) )\\
\tilde{v}_H({\nu_{H,2}})= \min( v_h(\nu_{H,1}),v_h(\nu_{h}),  v_h(\nu_{H,2}) )
\end{cases}
&& \forall \varepsilon_H \in \mathcal{E}_H \cap \Gamma_C
\end{align}
Anyhow, all the edges to which $\nu_{H,1}$ belongs need to be considered. Therefore the effect of the non linear interpolation $I_h^H$ can be summarized in this way: 
\begin{align}
\label{nonlinearprojectionun}
\displaystyle
&v_H=I_{h,u_n}^H v_h= \sum_{\nu_{H,i} \in \mathcal{N}_H \cap \Gamma_{C,H}} \left[\lambda_{U_H,H_i}\right]_n \:v_H(\nu_{H,i})
\qquad \text{with} \qquad
v_H(\nu_{H,1})= \min_{\varepsilon_H \in P_{\nu_{H,1}}^{E_H}} \tilde{v}(\nu_{H,i})
\end{align}
Now consider a coarse face $\phi_H \in \mathcal{T}_H \cap \Gamma_{C,H}$ and fine faces which belong to it, i.e. $\phi_h \in P_{\phi_H}^{\phi_h}$. Then consider $s_h \in \Sigma_{h,n}$, a piecewise constant function on $\Gamma_{C,h}$. We want to define its non-linear projection $s_H=I_{h,\sigma_n}^H s_h \in \Sigma_{H,n}$ so that $s_H \leq s_h$ and $\Pi_{H,n}^h s_H \leq s_h$, where $\Pi_{H,n}^h$ is defined in (\ref{normalinterpolations}). It suffices that:
\begin{align*}
s_H(\phi_H) \leq s_h(\phi_h) \quad \forall \phi_h \in P_{\phi_H}^{\phi_h}
\end{align*}
Thus:
\begin{align}
\label{nonlinearprojectionsigman}
\displaystyle
&s_H=I_{h,\sigma_n}^H s_h= \sum_{\phi_{H_i} \in T_H} \left[ \lambda_{\Sigma_H,H_i}\right]_n \:s_H(\phi_{H_i})
\qquad \text{with} \qquad
 s_H(\phi_{H_i})= \min_{\phi_{h} \in P_{\phi_{H_i}}^{\phi_{h}}} s_h(\phi_h)
\end{align}
Once the non-linear projections for both spaces have been introduced, let $g_J \in U_{J,n}$ and $0 \in \Sigma_{J,n}$ be the fine constraints of the problem. Then, for each level $j=1,...,J$, we define coarse constraints $g_{j,u_n} \in U_{j,n}$, for the normal displacement corrections:
\begin{align}
\label{coarseconstraintun}
\begin{cases}
g_{J,u_n}=g & j=J \\
g_{j,u_n}=I_{j+1,u_n}^j \left( g_{j+1,u_n} - \sum_{\nu=1}^{N_{j+1}} \left[ \bu_{j+1,\nu} |_{\Gamma_C} \right]_n\right) & j=J-1,...,1
\end{cases}
\end{align}
and $g_{j,\sigma_n} \in \Sigma_{j,n}$, for the pressure corrections:
\begin{align}
\label{coarseconstraintsigman}
\begin{cases}
 g_{J,\sigma_n}=0 & j=J \\
g_{j,\sigma_n}=I_{j+1,\sigma_n}^j \left( g_{j+1,\sigma_n} - \sum_{\nu=1}^{N_{j+1}}  \left[ \bsigma_{j+1,\nu} |_{\Gamma_C}\right]_n \right) & j=J-1,...,1
\end{cases}
\end{align}
By exploiting (\ref{nonlinearprojectionun}), (\ref{nonlinearprojectionsigman}), (\ref{coarseconstraintun}), (\ref{coarseconstraintsigman}) and the definitions of the interpolation operators (\ref{interpolations}), it follows $K_j \subset K_{j+1}$ for $j=1,...,J-1$:
\begin{align*}
\sum_{\nu=1}^{N_{j}}  \left[ \bu_{j,\nu} |_{\Gamma_C}\right]_n \leq g_{g,u_n} 
\quad& \Rightarrow \quad
P_{j,n}^{j+1} \left(
\sum_{\nu=1}^{N_{j}} \left[  \bu_{j,\nu} |_{\Gamma_C}\right]_n\right) \leq g_{j+1,u_n}-
\sum_{\nu=1}^{N_{j+1}}  \left[ \bu_{j+1,\nu} |_{\Gamma_C}\right]_n\\
\sum_{\nu=1}^{N_{j}}  \left[ \bsigma_{j,\nu} |_{\Gamma_C}\right]_n \leq g_{j,\sigma_n} 
\quad &\Rightarrow \quad
\Pi_{j,n}^{j+1} \left(
\sum_{\nu=1}^{N_{j}}   \left[\bsigma_{j,\nu} |_{\Gamma_C}\right]_n\right) \leq g_{j+1,\sigma_n}-
\sum_{\nu=1}^{N_{j+1}}  \left[ \bsigma_{j+1,\nu}|_{\Gamma_C} \right]_n 
\end{align*}
Furthermore, by iterating the same argument for each level, it is clear that adding to the current iterate $\bbx^k$ all the corrections $\sum_{j=1}^J \sum_{\nu}^{N_j }\bc_{j,\nu} \in K_J$, the resulting vector is still in the convex:
\begin{align*}
&\sum_{j=1}^{J-1}\:  \prod_{k=j}^{J-1}  P_{k,n}^{k+1}\left(
\sum_{\nu=1}^{N_{j}}   \left[\bu_{j,\nu}|_{\Gamma_C}\right]_n \right) +\left(
\sum_{\nu=1}^{N_{j+1}}  \left[\bu_{J,\nu}|_{\Gamma_C}\right]_n\right)
\leq g, \\
&\sum_{j=1}^{J-1} 
\prod_{k=j}^{J-1} \Pi_{k,n}^{k+1} \left(
\sum_{\nu=1}^{N_{k}}  \left[ \bsigma_{k,\nu}|_{\Gamma_C} \right]_n\right) +\:
 \left(
\sum_{\nu=1}^{N_{J}}  \left[ \bsigma_{J,\nu}|_{\Gamma_C}   \right]_n \right)
 \leq 0
\end{align*}
\section{Truncated Basis}
We can define the set of active nodes and faces on the level $j$ in the following way:
\begin{align}
\label{activedofs}
\begin{aligned}
&\mathcal{N}_j^{\bullet}=
\left\lbrace
\nu \in \mathcal{N}_j \cap \Gamma_{C,j}: \quad g_{j,u_n}\big\rvert_{\nu_{j}} = \sum_{\nu=1}^{N_{j}} \left[ \bu_{j,\nu} \big\rvert_{\Gamma_C} \right]_n
  \right\rbrace  \\ 
&\mathcal{F}_j^{\bullet}=
\left\lbrace
\phi_j \in \mathcal{F}_j \cap \Gamma_{C,j}:\quad
 g_{j,\sigma_n} \big\rvert_{\phi_{j}}=\sum_{\nu=1}^{N_{j}}  \left[ \bsigma_{j,\nu_{j}} \big\rvert_{\Gamma_C}\right]_n 
   \right\rbrace 
   \end{aligned}
\end{align}
By definition $g_{j,u_n}, g_{j,\sigma_n} \geq 0$. So  it is clear that if at some level exists a $\nu_{j+1} \in \mathcal{N}_{j+1}^{\bullet} $ or a $\phi_{j+1} \in \mathcal{F}_{j+1}^{\bullet}$,
the corresponding constraints on the successive coarser level j will be zero on all the nodes $\{ \nu_j \in \varepsilon_{j+1} :   \: \varepsilon_{j+1} \in P_{\nu_{j+1}}^{\varepsilon_{j+1}} \} $ or all the faces $\phi_j \in P_{\phi_{j+1}}^{\phi_j}$. 
Consequently no positive coarse correction in the direction of the obstacle can be expected there. If also the correction on level $j$ is zero on $\nu_j$ or $\phi_j$, then $\nu_{j} \in \mathcal{N}_{j}^{\bullet} $ or a $\phi_{j} \in \mathcal{F}_{j}^{\bullet}$ and the previous argument can be repeated. By following this path of reasoning, it turns out that less and less coarse corrections which are positive in the normal direction can be picked. Since we are not able to properly improve the solution in the obstacle direction, a slow down in the convergence speed has to expected. Truncated basis is the solution to this problem and has been already proposed in \cite{Kor94}. The idea is to consider the sets of basis functions as dependent on the current intermediate iterate, by switching off the degrees of freedom $\nu_{j} \in \mathcal{N}_{j}^{\bullet} $ or a $\phi_{j} \in \mathcal{F}_{j}^{\bullet}$ for each level $j=J,...,1$. Then the truncated basis of level $j=J,...,1$, for $ \nu \in \mathcal{N}_j$,  $\phi \in \mathcal{F}_j $, is:
\begin{align}
\label{truncatedbasis}
&
\left[\tilde{\blambda}_{U_j,\nu} \right]_i=
\begin{cases}
\left[\blambda_{U_j,\nu}\right]_i  & \nu \in \mathcal{N}_j \setminus  \mathcal{N}_{j}^{\bullet}, \: {i=n, t}   \\
0  & \nu \in   \mathcal{N}_{j}^{\bullet},\qquad \: {i=n}   \\
\left[\blambda_{U_j,\nu}\right]_i  & \nu \in\mathcal{N}_{j}^{\bullet}  , \qquad \: {i=t}   
\end{cases}
\quad
\left[\tilde{\blambda}_{\Sigma_j,\phi} \right]_i=
\begin{cases}
\left[\blambda_{\Sigma_j,\phi}\right]_i  & \phi \in \mathcal{F}_j \setminus  \mathcal{F}_{j}^{\bullet}, \: {i=n, t}   \\
0  & \phi \in   \mathcal{F}_{j}^{\bullet} \quad \quad\:\:\: {i=n}   \\
\left[\blambda_{\Sigma_j,\phi}\right]_i  & \phi \in\mathcal{F}_{j}^{\bullet}  , \qquad {i=t}   
\end{cases}
\end{align}
The truncated basis of the corresponding coarse subspace of level $j-1$ can be expressed as a linear combination of the truncated basis of level $j$. Then the corrections on level $j-1$ can be computed and new active degrees of freedom can be determined. Therefore the basis on a level $j$ will strictly depend on the one of the immediately higher level. 
Since all $\tilde{\blambda}_{U_j,\nu}$ and $\tilde{\blambda}_{\Sigma_j,\nu} $ depend on the $\mathcal{N}_j^{\bullet}$ and $\mathcal{F}_j^{\bullet}$, which in turn depend on the current intermediate iterate on level $j$, i.e. $\bbx^k +\sum_{j=1}^J \sum_{\nu}^{N_j }\bc_{j,\nu}$, the truncated basis will depend on it as well.\\
The truncation, that is simply a change of basis, affects all the quantities and all the transfer operator of each level. Particular care must be taken for the assembling of the interpolation operators, coarse matrices and vectors. Specifically the constraints $g_{j,u_n}$, $g_{j,\sigma_n}$ can be defined in the usual way, except for the degrees of freedom $\nu_{j+1} \in \mathcal{N}_j^{\bullet}$ or $\phi_{j+1} \in \mathcal{F}_j^{\bullet}$. To this aim, before projecting onto the coarser level, we redefine:
\begin{align}
\label{nodesorfacesincontactinf}
&g_{j+1,u_n}\big\rvert_{\nu_{j+1}} - \sum_{\nu=1}^{N_{j+1}} \left[ \bu_{j+1,\nu} \big\rvert_{\Gamma_C} \right]_n=+\infty ,\qquad  \nu_{j+1} \in \mathcal{N}_j^{\bullet}
\\
 &g_{j+1,\sigma_n} \big\rvert_{\phi_{j+1}}-\sum_{\nu=1}^{N_{j+1}}  \left[ \bsigma_{j+1,\nu_{j+1}} \big\rvert_{\Gamma_C}\right]_n =+\infty \quad \phi_{j+1} \in \mathcal{F}_j^{\bullet}
\end{align}
so that all vertices and faces that fulfill equality constraints have no influence on coarser corrections.
\section{Numerical Experiments}
The definition of the augmented functional $\mathcal{J}$ in (\ref{augmentedfunctional}) is not unique and depends on the weights that are chosen for each term. One of these constants is arbitrary and can be fixed as the reference value, e.g. $C_{\text{const}}=1$, while all the others must depend on this one. Experimentally a higher value for $C_{\text{eq}}$ is necessary for capturing the stress. Anyhow, no matter which values are used for $C_{\text{const}}$ and $C_{\text{eq}}$, the resulting linear system will be symmetric and positive definite. On the other hand the penalty contribution is non negative if and only if it is evaluated on the convex set. Nevertheless, in the discrete form, such term does not correspond to a positive definite matrix. As a consequence, the matrix of the overall problem is for sure symmetric, but it can be positive or indefinite according to the weights $C_{\text{eq}}$, $C_{\text{const}}$, $C_{\text{compl}}$. In order to avoid indefinite problems, a $C_{\text{compl}}$ which is not too large has to be considered. At the same time, it must not be too small, otherwise we would not be able to describe contact conditions. For FOSLS problems, a compromise is always required. A typical decision for the following tests can be $C_{\text{eq}}=$1e2, 1e3, $C_{\text{const}}=$1, $C_{\text{compl}}=$1e1, 1e2.  
For all the here presented experiments, the initial guess is computed as the solution of the problem on the coarsest mesh, then interpolated until the finest tassellation. This approach guarantees that for simulations with more levels, the initial guess is actually the same. For two levels, it also coincides with the nested iteration strategy. \\\\
Let $\Omega=[0,1]\times[0,1]$ be a square domain, with sides left, bottom, right, top. We enforce a displacement $\bu|_{\text{top}}=(0,-0.01)$ and zero stresses on the left and right sides, $\bsigma \bn|_{\text{left},\text{}right}= \textbf{0}$. The bottom line is actually $\Gamma_C$. The domain lies on the straight rigid foundation, described by the gap function $g(x,y)=0$ and by the normal $\bn(x,y)=(0,1)$. Thus, after the application of a uniform displacement on the top, the square will deform so that no penetration with the rigid obstacle occur. \\
The coarse mesh consists of only two elements. Then we uniformly refine this mesh until the level $J=9$. We solve the problem by means of the monotone multilevel method. We use, for each level $j>1$, 3 pre-smoothing and 3 post-smoothing, while, on the coarse level, we exploit the active set method. We study the convergence for the compressible and incompressible cases, i.e. $(\mu,\lambda)=(1,1)$ and $(\mu,\lambda)=(1,\infty)$.  As non-linear projection, we use the c) in (\ref{coarsegapfunctionconditionP1MinInterpolation}). But, since the square is already in contact and the initial guess is computed as previously mentioned, after one smoothing step all the active degrees of freedom are detected. Therefore the problem reduces to a linear one.\\
We can see that the convergence, for more than six levels, is reached around 20 iterations for the compressible material and around 30 for the incompressible one, with a constant limit convergence rate which is respectively about 0.7 and 0.6. In the first situation, we can also see that the behaviour is non-linear at the beginning and only after a while the convergence rate becomes constant. For the incompressible material, on the other hand, the convergence rate is practically constant. Furthermore, in the limit case, it is smaller than the one for $\lambda=1$. This suggests that for incompressible materials, this method can be very attractive. 
\begin{figure}[htbp!]
	\includegraphics[scale=0.14]{img/SquareResidualCompressible.eps}
	\quad
		\includegraphics[scale=0.14]{img/SquareRateCompressible.eps}
	\caption{Square mesh. Compressible material.}
	\label{ResidualRateSquare}
	\includegraphics[scale=0.14]{img/SquareResidualIncompressible.eps}
	\quad
		\includegraphics[scale=0.14]{img/SquareRateIncompressible.eps}
	\caption{Square mesh. Incompressible material.}
	\label{ResidualRateSquare}	
	\end{figure}
\\\\
Now we consider the Hertz's problem for a semicircle of radius $r=0.5$ and center $\bc=(0,1)$ which is pushed towards a rigid plane $f(x,y)=0$ by a uniform displacement $\bu=(0,-0.01)$ applied at the top. In this situation we just use a two level method. Indeed, for a circular mesh, non-nested semi-circular refinements would be optimal, but it is not an assumption of our framework. Therefore, given a semicircle coarse mesh, we just refine it one time. Altough the refinement is not optimal and the complementarity condition is just a penalty term, a two-level method still gives good results for the Signorini's problem. See \ref{HertzIncompressibleNonUniform}).
	\begin{figure}[htbp!]
\centering
	\includegraphics[scale=0.1]{img/ContactDisplacementUniformLambda1Mu1.eps}
	\qquad
		\includegraphics[scale=0.1]{img/ContactPressureUniformLambda1Mu1.eps}
\\
	\includegraphics[scale=0.1]{img/ContactDisplacementNonUniformLambda1Mu1.eps}
	\qquad
		\includegraphics[scale=0.1]{img/ContactPressureNonUniformLambda1Mu1.eps}\\
	\includegraphics[scale=0.1]{img/ContactDisplacementVeryNonUniformLambda1Mu1.eps}
	\qquad
		\includegraphics[scale=0.1]{img/ContactPressureVeryNonUniformLambda1Mu1.eps}\\		
	\includegraphics[scale=0.1]{img/ContactDisplacementUniformLambdaInfMu1.eps}
	\qquad
		\includegraphics[scale=0.1]{img/ContactPressureUniformLambdaInfMu1.eps}\\
	\includegraphics[scale=0.1]{img/ContactDisplacementNonUniformLambdaInfMu1.eps}
	\qquad
		\includegraphics[scale=0.1]{img/ContactPressureNonUniformLambdaInfMu1.eps}
		\\
	\includegraphics[scale=0.1]{img/ContactDisplacementVeryNonUniformLambdaInfMu1.eps}
	\qquad
		\includegraphics[scale=0.1]{img/ContactPressureVeryNonUniformLambdaInfMu1.eps}
			\caption{On the left, contact displacement components. On the right: the contact pressure. First three rows: $(\mu, \lambda)=(1,1)$ for meshes with $h_{\text{max}}/h_{\text{min}}=3.1688, 7.0567, 29.7936$. Last three rows: $(\mu, \lambda)=(1,\infty)$ for meshes with $h_{\text{max}}/h_{\text{min}}=3.1688, 7.0567, 29.7936$.}		
\label{HertzIncompressibleNonUniform}		
	\end{figure}
${}$\\
We study the convergence for the compressible and incompressible cases, i.e. $(\mu,\lambda)=(1,1)$ and $(\mu,\lambda)=(1,\infty)$. For each of these, we exploit the three kinds of projection operators a), b), c) in (\ref{coarsegapfunctionconditionP1LinearInterpolation}), (\ref{coarsegapfunctionconditionP1MeanInterpolation}), (\ref{coarsegapfunctionconditionP1MinInterpolation}). In addition, we consider the case d) in which the coarse constraints are enforced equal to infinity. The number of smoothing-steps is 5, $C_{\text{eq}}=$1e2 and $C_{\text{compl}}=$1e1. In pictures \ref{ResidualRateUniform}),  \ref{ResidualRateNonUniform}) and \ref{ResidualRateVeryNonUniform}) we study the behaviour of a two level method for three different meshes, whose ratio $h_{\text{max}}/h_{\text{min}}=3.1688, 7.0567, 29.7936$, where $h_{\text{max}}=\max_{\tau \in \mathcal{T}_1}\text{diam}(\tau)$ and  $h_{\text{min}}=\min_{\tau \in \mathcal{T}_1}\text{diam}(\tau)$. The less uniform the mesh and the more the contact boundary is refined.\\
The history of the sequence of iterations can be subdivided into three general main phases:
\begin{enumerate}[label=\Roman{enumi}]
\item a non linear phase in which the high frequencies components of the error are damped, with a non linearly increasing convergence rate;
\item a non linear phase in which the active set of the current iterate does not coincide with the one of the solution, although the convergence rate is linear;
\item a linear phase in which the active set of the current iterate coincides with the one of the solution (this condition is attained for sure for $k \to \infty$ , as established in \cite{Kor94});
\end{enumerate} 
In principle I), II) and III) should occur sequentially in this order. Anyhow  the only necesssary phase is indeed the first one. In fact if in the square example the true active set is known from the beginning, for the half-circle is discovered after a while. So for the first case, III) follows I), while for the semicircle, we can appreciate two different situations: I) followed by II) and I) followed by III). 
\\ In all cases the rate of convergence has a non linear behaviour at the beginning and at the end becomes constant. Since there is only one phase in which the rate is constant, after I) we have either II) or III), but not both. Essentially the third phase concerns a), b) and d) for $\lambda=1,\infty$ and c) for $\lambda=1$ and in the uniform case. Generally the convergence for c) is slow. This is because the coarse constraints are so strict that we have I) and II), while III) is never approached. If it is not in the uniform case, it is just because, for some particular reasons, the second phase is skipped and the third phase is quickly accessed. Anyhow, in principle the projections effect in a different manner how fast III) is approached. It is evident that it is desirable to avoid c), since III) is difficult to reach and therefore the convergence rates tend to be very large. On the other hand, a), b) and d) facilitate the access to the linear setting. \\
What is important to notice is that a), b) and d) give basically the same speed of convergence. Anyhow for a) it is necessary to know the coarse edges of the patch and also which is the corresponding fine midpoint. For b), instead, only the external nodes on the fine nodes patch are needed. This suggests to prefer b) and avoid the use of a). The d) case is actually the one which does not take into consideration coarse constraints. The advantage of d) is that there is no need for non-linear projection operators for the coarse constraints, although the truncated basis will make the interpolation operators depend on the current iterate. The disadvantage is that there is no guarantee that the functional is actually minimized. Therefore, among all, the b) case is the one which actually minimizes the functional, requires less effort of programming,  and is sufficiently fast.
\begin{figure}[htbp!]
	\includegraphics[scale=0.14]{img/ResidualsUniform.eps}
	\quad
		\includegraphics[scale=0.14]{img/RatesUniform.eps}
	\caption{Mesh with $h_{max}/h_{min}=3.1688$. Where:\\
	a) projection (\ref{coarsegapfunctionconditionP1}), 
	b)projection (\ref{coarsegapfunctionconditionP1MeanInterpolation}),
	c)projection (\ref{coarsegapfunctionconditionP1MinInterpolation}),
	d)coarse constraints=$\infty$.}
	\label{ResidualRateUniform}
		\includegraphics[scale=0.14]{img/ResidualsNonUniform.eps}
	\quad
		\includegraphics[scale=0.14]{img/RatesNonUniform.eps}
	\caption{Mesh with $h_{max}/h_{min}=7.0567$. 	Where:\\
	a)projection (\ref{coarsegapfunctionconditionP1}), 
	b)projection (\ref{coarsegapfunctionconditionP1MeanInterpolation}),
	c)projection (\ref{coarsegapfunctionconditionP1MinInterpolation}),
	d)coarse constraints=$\infty$.}
		\label{ResidualRateNonUniform}
				\includegraphics[scale=0.14]{img/ResidualsVeryNonUniform.eps}
	\quad
		\includegraphics[scale=0.14]{img/RatesVeryNonUniform.eps}
	\caption{Mesh with $h_{max}/h_{min}=29.7936$.	Where:\\
	a)projection (\ref{coarsegapfunctionconditionP1}), 
	b)projection (\ref{coarsegapfunctionconditionP1MeanInterpolation}),
	c)projection (\ref{coarsegapfunctionconditionP1MinInterpolation}),
	d)coarse constraints=$\infty$.
}
		\label{ResidualRateVeryNonUniform}
\end{figure}
\clearpage
\bibliographystyle{plain} 
\bibliography{biblio}
%\end{thebibliography}{9}
\end{document}
